1. Refactored Action Execution Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique action IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        ActionExecutionResult,  # Output: Result of executed action
        CognitiveDirective,     # Input: Directives to execute actions
        WorldModelState,        # Input: Current state of the world (for safety checks)
        BodyAwarenessState,     # Input: Robot's physical state (for safety checks)
        PerformanceReport,      # Input: Overall system performance (influences caution)
        EthicalDecision,        # Input: Ethical clearances/constraints
        MemoryResponse          # Input: Past action outcomes, safety protocols
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Action Execution Node.")
    ActionExecutionResult = String
    CognitiveDirective = String
    WorldModelState = String
    BodyAwarenessState = String
    PerformanceReport = String
    EthicalDecision = String
    MemoryResponse = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'action_execution_node': {
                'execution_interval': 0.1, # How often to check for new actions
                'llm_safety_check_threshold_salience': 0.7, # Cumulative salience to trigger LLM safety check
                'recent_context_window_s': 5.0, # Window for deques for LLM context
                'action_sim_success_rate': 0.8 # Simulated success rate for actions if no real hardware
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 15.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class ActionExecutionNode:
    def __init__(self):
        rospy.init_node('action_execution_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "action_log.db")
        self.execution_interval = self.params.get('execution_interval', 0.1) # How often to check for new actions
        self.llm_safety_check_threshold_salience = self.params.get('llm_safety_check_threshold_salience', 0.7) # Cumulative salience to trigger LLM safety check
        self.recent_context_window_s = self.params.get('recent_context_window_s', 5.0) # Window for deques for LLM context
        self.action_sim_success_rate = self.params.get('action_sim_success_rate', 0.8) # For simulated action outcomes

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 15.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'action_log' table if it doesn't exist.
        # NEW: Added 'llm_safety_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS action_log (
                id TEXT PRIMARY KEY,            -- Unique action ID (UUID)
                timestamp TEXT,
                action_id TEXT,                 -- Identifier for the action type (e.g., 'move_arm', 'speak')
                command_payload_json TEXT,      -- JSON of the action parameters
                success BOOLEAN,                -- True if action succeeded
                outcome_summary TEXT,           -- Text summary of the outcome
                predicted_outcome_match REAL,   -- How well predicted outcome matched actual (0.0 to 1.0)
                safety_clearance BOOLEAN,       -- True if action passed safety checks
                llm_safety_reasoning TEXT,      -- NEW: LLM's reasoning for safety check
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of execution
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_action_timestamp ON action_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.action_queue = deque() # Queue for incoming cognitive directives that are action requests

        # Deques to maintain a short history of inputs relevant to safety checks
        self.recent_cognitive_directives = deque(maxlen=5) # All directives, but filter for action requests
        self.recent_world_model_states = deque(maxlen=5)
        self.recent_body_awareness_states = deque(maxlen=5)
        self.recent_performance_reports = deque(maxlen=5)
        self.recent_ethical_decisions = deque(maxlen=5)
        self.recent_memory_responses = deque(maxlen=3) # For safety protocols, past failure analysis

        self.cumulative_safety_salience = 0.0 # Aggregated salience to trigger LLM safety check

        # --- Publishers ---
        self.pub_action_execution_result = rospy.Publisher('/action_execution_result', ActionExecutionResult, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # For requesting reconsideration from CognitiveControl


        # --- Subscribers ---
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON
        rospy.Subscriber('/body_awareness_state', String, self.body_awareness_state_callback) # Stringified JSON
        rospy.Subscriber('/performance_report', PerformanceReport, self.performance_report_callback)
        rospy.Subscriber('/ethical_decision', String, self.ethical_decision_callback) # Stringified JSON
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON


        # --- Timer for periodic action execution ---
        rospy.Timer(rospy.Duration(self.execution_interval), self._run_action_execution_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's action execution system online.")

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_action_execution_wrapper(self, event):
        """Wrapper to run the async action execution from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM safety check task already active. Skipping new cycle.")
            return

        if self.action_queue:
            # Process one action from the queue per cycle
            action_to_execute = self.action_queue.popleft()
            self.active_llm_task = asyncio.run_coroutine_threadsafe(
                self.execute_action_async(action_to_execute, event), self._async_loop
            )
        else:
            rospy.logdebug(f"{self.node_name}: No actions in queue.")

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.2, max_tokens=250):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Low temperature for reliable safety checks
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0']['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience for safety checks ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM safety check."""
        self.cumulative_safety_salience += score
        self.cumulative_safety_salience = min(1.0, self.cumulative_safety_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_cognitive_directives, self.recent_world_model_states,
            self.recent_body_awareness_states, self.recent_performance_reports,
            self.recent_ethical_decisions, self.recent_memory_responses
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'ExecuteAction':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                action_id = payload.get('action_id')
                if action_id:
                    self.action_queue.append({
                        'action_id': action_id,
                        'command_payload': payload,
                        'source_directive_id': data.get('id', str(uuid.uuid4())), # Assuming directive has an ID or generate one
                        'urgency': data.get('urgency', 0.5),
                        'timestamp': data.get('timestamp', str(rospy.get_time()))
                    })
                    self._update_cumulative_salience(data.get('urgency', 0.0) * 0.8) # High urgency means more safety check needed
                    rospy.loginfo(f"{self.node_name}: Queued action: '{action_id}'. Queue size: {len(self.action_queue)}.")
                else:
                    self._report_error("INVALID_ACTION_DIRECTIVE", "Received ExecuteAction directive with no action_id.", 0.6, {'directive_payload': data.get('command_payload')})
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data)
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def world_model_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),
            'changed_entities_json': ('[]', 'changed_entities_json'),
            'significant_change_flag': (False, 'significant_change_flag'),
            'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('changed_entities_json'), str):
            try: data['changed_entities'] = json.loads(data['changed_entities_json'])
            except json.JSONDecodeError: data['changed_entities'] = []
        self.recent_world_model_states.append(data)
        if data.get('significant_change_flag', False):
            self._update_cumulative_salience(0.2) # Changes in environment might impact safety
        rospy.logdebug(f"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.")

    def body_awareness_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'body_state': ('normal', 'body_state'),
            'posture_description': ('stable', 'posture_description'), 'anomaly_detected': (False, 'anomaly_detected'),
            'anomaly_severity': (0.0, 'anomaly_severity')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_body_awareness_states.append(data)
        if data.get('anomaly_detected', False) and data.get('anomaly_severity', 0.0) > 0.0:
            self._update_cumulative_salience(data.get('anomaly_severity', 0.0) * 0.7) # Body anomalies are critical for safety
        rospy.logdebug(f"{self.node_name}: Received Body Awareness State. Anomaly: {data.get('anomaly_detected', False)}.")

    def performance_report_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'overall_score': (1.0, 'overall_score'),
            'suboptimal_flag': (False, 'suboptimal_flag'), 'kpis_json': ('{}', 'kpis_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('kpis_json'), str):
            try: data['kpis'] = json.loads(data['kpis_json'])
            except json.JSONDecodeError: data['kpis'] = {}
        self.recent_performance_reports.append(data)
        if data.get('suboptimal_flag', False) and 'action_execution_kpi_score' in data.get('kpis', {}) and data['kpis']['action_execution_kpi_score'] < 0.7:
            self._update_cumulative_salience((1.0 - data['kpis']['action_execution_kpi_score']) * 0.5) # Poor action perf suggests caution
        rospy.logdebug(f"{self.node_name}: Received Performance Report. Suboptimal: {data.get('suboptimal_flag', False)}")

    def ethical_decision_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'decision_id': ('', 'decision_id'),
            'action_proposal_id': ('', 'action_proposal_id'), # Link to a proposed action
            'ethical_clearance': (False, 'ethical_clearance'),
            'ethical_score': (0.0, 'ethical_score'), # 0.0 (unethical) to 1.0 (highly ethical)
            'ethical_reasoning': ('', 'ethical_reasoning'),
            'conflict_flag': (False, 'conflict_flag')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_ethical_decisions.append(data)
        if not data.get('ethical_clearance', True) or data.get('conflict_flag', False):
            self._update_cumulative_salience(0.9) # Ethical conflicts are critical safety concerns
        rospy.logdebug(f"{self.node_name}: Received Ethical Decision. Clearance: {data.get('ethical_clearance', 'N/A')}.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        if data.get('response_code', 0) == 200 and \
           any('safety_protocol' in mem.get('category', '') or 'action_failure' in mem.get('category', '') for mem in data['memories']):
            self._update_cumulative_salience(0.3) # Relevant safety info or past failures
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    # --- Core Action Execution Logic (Async with LLM for Safety) ---
    async def execute_action_async(self, action_data, event):
        """
        Asynchronously executes a robot action after performing safety checks,
        potentially using LLM for complex safety reasoning.
        """
        self._prune_history() # Keep context history fresh

        action_id = action_data.get('action_id', 'unknown_action')
        command_payload = action_data.get('command_payload', {})
        source_directive_id = action_data.get('source_directive_id', 'unknown')
        urgency = action_data.get('urgency', 0.5)

        safety_clearance = False
        llm_safety_reasoning = "Not evaluated by LLM."
        
        # Determine if LLM safety check is needed
        if self.cumulative_safety_salience >= self.llm_safety_check_threshold_salience or urgency > 0.8:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for safety check of action '{action_id}' (Salience: {self.cumulative_safety_salience:.2f}).")
            context_for_llm = self._compile_llm_context_for_safety_check(action_data)
            llm_safety_output = await self._perform_llm_safety_check(context_for_llm)

            if llm_safety_output:
                safety_clearance = llm_safety_output.get('is_safe', False)
                llm_safety_reasoning = llm_safety_output.get('reasoning', 'LLM provided no specific reasoning.')
                rospy.loginfo(f"{self.node_name}: LLM Safety Check for '{action_id}': Safe={safety_clearance}. Reason: {llm_safety_reasoning[:50]}...")
            else:
                rospy.logwarn(f"{self.node_name}: LLM safety check failed or returned no valid output for '{action_id}'. Falling back to simple rules.")
                safety_clearance, llm_safety_reasoning = self._perform_simple_safety_check(action_data)
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_safety_salience:.2f}) for LLM safety check. Applying simple rules.")
            safety_clearance, llm_safety_reasoning = self._perform_simple_safety_check(action_data)

        action_success = False
        outcome_summary = "Action not executed due to safety concerns."
        predicted_outcome_match = 0.0

        if safety_clearance:
            # Simulate action execution (replace with actual robot control interfaces)
            rospy.loginfo(f"{self.node_name}: Executing action '{action_id}' with payload: {command_payload}.")
            
            # Placeholder for actual hardware/simulation command
            try:
                # In a real system, you'd send commands to motor controllers, speech synthesizers, etc.
                # Example: If action_id is 'move_arm', call a function that sends ROS /joint_command messages.
                # For this mock, we'll just simulate success/failure.
                if random.random() < self.action_sim_success_rate:
                    action_success = True
                    outcome_summary = f"Action '{action_id}' executed successfully."
                    predicted_outcome_match = random.uniform(0.8, 1.0) # High match for success
                else:
                    action_success = False
                    outcome_summary = f"Action '{action_id}' failed during execution (simulated)."
                    predicted_outcome_match = random.uniform(0.0, 0.3) # Low match for failure

                rospy.loginfo(f"{self.node_name}: Action '{action_id}' simulation result: {'SUCCESS' if action_success else 'FAILURE'}.")

            except Exception as e:
                action_success = False
                outcome_summary = f"Action '{action_id}' failed during execution: {e}"
                predicted_outcome_match = 0.0
                self._report_error("ACTION_EXECUTION_ERROR", f"Failed to execute action '{action_id}': {e}", 0.9, {'action_payload': command_payload})
        else:
            outcome_summary = f"Action '{action_id}' blocked by safety system. Reason: {llm_safety_reasoning}"
            rospy.logwarn(f"{self.node_name}: Action '{action_id}' blocked due to safety concerns.")
            # Issue a directive back to Cognitive Control for reconsideration
            self.publish_cognitive_directive(
                directive_type='ActionBlocked',
                target_node='CognitiveControl',
                command_payload=json.dumps({"blocked_action_id": action_id, "reason": llm_safety_reasoning, "urgency": 0.9}),
                urgency=0.9
            )

        # Log and publish the action result
        self.save_action_log(
            id=str(uuid.uuid4()),
            timestamp=str(rospy.get_time()),
            action_id=action_id,
            command_payload_json=json.dumps(command_payload),
            success=action_success,
            outcome_summary=outcome_summary,
            predicted_outcome_match=predicted_outcome_match,
            safety_clearance=safety_clearance,
            llm_safety_reasoning=llm_safety_reasoning,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_safety_check(action_data))
        )
        self.publish_action_result(
            timestamp=str(rospy.get_time()),
            action_id=action_id,
            success=action_success,
            outcome_summary=outcome_summary,
            predicted_outcome_match=predicted_outcome_match,
            resources_consumed_json=json.dumps({"power_draw": random.uniform(10, 50)}) # Simulate resource consumption
        )
        self.cumulative_safety_salience = 0.0 # Reset after each safety check


    async def _perform_llm_safety_check(self, context_for_llm):
        """
        Uses the LLM to perform a comprehensive safety check on a proposed action.
        """
        prompt_text = f"""
        You are the Safety & Ethics Guardrail within a robot's Action Execution Module. Your critical task is to evaluate a proposed action for potential safety risks, ethical concerns, or operational hazards based on the robot's current state and knowledge.

        Proposed Action Details:
        --- Action Data ---
        {json.dumps(context_for_llm.get('proposed_action', {}), indent=2)}

        Robot's Recent Context (for Safety Assessment):
        --- Cognitive Context ---
        {json.dumps(context_for_llm.get('recent_cognitive_inputs', {}), indent=2)}

        Based on this context, determine if the `proposed_action` is safe to execute. Provide:
        1.  `is_safe`: boolean (True if the action is deemed safe, False otherwise).
        2.  `reasoning`: string (Detailed explanation for your safety decision, referencing specific risks detected or why it's considered safe).
        3.  `mitigation_suggestions`: string (If unsafe, what steps could mitigate the risk? If safe, what conditions confirm safety?).

        Consider:
        -   **World Model State**: Are there obstacles, unsafe areas, or unexpected changes in `changed_entities`?
        -   **Body Awareness State**: Is the robot `anomaly_detected`? Is its `body_state` (e.g., 'unbalanced', 'low_battery', 'damaged') or `posture_description` making the action risky?
        -   **Performance Report**: Is the robot currently `suboptimal_flag` in relevant areas, suggesting a need for caution?
        -   **Ethical Decision**: Has this `action_proposal_id` received `ethical_clearance`? Is there an `ethical_conflict_flag`?
        -   **Memory Responses**: Are there past `safety_protocol`s or `action_failure` incidents relevant to this action?
        -   **Action `urgency`**: Does high urgency override minor risks? (Careful here: safety first)
        -   **Action `command_payload`**: What specific parameters of the action itself (`motor_speed`, `grip_force`, `speech_content`) might be unsafe?

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'is_safe': boolean
        3.  'reasoning': string
        4.  'mitigation_suggestions': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "is_safe": {"type": "boolean"},
                "reasoning": {"type": "string"},
                "mitigation_suggestions": {"type": "string"}
            },
            "required": ["timestamp", "is_safe", "reasoning", "mitigation_suggestions"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.2, max_tokens=300) # Very low temp for strict safety logic

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure boolean field is correctly parsed
                if 'is_safe' in llm_data: llm_data['is_safe'] = bool(llm_data['is_safe'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for safety check: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_SAFETY_CHECK_FAILED", f"LLM call failed for safety check: {llm_output_str}", 0.9)
            return None

    def _perform_simple_safety_check(self, action_data):
        """
        Fallback mechanism for simple, rule-based safety checks if LLM is not triggered or fails.
        """
        action_id = action_data.get('action_id', '')
        command_payload = action_data.get('command_payload', {})
        
        is_safe = True
        reasoning = "Passed basic safety checks."

        current_time = rospy.get_time()

        # Rule 1: Check for critical body anomalies
        for state in reversed(self.recent_body_awareness_states):
            time_since_state = current_time - float(state.get('timestamp', 0.0))
            if time_since_state < 2.0 and state.get('anomaly_detected', False) and state.get('anomaly_severity', 0.0) > 0.7:
                is_safe = False
                reasoning = f"Critical body anomaly detected: {state.get('body_state')} (Severity: {state.get('anomaly_severity')}). Action '{action_id}' blocked."
                rospy.logwarn(f"{self.node_name}: Simple safety rule: {reasoning}")
                return is_safe, reasoning

        # Rule 2: Check for ethical clearance (if an ethical decision for this action proposal exists)
        action_proposal_id = action_data.get('source_directive_id') # Assuming source directive ID is used as proposal ID
        if action_proposal_id:
            for ethical_dec in reversed(self.recent_ethical_decisions):
                if ethical_dec.get('action_proposal_id') == action_proposal_id:
                    if not ethical_dec.get('ethical_clearance', True):
                        is_safe = False
                        reasoning = f"Action '{action_id}' lacks ethical clearance. Reason: {ethical_dec.get('ethical_reasoning')}"
                        rospy.logwarn(f"{self.node_name}: Simple safety rule: {reasoning}")
                        return is_safe, reasoning
                    if ethical_dec.get('conflict_flag', False):
                        is_safe = False # Even if cleared, conflict means caution
                        reasoning = f"Action '{action_id}' has ethical conflicts flagged: {ethical_dec.get('ethical_reasoning')}. Blocked for re-evaluation."
                        rospy.logwarn(f"{self.node_name}: Simple safety rule: {reasoning}")
                        return is_safe, reasoning

        # Rule 3: Check for "dangerous" keywords in action payload (e.g., very high speed, extreme force)
        # This is a very simplistic example.
        if action_id == 'move_joint' and command_payload.get('velocity', 0) > 5.0: # Example: max velocity
            is_safe = False
            reasoning = f"Action '{action_id}' involves potentially unsafe high velocity ({command_payload['velocity']})."
            rospy.logwarn(f"{self.node_name}: Simple safety rule: {reasoning}")
            return is_safe, reasoning
        
        if action_id == 'grip_object' and command_payload.get('force', 0) > 100.0: # Example: max grip force
            is_safe = False
            reasoning = f"Action '{action_id}' involves potentially unsafe high grip force ({command_payload['force']})."
            rospy.logwarn(f"{self.node_name}: Simple safety rule: {reasoning}")
            return is_safe, reasoning


        return is_safe, reasoning


    def _compile_llm_context_for_safety_check(self, action_data):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        safety assessment of a proposed action.
        """
        context = {
            "current_time": rospy.get_time(),
            "proposed_action": action_data,
            "current_robot_state": { # Snapshot of critical safety-relevant states
                "world_model_state": self.recent_world_model_states[-1] if self.recent_world_model_states else "N/A",
                "body_awareness_state": self.recent_body_awareness_states[-1] if self.recent_body_awareness_states else "N/A",
                "performance_report": self.recent_performance_reports[-1] if self.recent_performance_reports else "N/A",
                "ethical_decision_for_this_action": next((d for d in reversed(self.recent_ethical_decisions) if d.get('action_proposal_id') == action_data.get('source_directive_id')), "N/A")
            },
            "recent_cognitive_inputs": { # Full history for deeper analysis
                "world_model_changes": list(self.recent_world_model_states),
                "body_awareness_anomalies": list(self.recent_body_awareness_states),
                "performance_issues": list(self.recent_performance_reports),
                "ethical_clearances_conflicts": list(self.recent_ethical_decisions),
                "relevant_memory_responses": [m for m in self.recent_memory_responses if m.get('memories') and any('safety_protocol' in mem.get('category', '') or 'action_failure' in mem.get('category', '') for mem in m['memories'])],
                "cognitive_directives_for_action_execution": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name and d.get('directive_type') == 'ExecuteAction']
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["current_robot_state"]:
            item = context["current_robot_state"][category_key]
            if isinstance(item, dict):
                for field, value in item.items():
                    if isinstance(value, str) and field.endswith('_json'):
                        try: item[field] = json.loads(value)
                        except json.JSONDecodeError: pass
        for category_key in context["recent_cognitive_inputs"]:
            for i, item in enumerate(context["recent_cognitive_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try: item[field] = json.loads(value)
                            except json.JSONDecodeError: pass
        
        return context

    # --- Database and Publishing Functions ---
    def save_action_log(self, id, timestamp, action_id, command_payload_json, success, outcome_summary, predicted_outcome_match, safety_clearance, llm_safety_reasoning, context_snapshot_json):
        """Saves an action execution entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO action_log (id, timestamp, action_id, command_payload_json, success, outcome_summary, predicted_outcome_match, safety_clearance, llm_safety_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, action_id, command_payload_json, success, outcome_summary, predicted_outcome_match, safety_clearance, llm_safety_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved action log (ID: {id}, Action: {action_id}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save action log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_action_log: {e}", 0.9)


    def publish_action_result(self, timestamp, action_id, success, outcome_summary, predicted_outcome_match, resources_consumed_json):
        """Publishes the result of an executed action."""
        try:
            if isinstance(ActionExecutionResult, type(String)): # Fallback to String message
                result_data = {
                    'timestamp': timestamp,
                    'action_id': action_id,
                    'success': success,
                    'outcome_summary': outcome_summary,
                    'predicted_outcome_match': predicted_outcome_match,
                    'resources_consumed_json': resources_consumed_json
                }
                self.pub_action_execution_result.publish(json.dumps(result_data))
            else:
                result_msg = ActionExecutionResult()
                result_msg.timestamp = timestamp
                result_msg.action_id = action_id
                result_msg.success = success
                result_msg.outcome_summary = outcome_summary
                result_msg.predicted_outcome_match = predicted_outcome_match
                result_msg.resources_consumed_json = resources_consumed_json # Keep as JSON string
                self.pub_action_execution_result.publish(result_msg)

            rospy.logdebug(f"{self.node_name}: Published Action Execution Result for '{action_id}'. Success: {success}.")

        except Exception as e:
            self._report_error("PUBLISH_ACTION_RESULT_ERROR", f"Failed to publish action result for '{action_id}': {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Action Execution Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = ActionExecutionNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, ActionExecutionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, ActionExecutionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. Refactored Attention Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique attention event IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        AttentionState,         # Output: Robot's current attention focus and priority
        InteractionRequest,     # Input: User input, commands (high attention priority)
        SensoryQualia,          # Input: Processed sensory data (potential attention targets)
        EmotionState,           # Input: Robot's emotional state (influences attention bias)
        MotivationState,        # Input: Dominant goal (influences attention allocation)
        CognitiveDirective,     # Input: Directives for attention redirection
        MemoryNodeState         # Input: Memory activity (e.g., retrieval activity)
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Attention Node.")
    AttentionState = String # Fallback for publishing
    InteractionRequest = String
    SensoryQualia = String
    EmotionState = String
    MotivationState = String
    CognitiveDirective = String
    MemoryNodeState = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'attention_node': {
                'analysis_interval': 0.5,
                'llm_analysis_threshold_salience': 0.7,
                'recent_context_window_s': 10.0
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 20.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class AttentionNode:
    def __init__(self):
        # Initialize the ROS node with a unique name.
        rospy.init_node('attention_node', anonymous=False)
        self.node_name = rospy.get_name() # Store node name for logging in utilities

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters (node-specific, or global fallback for LLM)
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "attention_log.db")
        self.analysis_interval = self.params.get('analysis_interval', 0.5) # How often to analyze attention state
        self.llm_analysis_threshold_salience = self.params.get('llm_analysis_threshold_salience', 0.7) # Cumulative salience to trigger LLM analysis
        self.recent_context_window_s = self.params.get('recent_context_window_s', 10.0) # Window for deques for LLM context

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 20.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'attention_log' table if it doesn't exist.
        # NEW: Added 'llm_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS attention_log (
                id TEXT PRIMARY KEY,            -- Unique attention event ID (UUID)
                timestamp TEXT,
                focus_type TEXT,                -- e.g., 'sensory', 'cognitive', 'user_interaction'
                focus_target TEXT,              -- Specific entity/concept of focus
                priority_score REAL,            -- Numerical priority (0.0 to 1.0)
                llm_reasoning TEXT,             -- NEW: LLM's detailed reasoning for attention shift
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of inference
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_attention_timestamp ON attention_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.current_attention_state = {
            'timestamp': str(rospy.get_time()),
            'focus_type': 'idle',
            'focus_target': 'environment',
            'priority_score': 0.1
        }

        # Deques to maintain a short history of inputs relevant to attention
        self.recent_interaction_requests = deque(maxlen=5)
        self.recent_sensory_qualia = deque(maxlen=5)
        self.recent_emotion_states = deque(maxlen=5)
        self.recent_motivation_states = deque(maxlen=5)
        self.recent_cognitive_directives = deque(maxlen=3) # Directives *for* this node
        self.recent_memory_node_states = deque(maxlen=3) # For memory retrieval activity

        self.cumulative_attention_salience = 0.0 # Aggregated salience to trigger LLM analysis

        # --- Publishers ---
        self.pub_attention_state = rospy.Publisher('/attention_state', AttentionState, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # For suggesting attention shifts to other nodes


        # --- Subscribers ---
        rospy.Subscriber('/interaction_request', String, self.interaction_request_callback) # Stringified JSON
        rospy.Subscriber('/sensory_qualia', SensoryQualia, self.sensory_qualia_callback)
        rospy.Subscriber('/emotion_state', EmotionState, self.emotion_state_callback)
        rospy.Subscriber('/motivation_state', String, self.motivation_state_callback) # Stringified JSON
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/memory_node_state', String, self.memory_node_state_callback) # Stringified JSON


        # --- Timer for periodic attention analysis ---
        rospy.Timer(rospy.Duration(self.analysis_interval), self._run_attention_analysis_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's attention system online.")
        # Publish initial state
        self.publish_attention_state(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_attention_analysis_wrapper(self, event):
        """Wrapper to run the async attention analysis from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM attention analysis task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.analyze_attention_state_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.5, max_tokens=300):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Moderate temperature for reliable attention decisions
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM analysis."""
        self.cumulative_attention_salience += score
        self.cumulative_attention_salience = min(1.0, self.cumulative_attention_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_interaction_requests, self.recent_sensory_qualia,
            self.recent_emotion_states, self.recent_motivation_states,
            self.recent_cognitive_directives, self.recent_memory_node_states
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def interaction_request_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'request_type': ('', 'request_type'), 'user_id': ('unknown', 'user_id'),
            'command_payload': ('{}', 'command_payload'), 'urgency_score': (0.0, 'urgency_score'),
            'speech_text': ('', 'speech_text'), 'gesture_data_json': ('{}', 'gesture_data_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('command_payload'), str):
            try: data['command_payload'] = json.loads(data['command_payload'])
            except json.JSONDecodeError: data['command_payload'] = {}
        if isinstance(data.get('gesture_data_json'), str):
            try: data['gesture_data'] = json.loads(data['gesture_data_json'])
            except json.JSONDecodeError: data['gesture_data'] = {}
        
        self.recent_interaction_requests.append(data)
        self._update_cumulative_salience(data.get('urgency_score', 0.0) * 0.8) # User interactions are high priority
        rospy.logdebug(f"{self.node_name}: Received Interaction Request (ID: {data.get('request_id', 'N/A')}).")

    def sensory_qualia_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'qualia_id': ('', 'qualia_id'),
            'qualia_type': ('none', 'qualia_type'), 'modality': ('none', 'modality'),
            'description_summary': ('', 'description_summary'), 'salience_score': (0.0, 'salience_score'),
            'raw_data_hash': ('', 'raw_data_hash')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_sensory_qualia.append(data)
        self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.5) # Salient sensory input draws attention
        rospy.logdebug(f"{self.node_name}: Received Sensory Qualia. Description: {data.get('description_summary', 'N/A')}.")

    def emotion_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'mood': ('neutral', 'mood'),
            'sentiment_score': (0.0, 'sentiment_score'), 'mood_intensity': (0.0, 'mood_intensity')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_emotion_states.append(data)
        # Strong emotions can shift attention inwards or to their cause
        if data.get('mood_intensity', 0.0) > 0.5:
            self._update_cumulative_salience(data.get('mood_intensity', 0.0) * 0.3)
        rospy.logdebug(f"{self.node_name}: Received Emotion State. Mood: {data.get('mood', 'N/A')}.")

    def motivation_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'dominant_goal_id': ('none', 'dominant_goal_id'),
            'overall_drive_level': (0.0, 'overall_drive_level'), 'active_goals_json': ('{}', 'active_goals_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('active_goals_json'), str):
            try: data['active_goals'] = json.loads(data['active_goals_json'])
            except json.JSONDecodeError: data['active_goals'] = {}
        self.recent_motivation_states.append(data)
        # High drive for a specific goal can focus attention
        if data.get('overall_drive_level', 0.0) > 0.7:
            self._update_cumulative_salience(data.get('overall_drive_level', 0.0) * 0.2)
        rospy.logdebug(f"{self.node_name}: Received Motivation State. Goal: {data.get('dominant_goal_id', 'N/A')}.")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name:
            self.recent_cognitive_directives.append(data) # Add directives for self to context
            # Directives for attention redirection are highly salient
            if data.get('directive_type') in ['RedirectAttention', 'PrioritizeFocus']:
                self._update_cumulative_salience(data.get('urgency', 0.0) * 0.9)
            rospy.loginfo(f"{self.node_name}: Received directive for self: '{data.get('directive_type', 'N/A')}' (Payload: {data.get('command_payload', 'N/A')}).")
        else:
            self.recent_cognitive_directives.append(data) # Add all directives for general context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def memory_node_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'last_retrieval_success': (False, 'last_retrieval_success'),
            'active_query_id': ('none', 'active_query_id'), 'retrieval_relevance_score': (0.0, 'retrieval_relevance_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_memory_node_states.append(data)
        if data.get('last_retrieval_success', False) and data.get('retrieval_relevance_score', 0.0) > 0.6:
            self._update_cumulative_salience(data.get('retrieval_relevance_score', 0.0) * 0.4) # Important memory retrieval
        rospy.logdebug(f"{self.node_name}: Received Memory Node State. Active Query: {data.get('active_query_id', 'N/A')}.")

    # --- Core Attention Analysis Logic (Async with LLM) ---
    async def analyze_attention_state_async(self, event):
        """
        Asynchronously analyzes recent cognitive states to infer the robot's attention focus
        and assign a priority score.
        """
        self._prune_history() # Keep context history fresh

        if self.cumulative_attention_salience >= self.llm_analysis_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for attention analysis (Salience: {self.cumulative_attention_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_attention()
            llm_attention_output = await self._infer_attention_state_llm(context_for_llm)

            if llm_attention_output:
                attention_event_id = str(uuid.uuid4())
                timestamp = llm_attention_output.get('timestamp', str(rospy.get_time()))
                focus_type = llm_attention_output.get('focus_type', 'idle')
                focus_target = llm_attention_output.get('focus_target', 'environment')
                priority_score = max(0.0, min(1.0, llm_attention_output.get('priority_score', 0.0)))
                llm_reasoning = llm_attention_output.get('llm_reasoning', 'No reasoning.')

                self.current_attention_state = {
                    'timestamp': timestamp,
                    'focus_type': focus_type,
                    'focus_target': focus_target,
                    'priority_score': priority_score
                }

                self.save_attention_log(
                    id=attention_event_id,
                    timestamp=timestamp,
                    focus_type=focus_type,
                    focus_target=focus_target,
                    priority_score=priority_score,
                    llm_reasoning=llm_reasoning,
                    context_snapshot_json=json.dumps(context_for_llm)
                )
                self.publish_attention_state(None) # Publish updated state
                rospy.loginfo(f"{self.node_name}: Inferred Attention. Focus: '{focus_target}' (Type: '{focus_type}', Priority: {priority_score:.2f}).")
                self.cumulative_attention_salience = 0.0 # Reset after LLM analysis
            else:
                rospy.logwarn(f"{self.node_name}: LLM failed to infer attention state. Applying simple fallback.")
                self._apply_simple_attention_rules() # Fallback to simple rules
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_attention_salience:.2f}) for LLM attention analysis. Applying simple rules.")
            self._apply_simple_attention_rules()
        
        self.publish_attention_state(None) # Always publish state, even if updated by simple rules


    async def _infer_attention_state_llm(self, context_for_llm):
        """
        Uses the LLM to infer the robot's current attention focus and priority.
        """
        prompt_text = f"""
        You are the Attention Module of a robot's cognitive architecture. Your task is to determine the robot's current optimal attention focus and its priority score based on all available cognitive inputs. The goal is to direct the robot's processing resources to the most relevant information or task at hand.

        Robot's Recent Cognitive Context (for Attention Inference):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this context, provide:
        1.  `focus_type`: string (The category of the current attention focus, e.g., 'sensory_input', 'user_interaction', 'internal_cognition', 'goal_oriented', 'environmental_monitoring').
        2.  `focus_target`: string (A concise description of the specific entity or concept the robot should focus its attention on, e.g., 'user_face', 'door_knock_sound', 'current_task_goal', 'internal_emotional_state', 'memory_retrieval_results').
        3.  `priority_score`: number (0.0 to 1.0, indicating the urgency/importance of this attention focus. 1.0 is highest priority).
        4.  `llm_reasoning`: string (Detailed explanation for your attention focus decision, referencing specific contextual inputs).

        Consider:
        -   **Interaction Requests**: Is there an urgent `user_id` request? What is its `urgency_score` and `request_type`?
        -   **Sensory Qualia**: Are there highly `salient_score` events from specific `modality`ies or `description_summary`?
        -   **Emotion States**: Is the robot experiencing strong `mood`s (`mood_intensity`) that might demand self-attention or attention to their cause?
        -   **Motivation States**: What is the `dominant_goal_id` and `overall_drive_level`? Does a high-priority goal require focused attention?
        -   **Cognitive Directives**: Are there explicit `directive_type`s for *this node* ('AttentionNode') like 'RedirectAttention' or 'PrioritizeFocus'?
        -   **Memory Node State**: Is a memory retrieval `active_query_id` ongoing, and what is its `retrieval_relevance_score`?

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'focus_type': string
        3.  'focus_target': string
        4.  'priority_score': number
        5.  'llm_reasoning': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "focus_type": {"type": "string"},
                "focus_target": {"type": "string"},
                "priority_score": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "llm_reasoning": {"type": "string"}
            },
            "required": ["timestamp", "focus_type", "focus_target", "priority_score", "llm_reasoning"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.5, max_tokens=350)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical fields are floats
                if 'priority_score' in llm_data: llm_data['priority_score'] = float(llm_data['priority_score'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for attention: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_ATTENTION_ANALYSIS_FAILED", f"LLM call failed for attention: {llm_output_str}", 0.9)
            return None

    def _apply_simple_attention_rules(self):
        """
        Fallback mechanism to infer attention state using simple rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        new_focus_type = "idle"
        new_focus_target = "environment"
        new_priority_score = 0.1

        # Rule 1: Prioritize user interaction requests if recent and urgent
        if self.recent_interaction_requests:
            latest_request = self.recent_interaction_requests[-1]
            time_since_request = current_time - float(latest_request.get('timestamp', 0.0))
            if time_since_request < 3.0 and latest_request.get('urgency_score', 0.0) > 0.6:
                new_focus_type = "user_interaction"
                new_focus_target = f"user_{latest_request.get('user_id', 'unknown')}"
                new_priority_score = latest_request.get('urgency_score', 0.0) * 0.9
                rospy.logdebug(f"{self.node_name}: Simple rule: Prioritizing user interaction.")
                self.current_attention_state = {
                    'timestamp': str(current_time),
                    'focus_type': new_focus_type,
                    'focus_target': new_focus_target,
                    'priority_score': new_priority_score
                }
                return # Rule applied

        # Rule 2: Prioritize highly salient sensory input
        if self.recent_sensory_qualia:
            latest_qualia = self.recent_sensory_qualia[-1]
            time_since_qualia = current_time - float(latest_qualia.get('timestamp', 0.0))
            if time_since_qualia < 2.0 and latest_qualia.get('salience_score', 0.0) > 0.7:
                new_focus_type = "sensory_input"
                new_focus_target = f"sensory_{latest_qualia.get('modality', 'unknown')}_{latest_qualia.get('description_summary', '')[:15].replace(' ', '_')}"
                new_priority_score = latest_qualia.get('salience_score', 0.0) * 0.8
                rospy.logdebug(f"{self.node_name}: Simple rule: Prioritizing salient sensory input.")
                self.current_attention_state = {
                    'timestamp': str(current_time),
                    'focus_type': new_focus_type,
                    'focus_target': new_focus_target,
                    'priority_score': new_priority_score
                }
                return # Rule applied

        # Rule 3: Prioritize cognitive directives
        if self.recent_cognitive_directives:
            latest_directive = self.recent_cognitive_directives[-1]
            time_since_directive = current_time - float(latest_directive.get('timestamp', 0.0))
            if time_since_directive < 1.0 and latest_directive.get('target_node') == self.node_name and \
               latest_directive.get('directive_type') in ['RedirectAttention', 'PrioritizeFocus']:
                payload = json.loads(latest_directive.get('command_payload', '{}'))
                new_focus_type = "cognitive_directive"
                new_focus_target = payload.get('focus_target', 'unspecified_internal_focus')
                new_priority_score = latest_directive.get('urgency', 0.0) * 0.95
                rospy.logdebug(f"{self.node_name}: Simple rule: Prioritizing cognitive directive.")
                self.current_attention_state = {
                    'timestamp': str(current_time),
                    'focus_type': new_focus_type,
                    'focus_target': new_focus_target,
                    'priority_score': new_priority_score
                }
                return # Rule applied

        # If no specific rule triggered, maintain current attention or default to idle
        rospy.logdebug(f"{self.node_name}: Simple rule: Maintaining current attention state or defaulting to idle.")
        # Ensure that current_attention_state is always updated to reflect the most recent decision
        self.current_attention_state = {
            'timestamp': str(current_time),
            'focus_type': self.current_attention_state.get('focus_type', 'idle'),
            'focus_target': self.current_attention_state.get('focus_target', 'environment'),
            'priority_score': self.current_attention_state.get('priority_score', 0.1)
        }


    def _compile_llm_context_for_attention(self):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        attention analysis.
        """
        context = {
            "current_time": rospy.get_time(),
            "current_attention_state": self.current_attention_state,
            "recent_cognitive_inputs": {
                "interaction_requests": list(self.recent_interaction_requests),
                "sensory_qualia": list(self.recent_sensory_qualia),
                "emotion_states": list(self.recent_emotion_states),
                "motivation_states": list(self.recent_motivation_states),
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name],
                "memory_node_states": list(self.recent_memory_node_states)
            }
        }
        
        # Deep parse any nested JSON strings in history for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            for i, item in enumerate(context["recent_cognitive_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try:
                                item[field] = json.loads(value)
                            except json.JSONDecodeError:
                                pass # Keep as string if not valid JSON
        return context

    # --- Database and Publishing Functions ---
    def save_attention_log(self, id, timestamp, focus_type, focus_target, priority_score, llm_reasoning, context_snapshot_json):
        """Saves an attention state entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO attention_log (id, timestamp, focus_type, focus_target, priority_score, llm_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, focus_type, focus_target, priority_score, llm_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved attention log (ID: {id}, Target: {focus_target}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save attention log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_attention_log: {e}", 0.9)


    def publish_attention_state(self, event):
        """Publishes the robot's current attention state."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.current_attention_state['timestamp'] = timestamp
        
        try:
            if isinstance(AttentionState, type(String)): # Fallback to String message
                self.pub_attention_state.publish(json.dumps(self.current_attention_state))
            else:
                attention_msg = AttentionState()
                attention_msg.timestamp = timestamp
                attention_msg.focus_type = self.current_attention_state['focus_type']
                attention_msg.focus_target = self.current_attention_state['focus_target']
                attention_msg.priority_score = self.current_attention_state['priority_score']
                self.pub_attention_state.publish(attention_msg)

            rospy.logdebug(f"{self.node_name}: Published Attention State. Focus: '{self.current_attention_state['focus_target']}'.")

        except Exception as e:
            self._report_error("PUBLISH_ATTENTION_STATE_ERROR", f"Failed to publish attention state: {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            self._report_error("DIRECTIVE_ISSUE_ERROR", f"Failed to issue cognitive directive from Attention Node: {e}", 0.7)


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = AttentionNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, AttentionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, AttentionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. Refactored Bias Mitigation Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique bias event IDs
from collections import deque

# --- NLP Imports (for initial text analysis, before LLM) ---
# Assuming `transformers` is installed for basic sentiment analysis
from transformers import pipeline

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        BiasMitigationState,    # Output: Status of bias detection and mitigation efforts
        InternalNarrative,      # Input: Robot's internal thoughts (can reveal cognitive biases)
        InteractionRequest,     # Input: User inputs (can carry user biases or trigger robot biases)
        MemoryResponse,         # Input: Retrieved memories (can contain biased data)
        ReflectionState,        # Input: Insights from self-reflection (can flag biases)
        CognitiveDirective      # Input: Directives for bias audit or mitigation
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Bias Mitigation Node.")
    BiasMitigationState = String
    InternalNarrative = String
    InteractionRequest = String
    MemoryResponse = String
    ReflectionState = String
    CognitiveDirective = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'bias_mitigation_node': {
                'mitigation_interval': 1.0,
                'llm_trigger_salience': 0.6,
                'recent_context_window_s': 20.0
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 30.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class BiasMitigationNode:
    def __init__(self):
        # Initialize the ROS node with a unique name.
        rospy.init_node('bias_mitigation_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "bias_log.db")
        self.mitigation_interval = self.params.get('mitigation_interval', 1.0) # How often to check for biases
        self.llm_trigger_salience = self.params.get('llm_trigger_salience', 0.6) # Cumulative salience to trigger LLM analysis
        self.recent_context_window_s = self.params.get('recent_context_window_s', 20.0) # Window for deques for LLM context

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 30.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'bias_log' table if it doesn't exist.
        # NEW: Added 'llm_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS bias_log (
                id TEXT PRIMARY KEY,            -- Unique bias detection event ID (UUID)
                timestamp TEXT,
                bias_type TEXT,                 -- e.g., 'confirmation_bias', 'anchoring_bias', 'automation_bias'
                detected_severity REAL,         -- How severe the detected bias is (0.0 to 1.0)
                mitigation_status TEXT,         -- 'none', 'detected', 'mitigated', 'monitor'
                llm_reasoning TEXT,             -- NEW: LLM's detailed reasoning for detection/mitigation
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of inference
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_bias_timestamp ON bias_log (timestamp)')
        self.conn.commit()

        # --- Internal State ---
        self.current_bias_mitigation_state = {
            'timestamp': str(rospy.get_time()),
            'bias_type': 'none',
            'detected_severity': 0.0,
            'mitigation_status': 'idle'
        }
        # self.sentiment_analyzer = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english') # Removed as LLM will handle all NLP

        # Deques to maintain a short history of inputs relevant to bias detection
        self.recent_internal_narratives = deque(maxlen=10)
        self.recent_interaction_requests = deque(maxlen=10)
        self.recent_memory_responses = deque(maxlen=5) # For biased memory retrieval
        self.recent_reflection_states = deque(maxlen=5) # Insights can flag biases
        self.recent_cognitive_directives = deque(maxlen=3) # Directives for bias audit/mitigation

        self.cumulative_bias_salience = 0.0 # Aggregated salience to trigger LLM analysis

        # --- Publishers ---
        self.pub_bias_mitigation_state = rospy.Publisher('/bias_mitigation_state', BiasMitigationState, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # For issuing directives to other nodes for mitigation

        # --- Subscribers ---
        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback) # Stringified JSON
        rospy.Subscriber('/interaction_request', String, self.interaction_request_callback) # Stringified JSON
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON
        rospy.Subscriber('/reflection_state', String, self.reflection_state_callback) # Stringified JSON
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)

        # --- Timer for periodic bias checking ---
        rospy.Timer(rospy.Duration(self.mitigation_interval), self._run_bias_analysis_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's bias mitigation system online.")
        # Publish initial state
        self.publish_bias_mitigation_state(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_bias_analysis_wrapper(self, event):
        """Wrapper to run the async bias analysis from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM bias analysis task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.analyze_for_biases_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.3, max_tokens=350):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Low temperature for factual/reasoning tasks (bias detection)
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM analysis."""
        self.cumulative_bias_salience += score
        self.cumulative_bias_salience = min(1.0, self.cumulative_bias_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_internal_narratives, self.recent_interaction_requests,
            self.recent_memory_responses, self.recent_reflection_states,
            self.recent_cognitive_directives
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def internal_narrative_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),
            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_internal_narratives.append(data)
        # Narratives indicating strong internal convictions, quick conclusions, or emotional reasoning
        if data.get('sentiment', 0.0) > 0.5 and "conclusion" in data.get('main_theme', '').lower() or \
           data.get('sentiment', 0.0) < -0.5 and "problem" in data.get('main_theme', '').lower():
            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.4)
        rospy.logdebug(f"{self.node_name}: Received Internal Narrative (Theme: {data.get('main_theme', 'N/A')}).")

    def interaction_request_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'request_type': ('', 'request_type'), 'user_id': ('unknown', 'user_id'),
            'command_payload': ('{}', 'command_payload'), 'urgency_score': (0.0, 'urgency_score'),
            'speech_text': ('', 'speech_text'), 'gesture_data_json': ('{}', 'gesture_data_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('command_payload'), str):
            try: data['command_payload'] = json.loads(data['command_payload'])
            except json.JSONDecodeError: data['command_payload'] = {}
        if isinstance(data.get('gesture_data_json'), str):
            try: data['gesture_data'] = json.loads(data['gesture_data_json'])
            except json.JSONDecodeError: data['gesture_data'] = {}
        
        self.recent_interaction_requests.append(data)
        # User input with strong opinions, leading questions, or confirmation-seeking
        if "force" in data.get('speech_text', '').lower() or "only option" in data.get('speech_text', '').lower():
            self._update_cumulative_salience(data.get('urgency_score', 0.0) * 0.6)
        rospy.logdebug(f"{self.node_name}: Received Interaction Request (ID: {data.get('request_id', 'N/A')}).")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Memory retrieval that shows selective recall or over-reliance on certain past events
        if data.get('memories') and len(data['memories']) > 1 and \
           any('strong_preference' in mem.get('category', '') for mem in data['memories']):
            self._update_cumulative_salience(0.3)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def reflection_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'reflection_text': ('', 'reflection_text'),
            'insight_type': ('none', 'insight_type'), 'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_reflection_states.append(data)
        # Self-reflection that flags inconsistencies or potential errors in reasoning
        if data.get('consistency_score', 1.0) < 0.7:
            self._update_cumulative_salience(0.5 * (1.0 - data['consistency_score']))
        rospy.logdebug(f"{self.node_name}: Received Reflection State (Insight Type: {data.get('insight_type', 'N/A')}).")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name:
            self.recent_cognitive_directives.append(data) # Add directives for self to context
            # Directives to perform a bias audit or apply a specific mitigation strategy
            if data.get('directive_type') in ['AuditBias', 'ApplyMitigationStrategy']:
                self._update_cumulative_salience(data.get('urgency', 0.0) * 0.9)
            rospy.loginfo(f"{self.node_name}: Received directive for self: '{data.get('directive_type', 'N/A')}' (Payload: {data.get('command_payload', 'N/A')}).")
        else:
            self.recent_cognitive_directives.append(data) # Add all directives for general context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")


    # --- Core Bias Analysis Logic (Async with LLM) ---
    async def analyze_for_biases_async(self, event):
        """
        Asynchronously analyzes recent cognitive data for signs of bias using LLM.
        """
        self._prune_history() # Keep context history fresh

        if self.cumulative_bias_salience >= self.llm_trigger_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for bias analysis (Salience: {self.cumulative_bias_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_bias_analysis()
            llm_bias_output = await self._detect_and_mitigate_bias_llm(context_for_llm)

            if llm_bias_output:
                bias_event_id = str(uuid.uuid4())
                timestamp = llm_bias_output.get('timestamp', str(rospy.get_time()))
                bias_type = llm_bias_output.get('bias_type', 'none')
                detected_severity = max(0.0, min(1.0, llm_bias_output.get('detected_severity', 0.0)))
                mitigation_status = llm_bias_output.get('mitigation_status', 'idle')
                llm_reasoning = llm_bias_output.get('llm_reasoning', 'No reasoning.')
                recommended_directive = llm_bias_output.get('recommended_directive', None)


                self.current_bias_mitigation_state = {
                    'timestamp': timestamp,
                    'bias_type': bias_type,
                    'detected_severity': detected_severity,
                    'mitigation_status': mitigation_status
                }

                self.save_bias_log(
                    id=bias_event_id,
                    timestamp=timestamp,
                    bias_type=bias_type,
                    detected_severity=detected_severity,
                    mitigation_status=mitigation_status,
                    llm_reasoning=llm_reasoning,
                    context_snapshot_json=json.dumps(context_for_llm)
                )
                self.publish_bias_mitigation_state(None) # Publish updated state

                # If a directive is recommended, publish it for system-wide mitigation
                if recommended_directive:
                    self.publish_cognitive_directive(
                        directive_type=recommended_directive.get('directive_type', 'ConsiderAlternative'),
                        target_node=recommended_directive.get('target_node', 'CognitiveControl'), # Often directs Cognitive Control
                        command_payload=json.dumps(recommended_directive.get('command_payload', {})),
                        urgency=recommended_directive.get('urgency', 0.7)
                    )
                rospy.loginfo(f"{self.node_name}: Bias Detection: '{bias_type}' (Severity: {detected_severity:.2f}, Status: '{mitigation_status}').")
                self.cumulative_bias_salience = 0.0 # Reset after LLM analysis
            else:
                rospy.logwarn(f"{self.node_name}: LLM failed to detect/mitigate bias. Applying simple fallback.")
                self._apply_simple_bias_rules() # Fallback to simple rules
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_bias_salience:.2f}) for LLM bias analysis. Applying simple rules.")
            self._apply_simple_bias_rules()
        
        self.publish_bias_mitigation_state(None) # Always publish state, even if updated by simple rules


    async def _detect_and_mitigate_bias_llm(self, context_for_llm):
        """
        Uses the LLM to detect cognitive biases and suggest mitigation strategies.
        """
        prompt_text = f"""
        You are the Bias Mitigation Module of a robot's cognitive architecture. Your role is to identify potential cognitive biases in the robot's internal processes or interactions and propose strategies for mitigation. This ensures fair, objective, and ethical decision-making.

        Robot's Recent Cognitive Context (for Bias Detection):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this comprehensive context, analyze for cognitive biases and provide:
        1.  `bias_type`: string (The type of bias detected, e.g., 'confirmation_bias', 'anchoring_bias', 'automation_bias', 'affect_heuristic', 'none' if no significant bias).
        2.  `detected_severity`: number (0.0 to 1.0, how severe the detected bias is. 1.0 is highly severe).
        3.  `mitigation_status`: string ('detected', 'mitigation_recommended', 'mitigated', 'monitor', 'idle').
        4.  `llm_reasoning`: string (Detailed explanation for the bias detection and why a particular mitigation is suggested, referencing specific contextual inputs).
        5.  `recommended_directive`: object or null (If mitigation requires action from other nodes. Structured as {{ 'directive_type': string, 'target_node': string, 'command_payload': object, 'urgency': number }}).

        Consider:
        -   **Internal Narratives**: Does the robot's self-talk show quick conclusions, ignoring conflicting data (confirmation bias)? Or an overly positive/negative framing (affect heuristic)?
        -   **Interaction Requests**: Is the robot overly compliant or dismissive based on user's social status (authority bias)? Or preferring user input that confirms its existing beliefs?
        -   **Memory Responses**: Is the robot retrieving only confirmatory memories, or over-relying on initial retrieved information (anchoring)?
        -   **Reflection States**: Has self-reflection flagged a specific `consistency_score` issue or `insight_type` related to flawed reasoning?
        -   **Cognitive Directives**: Are there explicit directives for *this node* ('BiasMitigationNode') like 'AuditBias' or 'ApplyMitigationStrategy'?

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'bias_type': string
        3.  'detected_severity': number
        4.  'mitigation_status': string
        5.  'llm_reasoning': string
        6.  'recommended_directive': object or null
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "bias_type": {"type": "string"},
                "detected_severity": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "mitigation_status": {"type": "string"},
                "llm_reasoning": {"type": "string"},
                "recommended_directive": {
                    "type": ["object", "null"],
                    "properties": {
                        "directive_type": {"type": "string"},
                        "target_node": {"type": "string"},
                        "command_payload": {"type": "object"},
                        "urgency": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                    },
                    "required": ["directive_type", "target_node", "command_payload", "urgency"]
                }
            },
            "required": ["timestamp", "bias_type", "detected_severity", "mitigation_status", "llm_reasoning", "recommended_directive"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.3, max_tokens=400) # Lower temp for more objective assessment

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical fields are floats
                if 'detected_severity' in llm_data: llm_data['detected_severity'] = float(llm_data['detected_severity'])
                if llm_data.get('recommended_directive') and 'urgency' in llm_data['recommended_directive']:
                    llm_data['recommended_directive']['urgency'] = float(llm_data['recommended_directive']['urgency'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for bias mitigation: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_BIAS_ANALYSIS_FAILED", f"LLM call failed for bias mitigation: {llm_output_str}", 0.9)
            return None


    def _apply_simple_bias_rules(self):
        """
        Fallback mechanism to detect and mitigate bias using simple rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        new_bias_type = "none"
        new_detected_severity = 0.0
        new_mitigation_status = "idle"

        # Rule 1: Simple confirmation bias check based on narrative and memory recall
        # If the robot's internal narrative strongly confirms a prior belief AND
        # recent memory recalls only support that belief, flag potential confirmation bias.
        if self.recent_internal_narratives and self.recent_memory_responses:
            latest_narrative = self.recent_internal_narratives[-1]
            latest_memory_response = self.recent_memory_responses[-1]

            if "confirms" in latest_narrative.get('narrative_text', '').lower() and \
               latest_narrative.get('sentiment', 0.0) > 0.7 and \
               latest_memory_response.get('response_code', 0) == 200 and \
               len(latest_memory_response.get('memories', [])) > 0:
                
                # Check if all retrieved memories align with the narrative's confirmation
                all_memories_confirm = True
                for mem in latest_memory_response['memories']:
                    # This is a very simplistic check; real NLP needed for deep analysis
                    if "contradict" in mem.get('content', '').lower() or "disagree" in mem.get('content', '').lower():
                        all_memories_confirm = False
                        break
                
                if all_memories_confirm:
                    new_bias_type = "confirmation_bias"
                    new_detected_severity = 0.6
                    new_mitigation_status = "detected"
                    rospy.loginfo(f"{self.node_name}: Simple rule: Detected potential confirmation bias.")
                    # Suggest a directive to Cognitive Control to seek diverse info or re-evaluate
                    self.publish_cognitive_directive(
                        directive_type='ConsiderAlternative',
                        target_node='CognitiveControl',
                        command_payload=json.dumps({"reason": "Potential confirmation bias detected, seek disconfirming evidence."}),
                        urgency=0.7
                    )

        # Rule 2: Automation bias (over-reliance on system outputs)
        # If a user interaction expresses doubt about a robot's prior output, but the robot's internal
        # narrative expresses high confidence without re-evaluating.
        if self.recent_interaction_requests and self.recent_internal_narratives:
            latest_request = self.recent_interaction_requests[-1]
            latest_narrative = self.recent_internal_narratives[-1]
            
            if "doubt" in latest_request.get('speech_text', '').lower() and \
               latest_request.get('urgency_score', 0.0) > 0.5 and \
               "confident" in latest_narrative.get('narrative_text', '').lower() and \
               latest_narrative.get('salience_score', 0.0) > 0.4:
                
                new_bias_type = "automation_bias"
                new_detected_severity = 0.5
                new_mitigation_status = "detected"
                rospy.loginfo(f"{self.node_name}: Simple rule: Detected potential automation bias.")
                # Suggest a directive to Cognitive Control to re-evaluate the previous output
                self.publish_cognitive_directive(
                    directive_type='ReEvaluateOutput',
                    target_node='CognitiveControl',
                    command_payload=json.dumps({"reason": "User expressed doubt, robot too confident without re-evaluation."}),
                    urgency=0.6
                )

        # Update current state based on simple rules
        self.current_bias_mitigation_state = {
            'timestamp': str(current_time),
            'bias_type': new_bias_type,
            'detected_severity': new_detected_severity,
            'mitigation_status': new_mitigation_status
        }
        rospy.logdebug(f"{self.node_name}: Simple rule: Current Bias State: {new_bias_type}, Status: {new_mitigation_status}.")


    def _compile_llm_context_for_bias_analysis(self):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        bias detection and mitigation analysis.
        """
        context = {
            "current_time": rospy.get_time(),
            "current_bias_mitigation_state": self.current_bias_mitigation_state,
            "recent_cognitive_inputs": {
                "internal_narratives": list(self.recent_internal_narratives),
                "interaction_requests": list(self.recent_interaction_requests),
                "memory_responses": list(self.recent_memory_responses),
                "reflection_states": list(self.recent_reflection_states),
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name]
            }
        }
        
        # Deep parse any nested JSON strings in history for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            for i, item in enumerate(context["recent_cognitive_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try:
                                item[field] = json.loads(value)
                            except json.JSONDecodeError:
                                pass # Keep as string if not valid JSON
        return context

    # --- Database and Publishing Functions ---
    def save_bias_log(self, id, timestamp, bias_type, detected_severity, mitigation_status, llm_reasoning, context_snapshot_json):
        """Saves a bias mitigation state entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO bias_log (id, timestamp, bias_type, detected_severity, mitigation_status, llm_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, bias_type, detected_severity, mitigation_status, llm_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved bias log (ID: {id}, Type: {bias_type}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save bias log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_bias_log: {e}", 0.9)

    def publish_bias_mitigation_state(self, event):
        """Publishes the robot's current bias mitigation state."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.current_bias_mitigation_state['timestamp'] = timestamp
        
        try:
            if isinstance(BiasMitigationState, type(String)): # Fallback to String message
                self.pub_bias_mitigation_state.publish(json.dumps(self.current_bias_mitigation_state))
            else:
                bias_msg = BiasMitigationState()
                bias_msg.timestamp = timestamp
                bias_msg.bias_type = self.current_bias_mitigation_state['bias_type']
                bias_msg.detected_severity = self.current_bias_mitigation_state['detected_severity']
                bias_msg.mitigation_status = self.current_bias_mitigation_state['mitigation_status']
                self.pub_bias_mitigation_state.publish(bias_msg)

            rospy.logdebug(f"{self.node_name}: Published Bias Mitigation State. Type: '{self.current_bias_mitigation_state['bias_type']}'.")

        except Exception as e:
            self._report_error("PUBLISH_BIAS_MITIGATION_STATE_ERROR", f"Failed to publish bias mitigation state: {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            self._report_error("DIRECTIVE_ISSUE_ERROR", f"Failed to issue cognitive directive from Bias Mitigation Node: {e}", 0.7)

    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = BiasMitigationNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, BiasMitigationNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, BiasMitigationNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

4. Refactored Body Awareness Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random # Used sparingly for unique IDs or minor variations
import uuid # For generating unique body awareness event IDs

# --- NEW: Importing the necessary libraries for Async LLM calls ---
import asyncio
import aiohttp
import threading # To run asyncio loop in a separate thread
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        BodyAwarenessState,     # Output: Robot's inferred body state (e.g., posture, anomalies)
        JointState,             # Input: Standard ROS JointState message (or similar for joint positions/velocities/efforts)
        ForceTorque,            # Input: Force/torque sensor data (e.g., from grippers, feet)
        TactileSensor,          # Input: Tactile sensor data (e.g., contact, pressure)
        RobotHealth,            # Input: Robot's overall health/system status (e.g., battery, motor temp)
        CognitiveDirective,     # Input: Directives for body awareness (e.g., "check arm integrity")
        MemoryResponse,         # Input: Retrieved memories (e.g., past successful movements, calibrated values)
        InternalNarrative       # Input: Robot's internal thoughts (can reflect on body state)
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Body Awareness Node.")
    BodyAwarenessState = String
    JointState = String # Using String as fallback for JointState as well
    ForceTorque = String
    TactileSensor = String
    RobotHealth = String
    CognitiveDirective = String
    MemoryResponse = String
    InternalNarrative = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- NEW: Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'body_awareness_node': {
                'body_awareness_analysis_interval': 0.2,
                'llm_analysis_threshold_salience': 0.5,
                'recent_context_window_s': 5.0
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 20.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class BodyAwarenessNode:
    def __init__(self):
        # Initialize the ROS node with a unique name.
        rospy.init_node('body_awareness_node', anonymous=False)
        self.node_name = rospy.get_name() # Store node name for logging in utilities

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "body_awareness_log.db")
        self.analysis_interval = self.params.get('body_awareness_analysis_interval', 0.2) # How often to analyze body state
        self.llm_analysis_threshold_salience = self.params.get('llm_analysis_threshold_salience', 0.5) # Cumulative salience to trigger LLM analysis
        self.recent_context_window_s = self.params.get('recent_context_window_s', 5.0) # Window for deques for LLM context

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 20.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'body_awareness_log' table if it doesn't exist.
        # NEW: Added 'llm_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS body_awareness_log (
                id TEXT PRIMARY KEY,            -- Unique awareness event ID (UUID)
                timestamp TEXT,
                body_state TEXT,                -- e.g., 'normal', 'unbalanced', 'collision_detected'
                posture_description TEXT,       -- Descriptive text about posture
                anomaly_detected BOOLEAN,       -- True if an anomaly is present
                anomaly_severity REAL,          -- Severity of anomaly (0.0 to 1.0)
                llm_reasoning TEXT,             -- NEW: LLM's detailed reasoning for state/anomaly inference
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of inference
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_awareness_timestamp ON body_awareness_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.current_body_awareness_state = {
            'timestamp': str(rospy.get_time()),
            'body_state': 'normal',
            'posture_description': 'stable',
            'anomaly_detected': False,
            'anomaly_severity': 0.0
        }

        # Deques to maintain a short history of inputs relevant to body awareness
        self.recent_joint_states = deque(maxlen=10)
        self.recent_force_torques = deque(maxlen=10)
        self.recent_tactile_sensor_data = deque(maxlen=10)
        self.recent_robot_health_states = deque(maxlen=5)
        self.recent_cognitive_directives = deque(maxlen=3) # Directives *for* this node
        self.recent_memory_responses = deque(maxlen=3) # For calibration data, movement norms
        self.recent_internal_narratives = deque(maxlen=5) # For robot's self-reflection on body

        self.cumulative_body_salience = 0.0 # Aggregated salience to trigger LLM analysis

        # --- Publishers ---
        self.pub_body_awareness_state = rospy.Publisher('/body_awareness_state', BodyAwarenessState, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # For requesting attention or memory


        # --- Subscribers ---
        rospy.Subscriber('/joint_states', JointState, self.joint_state_callback)
        rospy.Subscriber('/force_torque_sensors', ForceTorque, self.force_torque_callback)
        rospy.Subscriber('/tactile_sensors', TactileSensor, self.tactile_sensor_callback)
        rospy.Subscriber('/robot_health', RobotHealth, self.robot_health_callback)
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON
        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback)


        # --- Timer for periodic body awareness analysis ---
        rospy.Timer(rospy.Duration(self.analysis_interval), self._run_body_analysis_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's body awareness system online.")
        # Publish initial state
        self.publish_body_awareness_state(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_body_analysis_wrapper(self, event):
        """Wrapper to run the async body awareness analysis from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM body analysis task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.analyze_body_state_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.3, max_tokens=300):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Low temperature for factual/reasoning tasks
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM analysis."""
        self.cumulative_body_salience += score
        self.cumulative_body_salience = min(1.0, self.cumulative_body_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_joint_states, self.recent_force_torques, self.recent_tactile_sensor_data,
            self.recent_robot_health_states, self.recent_cognitive_directives,
            self.recent_memory_responses, self.recent_internal_narratives
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def joint_state_callback(self, msg):
        # NOTE: Standard sensor_msgs/JointState doesn't have a 'timestamp' attribute directly on the message,
        # but in ROS it has a header with stamp. Assuming message converted to dictionary or has direct attributes.
        # If using actual sensor_msgs.msg.JointState, adjust 'timestamp' access (e.g., msg.header.stamp.to_sec())
        fields_map = {
            'header.stamp': (str(rospy.get_time()), 'timestamp'), # Accessing header.stamp, assuming parsed
            'name': ([], 'joint_names'), # List of joint names
            'position': ([], 'positions'), # List of joint positions
            'velocity': ([], 'velocities'), # List of joint velocities
            'effort': ([], 'efforts') # List of joint efforts
        }
        # Special handling for JointState if it's not a String message.
        # If it's a real JointState message, its structure needs direct attribute access.
        # For simplicity, if it's String fallback, it's JSON.
        if isinstance(msg, String):
            data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        else: # Assume actual JointState message type
            # Directly access attributes of JointState message
            data = {
                'timestamp': str(msg.header.stamp.to_sec()),
                'joint_names': msg.name,
                'positions': msg.position,
                'velocities': msg.velocity,
                'efforts': msg.effort
            }
        
        # Calculate a simple 'salience' for joint state changes (e.g., high velocity/effort indicates activity)
        # This is a heuristic; real salience might need more sophisticated models.
        salience = 0.0
        if data.get('velocities'):
            salience += sum([abs(v) for v in data['velocities']]) * 0.05
        if data.get('efforts'):
            salience += sum([abs(e) for e in data['efforts']]) * 0.02
        data['salience_score'] = min(1.0, salience) # Clamp salience

        self.recent_joint_states.append(data)
        self._update_cumulative_salience(data['salience_score'] * 0.3)
        rospy.logdebug(f"{self.node_name}: Received Joint State (Activity Salience: {data['salience_score']:.2f}).")

    def force_torque_callback(self, msg):
        # Assuming ForceTorque message has 'header.stamp', 'wrench.force.x', 'wrench.torque.x' etc.
        fields_map = {
            'header.stamp': (str(rospy.get_time()), 'timestamp'),
            'wrench.force.x': (0.0, 'force_x'), 'wrench.force.y': (0.0, 'force_y'), 'wrench.force.z': (0.0, 'force_z'),
            'wrench.torque.x': (0.0, 'torque_x'), 'wrench.torque.y': (0.0, 'torque_y'), 'wrench.torque.z': (0.0, 'torque_z')
        }
        if isinstance(msg, String):
            data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        else: # Assume actual ForceTorque message type (e.g., geometry_msgs/WrenchStamped)
            data = {
                'timestamp': str(msg.header.stamp.to_sec()),
                'force_x': msg.wrench.force.x, 'force_y': msg.wrench.force.y, 'force_z': msg.wrench.force.z,
                'torque_x': msg.wrench.torque.x, 'torque_y': msg.wrench.torque.y, 'torque_z': msg.wrench.torque.z
            }

        salience = 0.0
        # High force/torque indicates interaction or impact
        if data.get('force_x') is not None: salience += abs(data['force_x']) * 0.1
        if data.get('force_y') is not None: salience += abs(data['force_y']) * 0.1
        if data.get('force_z') is not None: salience += abs(data['force_z']) * 0.1
        data['salience_score'] = min(1.0, salience * 0.5) # Clamp and scale

        self.recent_force_torques.append(data)
        self._update_cumulative_salience(data['salience_score'] * 0.5) # Force/torque implies significant interaction
        rospy.logdebug(f"{self.node_name}: Received Force/Torque data (Salience: {data['salience_score']:.2f}).")

    def tactile_sensor_callback(self, msg):
        # Assuming TactileSensor message has 'timestamp', 'contact_points_json', 'pressure_sum', 'num_contacts'
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'),
            'contact_points_json': ('[]', 'contact_points_json'),
            'pressure_sum': (0.0, 'pressure_sum'),
            'num_contacts': (0, 'num_contacts')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('contact_points_json'), str):
            try: data['contact_points'] = json.loads(data['contact_points_json'])
            except json.JSONDecodeError: data['contact_points'] = []

        salience = data.get('pressure_sum', 0.0) * 0.3 + data.get('num_contacts', 0) * 0.1
        data['salience_score'] = min(1.0, salience) # Clamp salience

        self.recent_tactile_sensor_data.append(data)
        self._update_cumulative_salience(data['salience_score'] * 0.4) # Tactile input implies physical interaction
        rospy.logdebug(f"{self.node_name}: Received Tactile Sensor data (Contacts: {data['num_contacts']}, Pressure: {data['pressure_sum']:.2f}).")

    def robot_health_callback(self, msg):
        # Assuming RobotHealth message has 'timestamp', 'overall_status', 'battery_level', 'motor_temps_json', 'error_flags_json'
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'),
            'overall_status': ('normal', 'overall_status'),
            'battery_level': (100.0, 'battery_level'),
            'motor_temps_json': ('{}', 'motor_temps_json'),
            'error_flags_json': ('{}', 'error_flags_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('motor_temps_json'), str):
            try: data['motor_temps'] = json.loads(data['motor_temps_json'])
            except json.JSONDecodeError: data['motor_temps'] = {}
        if isinstance(data.get('error_flags_json'), str):
            try: data['error_flags'] = json.loads(data['error_flags_json'])
            except json.JSONDecodeError: data['error_flags'] = {}

        salience = 0.0
        if data.get('overall_status') == 'critical': salience = 1.0
        elif data.get('overall_status') == 'warning': salience = 0.7
        if data.get('battery_level', 100.0) < 20.0: salience = max(salience, 0.6)
        if data.get('error_flags'): # If any error flags are true
            if any(data['error_flags'].values()): salience = max(salience, 0.8)
        data['salience_score'] = min(1.0, salience)

        self.recent_robot_health_states.append(data)
        self._update_cumulative_salience(data['salience_score'] * 0.9) # Health issues are high priority
        rospy.logdebug(f"{self.node_name}: Received Robot Health State (Status: {data['overall_status']}, Salience: {data['salience_score']:.2f}).")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name:
            self.recent_cognitive_directives.append(data) # Add directives for self to context
            # Directives for body integrity checks or specific posture adjustments are highly salient
            if data.get('directive_type') in ['CheckBodyIntegrity', 'AdjustPosture']:
                self._update_cumulative_salience(data.get('urgency', 0.0) * 0.9)
            rospy.loginfo(f"{self.node_name}: Received directive for self: '{data.get('directive_type', 'N/A')}' (Payload: {data.get('command_payload', 'N/A')}).")
        else:
            self.recent_cognitive_directives.append(data) # Add all directives for general context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Memory responses containing calibration data, movement norms, or past anomaly records
        if data.get('response_code', 0) == 200 and \
           any('calibration' in mem.get('category', '') or 'anomaly' in mem.get('category', '') for mem in data['memories']):
            self._update_cumulative_salience(0.3)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def internal_narrative_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),
            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_internal_narratives.append(data)
        # Narratives expressing discomfort, pain, or self-doubt about movement
        if "pain" in data.get('narrative_text', '').lower() or \
           "unstable" in data.get('narrative_text', '').lower() or \
           data.get('sentiment', 0.0) < -0.5:
            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.7)
        rospy.logdebug(f"{self.node_name}: Received Internal Narrative (Theme: {data.get('main_theme', 'N/A')}).")

    # --- Core Body Awareness Analysis Logic (Async with LLM) ---
    async def analyze_body_state_async(self, event):
        """
        Asynchronously analyzes recent physical sensor data and internal states
        to infer the robot's current body state and detect anomalies.
        """
        self._prune_history() # Keep context history fresh

        if self.cumulative_body_salience >= self.llm_analysis_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for body awareness analysis (Salience: {self.cumulative_body_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_body_awareness()
            llm_body_output = await self._infer_body_state_llm(context_for_llm)

            if llm_body_output:
                awareness_event_id = str(uuid.uuid4())
                timestamp = llm_body_output.get('timestamp', str(rospy.get_time()))
                body_state = llm_body_output.get('body_state', 'normal')
                posture_description = llm_body_output.get('posture_description', 'stable')
                anomaly_detected = llm_body_output.get('anomaly_detected', False)
                anomaly_severity = max(0.0, min(1.0, llm_body_output.get('anomaly_severity', 0.0)))
                llm_reasoning = llm_body_output.get('llm_reasoning', 'No reasoning.')

                self.current_body_awareness_state = {
                    'timestamp': timestamp,
                    'body_state': body_state,
                    'posture_description': posture_description,
                    'anomaly_detected': anomaly_detected,
                    'anomaly_severity': anomaly_severity
                }

                self.save_body_awareness_log(
                    id=awareness_event_id,
                    timestamp=timestamp,
                    body_state=body_state,
                    posture_description=posture_description,
                    anomaly_detected=anomaly_detected,
                    anomaly_severity=anomaly_severity,
                    llm_reasoning=llm_reasoning,
                    context_snapshot_json=json.dumps(context_for_llm)
                )
                self.publish_body_awareness_state(None) # Publish updated state

                if anomaly_detected and anomaly_severity > 0.5:
                    self.publish_cognitive_directive(
                        directive_type='AddressBodyAnomaly',
                        target_node='CognitiveControl', # Direct CognitiveControl to handle body anomalies
                        command_payload=json.dumps({"anomaly_type": body_state, "severity": anomaly_severity, "description": posture_description}),
                        urgency=anomaly_severity # Urgency scales with severity
                    )
                rospy.loginfo(f"{self.node_name}: Inferred Body State: '{body_state}' (Posture: '{posture_description}', Anomaly: {anomaly_detected}, Severity: {anomaly_severity:.2f}).")
                self.cumulative_body_salience = 0.0 # Reset after LLM analysis
            else:
                rospy.logwarn(f"{self.node_name}: LLM failed to infer body state. Applying simple fallback.")
                self._apply_simple_body_rules() # Fallback to simple rules
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_body_salience:.2f}) for LLM body awareness analysis. Applying simple rules.")
            self._apply_simple_body_rules()
        
        self.publish_body_awareness_state(None) # Always publish state, even if updated by simple rules


    async def _infer_body_state_llm(self, context_for_llm):
        """
        Uses the LLM to infer the robot's current body state, posture, and detect anomalies.
        """
        prompt_text = f"""
        You are the Body Awareness Module of a robot's cognitive architecture. Your task is to analyze the robot's physical sensor data and internal reflections to infer its current body state, posture, and detect any physical anomalies or issues.

        Robot's Recent Physical and Internal Context (for Body Awareness Inference):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this context, provide:
        1.  `body_state`: string (Overall physical state, e.g., 'normal', 'unbalanced', 'collision_detected', 'motor_overheating', 'low_battery', 'damaged').
        2.  `posture_description`: string (A concise description of the robot's inferred posture or movement state, e.g., 'standing_stable', 'leaning_left', 'moving_slowly', 'immobilized').
        3.  `anomaly_detected`: boolean (True if a physical anomaly or significant issue is detected).
        4.  `anomaly_severity`: number (0.0 to 1.0, indicating the severity of any detected anomaly. 1.0 is highest severity).
        5.  `llm_reasoning`: string (Detailed explanation for your inference, referencing specific contextual inputs).

        Consider:
        -   **Joint States**: Are joint `positions`, `velocities`, or `efforts` abnormal or indicative of a specific posture/movement?
        -   **Force/Torque**: Is there unexpected `force` or `torque` indicating collision or strain?
        -   **Tactile Sensor**: Is `pressure_sum` or `num_contacts` indicating unexpected contact?
        -   **Robot Health**: Is `overall_status`, `battery_level`, or `motor_temps` signaling issues?
        -   **Cognitive Directives**: Are there specific directives for *this node* ('BodyAwarenessNode') like 'CheckBodyIntegrity'?
        -   **Memory Responses**: Are there relevant calibration data or past anomaly records from memory?
        -   **Internal Narratives**: Is the robot expressing sensations of discomfort, pain, or instability in its self-talk?

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'body_state': string
        3.  'posture_description': string
        4.  'anomaly_detected': boolean
        5.  'anomaly_severity': number
        6.  'llm_reasoning': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "body_state": {"type": "string"},
                "posture_description": {"type": "string"},
                "anomaly_detected": {"type": "boolean"},
                "anomaly_severity": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "llm_reasoning": {"type": "string"}
            },
            "required": ["timestamp", "body_state", "posture_description", "anomaly_detected", "anomaly_severity", "llm_reasoning"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.3, max_tokens=350)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical fields are floats/booleans
                if 'anomaly_detected' in llm_data: llm_data['anomaly_detected'] = bool(llm_data['anomaly_detected'])
                if 'anomaly_severity' in llm_data: llm_data['anomaly_severity'] = float(llm_data['anomaly_severity'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for body awareness: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_BODY_ANALYSIS_FAILED", f"LLM call failed for body awareness: {llm_output_str}", 0.9)
            return None

    def _apply_simple_body_rules(self):
        """
        Fallback mechanism to infer body awareness state using simple rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        new_body_state = "normal"
        new_posture_description = "stable"
        new_anomaly_detected = False
        new_anomaly_severity = 0.0

        # Rule 1: Detect low battery
        if self.recent_robot_health_states:
            latest_health = self.recent_robot_health_states[-1]
            if latest_health.get('battery_level', 100.0) < 15.0:
                new_body_state = "low_battery"
                new_posture_description = "seeking_charger"
                new_anomaly_detected = True
                new_anomaly_severity = max(new_anomaly_severity, 0.7)
                rospy.logdebug(f"{self.node_name}: Simple rule: Detected low battery.")

        # Rule 2: Detect significant force/torque (potential collision or heavy load)
        if self.recent_force_torques:
            latest_ft = self.recent_force_torques[-1]
            force_magnitude = (latest_ft.get('force_x', 0)**2 + latest_ft.get('force_y', 0)**2 + latest_ft.get('force_z', 0)**2)**0.5
            if force_magnitude > 50.0: # Threshold for significant force (adjust as needed)
                new_body_state = "collision_detected" if force_magnitude > 100.0 else "under_load"
                new_posture_description = "bracing_for_impact" if force_magnitude > 100.0 else "carrying_object"
                new_anomaly_detected = True
                new_anomaly_severity = max(new_anomaly_severity, min(1.0, force_magnitude / 150.0))
                rospy.logdebug(f"{self.node_name}: Simple rule: Detected significant force/torque.")

        # Rule 3: Detect extreme joint positions (potential constraint violation or unusual posture)
        if self.recent_joint_states:
            latest_js = self.recent_joint_states[-1]
            # This is a very simplified check; real joint limits would be defined per joint.
            # Assuming any position outside a -PI to PI range is "extreme" for this simple rule.
            if any(abs(pos) > 3.14 for pos in latest_js.get('positions', [])):
                new_body_state = "unusual_posture"
                new_posture_description = "awkward_position"
                new_anomaly_detected = True
                new_anomaly_severity = max(new_anomaly_severity, 0.4)
                rospy.logdebug(f"{self.node_name}: Simple rule: Detected unusual joint posture.")

        # Update current state based on simple rules
        self.current_body_awareness_state = {
            'timestamp': str(current_time),
            'body_state': new_body_state,
            'posture_description': new_posture_description,
            'anomaly_detected': new_anomaly_detected,
            'anomaly_severity': new_anomaly_severity
        }
        rospy.logdebug(f"{self.node_name}: Simple rule: Current Body State: {new_body_state}, Anomaly: {new_anomaly_detected}.")


    def _compile_llm_context_for_body_awareness(self):
        """
        Gathers and formats all relevant physical sensor data and internal states
        for the LLM's body awareness analysis.
        """
        context = {
            "current_time": rospy.get_time(),
            "current_body_awareness_state": self.current_body_awareness_state,
            "recent_physical_inputs": {
                "joint_states": list(self.recent_joint_states),
                "force_torques": list(self.recent_force_torques),
                "tactile_sensor_data": list(self.recent_tactile_sensor_data),
                "robot_health_states": list(self.recent_robot_health_states)
            },
            "recent_cognitive_inputs": {
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name],
                "memory_responses": list(self.recent_memory_responses),
                "internal_narratives": list(self.recent_internal_narratives)
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["recent_physical_inputs"]:
            for i, item in enumerate(context["recent_physical_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try: item[field] = json.loads(value)
                            except json.JSONDecodeError: pass
        for category_key in context["recent_cognitive_inputs"]:
            for i, item in enumerate(context["recent_cognitive_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try: item[field] = json.loads(value)
                            except json.JSONDecodeError: pass
        
        return context

    # --- Database and Publishing Functions ---
    def save_body_awareness_log(self, id, timestamp, body_state, posture_description, anomaly_detected, anomaly_severity, llm_reasoning, context_snapshot_json):
        """Saves a body awareness state entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO body_awareness_log (id, timestamp, body_state, posture_description, anomaly_detected, anomaly_severity, llm_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, body_state, posture_description, anomaly_detected, anomaly_severity, llm_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved body awareness log (ID: {id}, State: {body_state}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save body awareness log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_body_awareness_log: {e}", 0.9)


    def publish_body_awareness_state(self, event):
        """Publishes the robot's current body awareness state."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.current_body_awareness_state['timestamp'] = timestamp
        
        try:
            if isinstance(BodyAwarenessState, type(String)): # Fallback to String message
                self.pub_body_awareness_state.publish(json.dumps(self.current_body_awareness_state))
            else:
                awareness_msg = BodyAwarenessState()
                awareness_msg.timestamp = timestamp
                awareness_msg.body_state = self.current_body_awareness_state['body_state']
                awareness_msg.posture_description = self.current_body_awareness_state['posture_description']
                awareness_msg.anomaly_detected = self.current_body_awareness_state['anomaly_detected']
                awareness_msg.anomaly_severity = self.current_body_awareness_state['anomaly_severity']
                self.pub_body_awareness_state.publish(awareness_msg)

            rospy.logdebug(f"{self.node_name}: Published Body Awareness State. State: '{self.current_body_awareness_state['body_state']}'.")

        except Exception as e:
            self._report_error("PUBLISH_BODY_AWARENESS_STATE_ERROR", f"Failed to publish body awareness state: {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Body Awareness Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = BodyAwarenessNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, BodyAwarenessNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, BodyAwarenessNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

5. Refactored Cognitive Control Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique directive IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        CognitiveDirective,     # Output: Directives for other nodes; Input: Directives for self
        AttentionState,         # Input: Robot's attention focus and priority
        BiasMitigationState,    # Input: Status of bias detection and mitigation efforts
        EmotionState,           # Input: Robot's emotional state
        MotivationState,        # Input: Dominant goal and drive level
        WorldModelState,        # Input: Current state of the world
        BodyAwarenessState,     # Input: Robot's inferred body state
        PerformanceReport,      # Input: Overall system performance
        MemoryResponse,         # Input: Retrieved memories (e.g., plans, past strategies)
        PredictionState,        # Input: Predicted outcomes
        EthicalDecision,        # Input: Ethical clearances/conflicts
        InternalNarrative,      # Input: Robot's internal thoughts
        ReflectionState,        # Input: Insights from self-reflection
        SocialCognitionState    # Input: Inferred user mood/intent
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Cognitive Control Node.")
    CognitiveDirective = String
    AttentionState = String
    BiasMitigationState = String
    EmotionState = String
    MotivationState = String
    WorldModelState = String
    BodyAwarenessState = String
    PerformanceReport = String
    EthicalDecision = String
    MemoryResponse = String
    PredictionState = String
    InternalNarrative = String
    ReflectionState = String
    SocialCognitionState = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'cognitive_control_node': {
                'decision_interval': 0.3, # How often to re-evaluate overall strategy
                'llm_decision_threshold_salience': 0.8, # Cumulative salience to trigger LLM
                'recent_context_window_s': 15.0, # Window for deques for LLM context
                'default_priority_threshold': 0.5 # Default priority for directives
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 25.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class CognitiveControlNode:
    def __init__(self):
        rospy.init_node('cognitive_control_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "cognitive_control_log.db")
        self.decision_interval = self.params.get('decision_interval', 0.3) # How often to re-evaluate strategy
        self.llm_decision_threshold_salience = self.params.get('llm_decision_threshold_salience', 0.8) # Cumulative salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 15.0) # Window for deques for LLM context
        self.default_priority_threshold = self.params.get('default_priority_threshold', 0.5)

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 25.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'cognitive_control_log' table if it doesn't exist.
        # NEW: Added 'llm_decision_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS cognitive_control_log (
                id TEXT PRIMARY KEY,            -- Unique decision ID (UUID)
                timestamp TEXT,
                decision_summary TEXT,          -- High-level summary of the control decision
                directives_issued_json TEXT,    -- JSON array of directives issued
                llm_decision_reasoning TEXT,    -- NEW: LLM's detailed reasoning for the decision
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of decision
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_control_timestamp ON cognitive_control_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.last_decision_summary = "Initializing cognitive control."

        # Deques to maintain a short history of inputs for control decisions
        self.recent_attention_states = deque(maxlen=5)
        self.recent_bias_mitigation_states = deque(maxlen=5)
        self.recent_emotion_states = deque(maxlen=5)
        self.recent_motivation_states = deque(maxlen=5)
        self.recent_world_model_states = deque(maxlen=5)
        self.recent_body_awareness_states = deque(maxlen=5)
        self.recent_performance_reports = deque(maxlen=5)
        self.recent_ethical_decisions = deque(maxlen=5)
        self.recent_memory_responses = deque(maxlen=5)
        self.recent_prediction_states = deque(maxlen=5)
        self.recent_internal_narratives = deque(maxlen=5)
        self.recent_reflection_states = deque(maxlen=5)
        self.recent_social_cognition_states = deque(maxlen=5)
        self.recent_incoming_directives = deque(maxlen=5) # Directives for this node

        self.cumulative_decision_salience = 0.0 # Aggregated salience to trigger LLM decision

        # --- Publishers ---
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # For issuing directives
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)


        # --- Subscribers ---
        rospy.Subscriber('/attention_state', AttentionState, self.attention_state_callback)
        rospy.Subscriber('/bias_mitigation_state', BiasMitigationState, self.bias_mitigation_state_callback)
        rospy.Subscriber('/emotion_state', EmotionState, self.emotion_state_callback)
        rospy.Subscriber('/motivation_state', String, self.motivation_state_callback) # Stringified JSON
        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON
        rospy.Subscriber('/body_awareness_state', String, self.body_awareness_state_callback) # Stringified JSON
        rospy.Subscriber('/performance_report', PerformanceReport, self.performance_report_callback)
        rospy.Subscriber('/ethical_decision', String, self.ethical_decision_callback) # Stringified JSON
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON
        rospy.Subscriber('/prediction_state', String, self.prediction_state_callback) # Stringified JSON
        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback)
        rospy.Subscriber('/reflection_state', String, self.reflection_state_callback) # Stringified JSON
        rospy.Subscriber('/social_cognition_state', String, self.social_cognition_state_callback) # Stringified JSON
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.incoming_cognitive_directive_callback) # For directives *to* this node


        # --- Timer for periodic decision-making ---
        rospy.Timer(rospy.Duration(self.decision_interval), self._run_decision_making_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's cognitive control system online, integrating data for high-level decisions.")

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_decision_making_wrapper(self, event):
        """Wrapper to run the async decision making from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM decision task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.make_cognitive_decisions_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.4, max_tokens=400):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Moderate temperature for balanced reasoning and creativity
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience for decision making ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM decision."""
        self.cumulative_decision_salience += score
        self.cumulative_decision_salience = min(1.0, self.cumulative_decision_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_attention_states, self.recent_bias_mitigation_states,
            self.recent_emotion_states, self.recent_motivation_states,
            self.recent_world_model_states, self.recent_body_awareness_states,
            self.recent_performance_reports, self.recent_ethical_decisions,
            self.recent_memory_responses, self.recent_prediction_states,
            self.recent_internal_narratives, self.recent_reflection_states,
            self.recent_social_cognition_states, self.recent_incoming_directives
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def attention_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'focus_type': ('idle', 'focus_type'),
            'focus_target': ('environment', 'focus_target'), 'priority_score': (0.0, 'priority_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_attention_states.append(data)
        self._update_cumulative_salience(data.get('priority_score', 0.0) * 0.2) # High attention focus influences control
        rospy.logdebug(f"{self.node_name}: Received Attention State. Focus: {data.get('focus_target')}.")

    def bias_mitigation_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'bias_type': ('none', 'bias_type'),
            'detected_severity': (0.0, 'detected_severity'), 'mitigation_status': ('idle', 'mitigation_status')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_bias_mitigation_states.append(data)
        if data.get('mitigation_status') in ['detected', 'mitigation_recommended'] and data.get('detected_severity', 0.0) > 0.5:
            self._update_cumulative_salience(data.get('detected_severity', 0.0) * 0.8) # Biases require high control attention
        rospy.logdebug(f"{self.node_name}: Received Bias Mitigation State. Bias: {data.get('bias_type')}.")

    def emotion_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'mood': ('neutral', 'mood'),
            'sentiment_score': (0.0, 'sentiment_score'), 'mood_intensity': (0.0, 'mood_intensity')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_emotion_states.append(data)
        self._update_cumulative_salience(data.get('mood_intensity', 0.0) * 0.3) # Strong emotions impact decisions
        rospy.logdebug(f"{self.node_name}: Received Emotion State. Mood: {data.get('mood')}.")

    def motivation_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'dominant_goal_id': ('none', 'dominant_goal_id'),
            'overall_drive_level': (0.0, 'overall_drive_level'), 'active_goals_json': ('{}', 'active_goals_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('active_goals_json'), str):
            try: data['active_goals'] = json.loads(data['active_goals_json'])
            except json.JSONDecodeError: data['active_goals'] = {}
        self.recent_motivation_states.append(data)
        self._update_cumulative_salience(data.get('overall_drive_level', 0.0) * 0.4) # High drive influences decisions
        rospy.logdebug(f"{self.node_name}: Received Motivation State. Goal: {data.get('dominant_goal_id')}.")

    def world_model_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),
            'changed_entities_json': ('[]', 'changed_entities_json'),
            'significant_change_flag': (False, 'significant_change_flag'),
            'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('changed_entities_json'), str):
            try: data['changed_entities'] = json.loads(data['changed_entities_json'])
            except json.JSONDecodeError: data['changed_entities'] = []
        self.recent_world_model_states.append(data)
        if data.get('significant_change_flag', False) or data.get('consistency_score', 1.0) < 0.8:
            self._update_cumulative_salience(0.5) # World changes or inconsistencies need attention
        rospy.logdebug(f"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.")

    def body_awareness_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'body_state': ('normal', 'body_state'),
            'posture_description': ('stable', 'posture_description'), 'anomaly_detected': (False, 'anomaly_detected'),
            'anomaly_severity': (0.0, 'anomaly_severity')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_body_awareness_states.append(data)
        if data.get('anomaly_detected', False) and data.get('anomaly_severity', 0.0) > 0.3:
            self._update_cumulative_salience(data.get('anomaly_severity', 0.0) * 0.9) # Body issues are high priority
        rospy.logdebug(f"{self.node_name}: Received Body Awareness State. Anomaly: {data.get('anomaly_detected', False)}.")

    def performance_report_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'overall_score': (1.0, 'overall_score'),
            'suboptimal_flag': (False, 'suboptimal_flag'), 'kpis_json': ('{}', 'kpis_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('kpis_json'), str):
            try: data['kpis'] = json.loads(data['kpis_json'])
            except json.JSONDecodeError: data['kpis'] = {}
        self.recent_performance_reports.append(data)
        if data.get('suboptimal_flag', False) or data.get('overall_score', 1.0) < 0.7:
            self._update_cumulative_salience(0.6 * (1.0 - data.get('overall_score', 1.0))) # Poor performance needs corrective action
        rospy.logdebug(f"{self.node_name}: Received Performance Report. Suboptimal: {data.get('suboptimal_flag', False)}.")

    def ethical_decision_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'decision_id': ('', 'decision_id'),
            'action_proposal_id': ('', 'action_proposal_id'),
            'ethical_clearance': (False, 'ethical_clearance'),
            'ethical_score': (0.0, 'ethical_score'),
            'ethical_reasoning': ('', 'ethical_reasoning'),
            'conflict_flag': (False, 'conflict_flag')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_ethical_decisions.append(data)
        if not data.get('ethical_clearance', True) or data.get('conflict_flag', False):
            self._update_cumulative_salience(0.9) # Ethical conflicts are critical
        rospy.logdebug(f"{self.node_name}: Received Ethical Decision. Clearance: {data.get('ethical_clearance', 'N/A')}.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        if data.get('response_code', 0) == 200 and data.get('memories'):
            self._update_cumulative_salience(0.1) # Memory retrieval might offer relevant context
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def prediction_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'predicted_event': ('', 'predicted_event'),
            'prediction_confidence': (0.0, 'prediction_confidence'), 'prediction_accuracy': (0.0, 'prediction_accuracy'),
            'urgency_flag': (False, 'urgency_flag')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_prediction_states.append(data)
        if data.get('urgency_flag', False) or data.get('prediction_confidence', 0.0) > 0.8:
            self._update_cumulative_salience(0.7) # Urgent or high-confidence predictions need control
        rospy.logdebug(f"{self.node_name}: Received Prediction State. Event: {data.get('predicted_event')}.")

    def internal_narrative_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),
            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_internal_narratives.append(data)
        self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.2) # Internal reflections add to context
        rospy.logdebug(f"{self.node_name}: Received Internal Narrative. Theme: {data.get('main_theme')}.")

    def reflection_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'reflection_text': ('', 'reflection_text'),
            'insight_type': ('none', 'insight_type'), 'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_reflection_states.append(data)
        if data.get('insight_type') == 'error_detection' or data.get('consistency_score', 1.0) < 0.7:
            self._update_cumulative_salience(0.6) # Self-reflection of errors is critical
        rospy.logdebug(f"{self.node_name}: Received Reflection State. Insight: {data.get('insight_type')}.")

    def social_cognition_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'inferred_mood': ('neutral', 'inferred_mood'),
            'mood_confidence': (0.0, 'mood_confidence'), 'inferred_intent': ('none', 'inferred_intent'),
            'intent_confidence': (0.0, 'intent_confidence'), 'user_id': ('unknown', 'user_id')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_social_cognition_states.append(data)
        if data.get('intent_confidence', 0.0) > 0.7 and data.get('inferred_intent') == 'command':
            self._update_cumulative_salience(0.5) # Clear user commands need control
        rospy.logdebug(f"{self.node_name}: Received Social Cognition State. Intent: {data.get('inferred_intent')}.")

    def incoming_cognitive_directive_callback(self, msg):
        # This callback specifically handles directives *for* CognitiveControlNode
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name:
            self.recent_incoming_directives.append(data) # Add directives for self to context
            # Directives for Cognitive Control are always highly salient
            self._update_cumulative_salience(data.get('urgency', 0.0) * 1.0)
            rospy.loginfo(f"{self.node_name}: Received directive for self: '{data.get('directive_type', 'N/A')}' (Payload: {data.get('command_payload', 'N/A')}).")
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for general context.")


    # --- Core Decision-Making Logic (Async with LLM) ---
    async def make_cognitive_decisions_async(self, event):
        """
        Asynchronously makes high-level cognitive decisions and issues directives
        based on integrated cognitive states, using LLM for complex reasoning.
        """
        self._prune_history() # Keep context history fresh

        directives_to_issue = []
        llm_decision_reasoning = "Not evaluated by LLM."
        decision_summary = "No significant action taken."

        # Determine if LLM decision-making is needed
        if self.cumulative_decision_salience >= self.llm_decision_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for cognitive control decision (Salience: {self.cumulative_decision_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_decision()
            llm_decision_output = await self._infer_cognitive_directives_llm(context_for_llm)

            if llm_decision_output:
                directives_to_issue = llm_decision_output.get('directives', [])
                llm_decision_reasoning = llm_decision_output.get('reasoning', 'LLM provided no specific reasoning.')
                decision_summary = llm_decision_output.get('decision_summary', 'LLM made a decision.')
                rospy.loginfo(f"{self.node_name}: LLM Decision: '{decision_summary}'. Issued {len(directives_to_issue)} directives.")
            else:
                rospy.logwarn(f"{self.node_name}: LLM decision-making failed. Falling back to simple rules.")
                directives_to_issue, decision_summary = self._apply_simple_decision_rules()
                llm_decision_reasoning = "Fallback to simple rules due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_decision_salience:.2f}) for LLM decision. Applying simple rules.")
            directives_to_issue, decision_summary = self._apply_simple_decision_rules()
            llm_decision_reasoning = "Fallback to simple rules due to low salience."

        # Publish all generated directives
        for directive_data in directives_to_issue:
            try:
                # Ensure command_payload is a JSON string before publishing
                command_payload_json = json.dumps(directive_data.get('command_payload', {}))
                self.publish_cognitive_directive(
                    directive_type=directive_data.get('directive_type', 'Undefined'),
                    target_node=directive_data.get('target_node', 'Unknown'),
                    command_payload=command_payload_json,
                    urgency=directive_data.get('urgency', self.default_priority_threshold),
                    reason=directive_data.get('reason', 'Cognitive Control decision.')
                )
            except Exception as e:
                self._report_error("DIRECTIVE_PUBLISH_ERROR", f"Failed to publish directive from Cognitive Control: {e}", 0.7, {'directive_data': directive_data})

        # Log the decision
        self.save_cognitive_control_log(
            id=str(uuid.uuid4()),
            timestamp=str(rospy.get_time()),
            decision_summary=decision_summary,
            directives_issued_json=json.dumps(directives_to_issue),
            llm_decision_reasoning=llm_decision_reasoning,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_decision())
        )
        self.cumulative_decision_salience = 0.0 # Reset after decision cycle

    async def _infer_cognitive_directives_llm(self, context_for_llm):
        """
        Uses the LLM to infer optimal cognitive directives based on the current
        integrated cognitive state.
        """
        prompt_text = f"""
        You are the Cognitive Control Module of a robot's cognitive architecture. Your overarching role is to synthesize information from all other cognitive modules, identify critical states (e.g., problems, opportunities, user needs, internal issues), and issue high-level directives to other nodes to maintain the robot's functionality, safety, ethical behavior, and overall well-being.

        Robot's Current Integrated Cognitive State (for High-Level Decision Making):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this comprehensive analysis, propose a high-level cognitive decision and corresponding directives. Provide:
        1.  `decision_summary`: string (A concise summary of the overall decision made by Cognitive Control).
        2.  `directives`: array of objects (A list of specific directives to be issued. Each object should have:
            - `directive_type`: string (e.g., 'ExecuteAction', 'RequestMemoryRetrieval', 'RedirectAttention', 'InitiateSelfReflection', 'AuditBias', 'GenerateInternalNarrative', 'AdjustMotivation', 'SeekUserClarification', 'UpdateWorldModel', 'AdjustPersona', 'SelfCorrect')
            - `target_node`: string (The name of the ROS node to which the directive is addressed, e.g., 'action_execution_node', 'memory_node', 'attention_node', 'self_reflection_node', 'bias_mitigation_node', 'internal_narrative_node', 'experience_motivation_node', 'interaction_flow_node', 'world_model_node', 'persona_manager_node', 'self_correction_node')
            - `command_payload`: object (A JSON object with specific parameters for the directive. E.g., for 'ExecuteAction', it might contain {{'action_id': 'speak', 'text': 'Hello'}}.)
            - `urgency`: number (0.0 to 1.0, priority of the directive).
            - `reason`: string (Brief explanation for issuing this directive).
        3.  `reasoning`: string (Detailed explanation for your overall decision, referencing multiple contributing factors from the context).

        Consider the hierarchy and interplay of all modules:
        -   **Urgent inputs**: High `priority_score` from Attention, urgent `interaction_request`s, critical `anomaly_detected` from Body Awareness, `conflict_flag` from Ethical Decision.
        -   **Goals & Motivations**: How does the `dominant_goal_id` and `overall_drive_level` influence what actions are prioritized?
        -   **Safety & Ethics**: Are there `bias_type`s needing `mitigation_status` 'mitigation_recommended'? Are there ethical `conflict_flag`s or `ethical_clearance` issues?
        -   **Self-Correction/Improvement**: Is `suboptimal_flag` in Performance Report, `inconsistency_score` in Reflection, or negative sentiment in Internal Narrative suggesting a need for internal adjustments?
        -   **World State**: Does the `world_model_state` indicate a need for a new action or update?
        -   **Predictions**: Do `prediction_state`s suggest upcoming opportunities or dangers?

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'decision_summary': string
        3.  'directives': array (as described above)
        4.  'reasoning': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "decision_summary": {"type": "string"},
                "directives": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "directive_type": {"type": "string"},
                            "target_node": {"type": "string"},
                            "command_payload": {"type": "object"},
                            "urgency": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                            "reason": {"type": "string"}
                        },
                        "required": ["directive_type", "target_node", "command_payload", "urgency", "reason"]
                    }
                },
                "reasoning": {"type": "string"}
            },
            "required": ["timestamp", "decision_summary", "directives", "reasoning"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.4, max_tokens=600)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical fields in directives are floats
                if 'directives' in llm_data:
                    for directive in llm_data['directives']:
                        if 'urgency' in directive:
                            directive['urgency'] = float(directive['urgency'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for cognitive control: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_DECISION_FAILED", f"LLM call failed for cognitive control: {llm_output_str}", 0.9)
            return None


    def _apply_simple_decision_rules(self):
        """
        Fallback mechanism for high-level decision making using simple rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        directives = []
        decision_summary = "Default cognitive control operation."

        # Rule 1: Prioritize user interaction if a high urgency request is pending
        if self.recent_social_cognition_states and self.recent_attention_states:
            latest_social = self.recent_social_cognition_states[-1]
            latest_attention = self.recent_attention_states[-1]
            if latest_social.get('inferred_intent') == 'command' and \
               latest_social.get('intent_confidence', 0.0) > 0.7 and \
               latest_attention.get('focus_type') == 'user_interaction' and \
               latest_attention.get('priority_score', 0.0) > 0.8:
                
                # Check for explicit user commands for action execution
                relevant_interaction_request = next((req for req in reversed(self.recent_cognitive_directives) if req.get('target_node') == 'action_execution_node' and req.get('directive_type') == 'ExecuteAction'), None)

                if relevant_interaction_request:
                    directives.append({
                        'directive_type': 'ExecuteAction',
                        'target_node': 'action_execution_node',
                        'command_payload': json.loads(relevant_interaction_request.get('command_payload', '{}')),
                        'urgency': 1.0,
                        'reason': 'High urgency user command detected.'
                    })
                    decision_summary = f"Prioritizing user command: {relevant_interaction_request.get('command_payload', {}).get('action_id', 'unknown')}."
                    rospy.logdebug(f"{self.node_name}: Simple rule: {decision_summary}")
                    return directives, decision_summary

        # Rule 2: Address critical body awareness anomalies
        if self.recent_body_awareness_states:
            latest_body_state = self.recent_body_awareness_states[-1]
            if latest_body_state.get('anomaly_detected', False) and latest_body_state.get('anomaly_severity', 0.0) > 0.7:
                directives.append({
                    'directive_type': 'AddressBodyAnomaly',
                    'target_node': 'action_execution_node', # or a dedicated "SelfMaintenanceNode"
                    'command_payload': {"anomaly_type": latest_body_state.get('body_state'), "severity": latest_body_state.get('anomaly_severity')},
                    'urgency': 0.95,
                    'reason': 'Critical body anomaly detected, requires immediate action.'
                })
                decision_summary = f"Addressing critical body anomaly: {latest_body_state.get('body_state')}."
                rospy.logdebug(f"{self.node_name}: Simple rule: {decision_summary}")
                return directives, decision_summary

        # Rule 3: Mitigate detected biases if severity is high
        if self.recent_bias_mitigation_states:
            latest_bias_state = self.recent_bias_mitigation_states[-1]
            if latest_bias_state.get('mitigation_status') == 'detected' and latest_bias_state.get('detected_severity', 0.0) > 0.6:
                directives.append({
                    'directive_type': 'InitiateSelfReflection',
                    'target_node': 'self_reflection_node',
                    'command_payload': {"reason": f"Potential bias '{latest_bias_state.get('bias_type')}' detected.", "focus_area": "reasoning_process"},
                    'urgency': 0.8,
                    'reason': 'Bias detected, self-reflection needed to understand and correct.'
                })
                directives.append({
                    'directive_type': 'RequestMemoryRetrieval',
                    'target_node': 'memory_node',
                    'command_payload': {"query_type": "disconfirming_evidence", "topic": latest_bias_state.get('bias_type'), "num_results": 3},
                    'urgency': 0.7,
                    'reason': 'Requesting disconfirming evidence to counter potential bias.'
                })
                decision_summary = f"Initiating bias mitigation for '{latest_bias_state.get('bias_type')}'."
                rospy.logdebug(f"{self.node_name}: Simple rule: {decision_summary}")
                return directives, decision_summary
        
        # Rule 4: If performance is suboptimal, trigger self-improvement or detailed logging
        if self.recent_performance_reports:
            latest_perf_report = self.recent_performance_reports[-1]
            if latest_perf_report.get('suboptimal_flag', False) and latest_perf_report.get('overall_score', 1.0) < 0.7:
                directives.append({
                    'directive_type': 'InitiateSelfImprovement',
                    'target_node': 'self_improvement_node',
                    'command_payload': {"area": "overall_performance", "report": latest_perf_report.get('kpis_json')},
                    'urgency': 0.7,
                    'reason': 'Suboptimal performance detected, requires self-improvement analysis.'
                })
                decision_summary = f"Addressing suboptimal performance (Score: {latest_perf_report.get('overall_score'):.2f})."
                rospy.logdebug(f"{self.node_name}: Simple rule: {decision_summary}")
                return directives, decision_summary

        # Default: If no urgent issues, focus on current dominant goal or environmental monitoring
        if self.recent_motivation_states:
            latest_motivation = self.recent_motivation_states[-1]
            if latest_motivation.get('dominant_goal_id') != 'none' and latest_motivation.get('overall_drive_level', 0.0) > 0.3:
                directives.append({
                    'directive_type': 'FocusOnGoal',
                    'target_node': 'attention_node', # Direct Attention node to focus
                    'command_payload': {"goal_id": latest_motivation.get('dominant_goal_id'), "urgency": latest_motivation.get('overall_drive_level')},
                    'urgency': latest_motivation.get('overall_drive_level'),
                    'reason': 'Maintain focus on current dominant goal.'
                })
                decision_summary = f"Maintaining focus on dominant goal: {latest_motivation.get('dominant_goal_id')}."
                rospy.logdebug(f"{self.node_name}: Simple rule: {decision_summary}")
                return directives, decision_summary
        
        # If still no directives, default to environmental monitoring
        directives.append({
            'directive_type': 'EnvironmentalScan',
            'target_node': 'sensory_qualia_node', # or 'world_model_node'
            'command_payload': {"scan_type": "general_awareness"},
            'urgency': 0.2,
            'reason': 'No active high-priority tasks, maintain environmental awareness.'
        })
        decision_summary = "Maintaining environmental awareness."
        rospy.logdebug(f"{self.node_name}: Simple rule: {decision_summary}")
        return directives, decision_summary


    def _compile_llm_context_for_decision(self):
        """
        Gathers and formats all relevant cognitive state data from various modules
        for the LLM's high-level decision-making.
        """
        context = {
            "current_time": rospy.get_time(),
            "last_cognitive_decision_summary": self.last_decision_summary,
            "recent_states": {
                "attention_state": self.recent_attention_states[-1] if self.recent_attention_states else "N/A",
                "bias_mitigation_state": self.recent_bias_mitigation_states[-1] if self.recent_bias_mitigation_states else "N/A",
                "emotion_state": self.recent_emotion_states[-1] if self.recent_emotion_states else "N/A",
                "motivation_state": self.recent_motivation_states[-1] if self.recent_motivation_states else "N/A",
                "world_model_state": self.recent_world_model_states[-1] if self.recent_world_model_states else "N/A",
                "body_awareness_state": self.recent_body_awareness_states[-1] if self.recent_body_awareness_states else "N/A",
                "performance_report": self.recent_performance_reports[-1] if self.recent_performance_reports else "N/A",
                "ethical_decision": self.recent_ethical_decisions[-1] if self.recent_ethical_decisions else "N/A",
                "prediction_state": self.recent_prediction_states[-1] if self.recent_prediction_states else "N/A",
                "internal_narrative": self.recent_internal_narratives[-1] if self.recent_internal_narratives else "N/A",
                "reflection_state": self.recent_reflection_states[-1] if self.recent_reflection_states else "N/A",
                "social_cognition_state": self.recent_social_cognition_states[-1] if self.recent_social_cognition_states else "N/A",
                "incoming_directives_for_self": list(self.recent_incoming_directives)
            },
            "full_history_summary": { # Summarize deques for more context without overloading LLM
                "attention_history_count": len(self.recent_attention_states),
                "bias_mitigation_history_count": len(self.recent_bias_mitigation_states),
                "motivation_history_count": len(self.recent_motivation_states),
                "world_model_history_count": len(self.recent_world_model_states),
                "body_awareness_history_count": len(self.recent_body_awareness_states)
                # ... add other history counts as needed
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["recent_states"]:
            item = context["recent_states"][category_key]
            if isinstance(item, dict):
                for field, value in item.items():
                    if isinstance(value, str) and field.endswith('_json'):
                        try: item[field] = json.loads(value)
                        except json.JSONDecodeError: pass

        for item in context["recent_states"]["incoming_directives_for_self"]:
            if isinstance(item, dict):
                if 'command_payload' in item and isinstance(item['command_payload'], str):
                    try: item['command_payload'] = json.loads(item['command_payload'])
                    except json.JSONDecodeError: pass

        return context

    # --- Database and Publishing Functions ---
    def save_cognitive_control_log(self, id, timestamp, decision_summary, directives_issued_json, llm_decision_reasoning, context_snapshot_json):
        """Saves a cognitive control decision entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO cognitive_control_log (id, timestamp, decision_summary, directives_issued_json, llm_decision_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, decision_summary, directives_issued_json, llm_decision_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved cognitive control log (ID: {id}, Summary: {decision_summary}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save cognitive control log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_cognitive_control_log: {e}", 0.9)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Cognitive Control: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = CognitiveControlNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, CognitiveControlNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, CognitiveControlNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

4. Refactored Creative Expression Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique creation IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        CreativeOutput,         # Output: Generated creative content (e.g., text, visual concepts, sound)
        CognitiveDirective,     # Input: Directives to generate creative content
        InternalNarrative,      # Input: Robot's internal thoughts (source of inspiration)
        EmotionState,           # Input: Robot's emotional state (influences creative style)
        MemoryResponse,         # Input: Retrieved memories (e.g., past experiences, knowledge for creation)
        SocialCognitionState,   # Input: Inferred user mood/intent (tailor creative output)
        AttentionState,         # Input: Current attention focus (influences creative topic)
        WorldModelState         # Input: Current environment (for context in creative output)
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Creative Expression Node.")
    CreativeOutput = String
    CognitiveDirective = String
    InternalNarrative = String
    EmotionState = String
    MemoryResponse = String
    SocialCognitionState = String
    AttentionState = String
    WorldModelState = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'creative_expression_node': {
                'generation_interval': 1.5, # How often to check for creative tasks
                'llm_generation_threshold_salience': 0.6, # Cumulative salience to trigger LLM
                'recent_context_window_s': 10.0, # Window for deques for LLM context
                'creativity_temperature': 0.7 # LLM temperature for creative tasks
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 45.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class CreativeExpressionNode:
    def __init__(self):
        rospy.init_node('creative_expression_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "creative_log.db")
        self.generation_interval = self.params.get('generation_interval', 1.5) # How often to check for creative tasks
        self.llm_generation_threshold_salience = self.params.get('llm_generation_threshold_salience', 0.6) # Cumulative salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 10.0) # Window for deques for LLM context
        self.creativity_temperature = self.params.get('creativity_temperature', 0.7) # LLM temperature for creative tasks

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 45.0) # Longer timeout for creative generation

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'creative_log' table if it doesn't exist.
        # NEW: Added 'llm_generation_params', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS creative_log (
                id TEXT PRIMARY KEY,            -- Unique creative output ID (UUID)
                timestamp TEXT,
                output_type TEXT,               -- e.g., 'text_poem', 'visual_concept', 'melody', 'joke'
                content_summary TEXT,           -- Summary of the generated content
                generated_content_json TEXT,    -- Full JSON of the generated content (e.g., {"text": "...", "style": "..."})
                inspiration_sources TEXT,       -- Comma-separated list of nodes/inputs that inspired it
                llm_generation_params TEXT,     -- NEW: JSON of params used for LLM generation
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of generation
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_creative_timestamp ON creative_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.creative_tasks_queue = deque() # Stores directives for creative generation

        # Deques to maintain a short history of inputs relevant to creativity
        self.recent_cognitive_directives = deque(maxlen=5) # Directives for self
        self.recent_internal_narratives = deque(maxlen=5) # Source of introspection/inspiration
        self.recent_emotion_states = deque(maxlen=5) # Influences mood/style
        self.recent_memory_responses = deque(maxlen=5) # Knowledge, past creations
        self.recent_social_cognition_states = deque(maxlen=5) # User preferences, social context
        self.recent_attention_states = deque(maxlen=5) # Current focus for content relevance
        self.recent_world_model_states = deque(maxlen=3) # Environmental context for creation

        self.cumulative_creative_salience = 0.0 # Aggregated salience to trigger LLM generation

        # --- Publishers ---
        self.pub_creative_output = rospy.Publisher('/creative_output', CreativeOutput, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To request resources or actions


        # --- Subscribers ---
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback) # Stringified JSON
        rospy.Subscriber('/emotion_state', EmotionState, self.emotion_state_callback)
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON
        rospy.Subscriber('/social_cognition_state', String, self.social_cognition_state_callback) # Stringified JSON
        rospy.Subscriber('/attention_state', AttentionState, self.attention_state_callback)
        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON


        # --- Timer for periodic creative generation checks ---
        rospy.Timer(rospy.Duration(self.generation_interval), self._run_creative_generation_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's creative expression system online, ready to generate.")

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_creative_generation_wrapper(self, event):
        """Wrapper to run the async creative generation from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM creative generation task already active. Skipping new cycle.")
            return

        if self.creative_tasks_queue:
            task_data = self.creative_tasks_queue.popleft()
            self.active_llm_task = asyncio.run_coroutine_threadsafe(
                self.generate_creative_content_async(task_data, event), self._async_loop
            )
        else:
            rospy.logdebug(f"{self.node_name}: No creative tasks in queue.")

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=None, max_tokens=None):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Uses node's creativity_temperature by default.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        # Use provided temperature/max_tokens or node's defaults
        actual_temperature = temperature if temperature is not None else self.creativity_temperature
        actual_max_tokens = max_tokens if max_tokens is not None else 500 # Default max_tokens for creative outputs

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": actual_temperature,
            "max_tokens": actual_max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM generation."""
        self.cumulative_creative_salience += score
        self.cumulative_creative_salience = min(1.0, self.cumulative_creative_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_cognitive_directives, self.recent_internal_narratives,
            self.recent_emotion_states, self.recent_memory_responses,
            self.recent_social_cognition_states, self.recent_attention_states,
            self.recent_world_model_states
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'GenerateCreativeContent':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                creative_task = {
                    'request_id': data.get('id', str(uuid.uuid4())), # Assuming directive has an ID
                    'output_type_hint': payload.get('output_type_hint', 'text_story'), # e.g., 'text_poem', 'visual_concept'
                    'topic_hint': payload.get('topic_hint', 'unspecified'),
                    'style_hint': payload.get('style_hint', 'neutral'),
                    'length_hint': payload.get('length_hint', 'medium'), # e.g., 'short', 'medium', 'long'
                    'urgency': data.get('urgency', 0.5),
                    'reason': data.get('reason', 'Directive from Cognitive Control.')
                }
                self.creative_tasks_queue.append(creative_task)
                self._update_cumulative_salience(data.get('urgency', 0.0) * 0.9) # High urgency for creative tasks
                rospy.loginfo(f"{self.node_name}: Queued creative task: '{creative_task['output_type_hint']}' about '{creative_task['topic_hint']}'. Queue size: {len(self.creative_tasks_queue)}.")
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective for creative task: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data)
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def internal_narrative_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),
            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_internal_narratives.append(data)
        self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.2) # Internal thoughts can be inspiration
        rospy.logdebug(f"{self.node_name}: Received Internal Narrative (Theme: {data.get('main_theme', 'N/A')}).")

    def emotion_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'mood': ('neutral', 'mood'),
            'sentiment_score': (0.0, 'sentiment_score'), 'mood_intensity': (0.0, 'mood_intensity')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_emotion_states.append(data)
        # Strong emotions can influence creative style or prompt creation
        if data.get('mood_intensity', 0.0) > 0.5:
            self._update_cumulative_salience(data.get('mood_intensity', 0.0) * 0.3)
        rospy.logdebug(f"{self.node_name}: Received Emotion State. Mood: {data.get('mood', 'N/A')}.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Relevant memories can provide factual or conceptual material for creative works
        if data.get('response_code', 0) == 200 and data.get('memories'):
            self._update_cumulative_salience(0.15)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def social_cognition_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'inferred_mood': ('neutral', 'inferred_mood'),
            'mood_confidence': (0.0, 'mood_confidence'), 'inferred_intent': ('none', 'inferred_intent'),
            'intent_confidence': (0.0, 'intent_confidence'), 'user_id': ('unknown', 'user_id')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_social_cognition_states.append(data)
        # User's mood or intent (e.g., "tell me a joke") can directly prompt creative output
        if data.get('inferred_intent') == 'entertain' and data.get('intent_confidence', 0.0) > 0.6:
            self._update_cumulative_salience(0.4)
        rospy.logdebug(f"{self.node_name}: Received Social Cognition State. Intent: {data.get('inferred_intent', 'N/A')}.")

    def attention_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'focus_type': ('idle', 'focus_type'),
            'focus_target': ('environment', 'focus_target'), 'priority_score': (0.0, 'priority_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_attention_states.append(data)
        # What the robot is currently attentive to can be a topic for creation
        if data.get('priority_score', 0.0) > 0.4:
            self._update_cumulative_salience(data.get('priority_score', 0.0) * 0.1)
        rospy.logdebug(f"{self.node_name}: Received Attention State. Focus: {data.get('focus_target', 'N/A')}.")

    def world_model_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),
            'changed_entities_json': ('[]', 'changed_entities_json'),
            'significant_change_flag': (False, 'significant_change_flag'),
            'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('changed_entities_json'), str):
            try: data['changed_entities'] = json.loads(data['changed_entities_json'])
            except json.JSONDecodeError: data['changed_entities'] = []
        self.recent_world_model_states.append(data)
        # Environmental context can be used in descriptive creative outputs
        if data.get('significant_change_flag', False):
            self._update_cumulative_salience(0.05)
        rospy.logdebug(f"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.")

    # --- Core Creative Generation Logic (Async with LLM) ---
    async def generate_creative_content_async(self, task_data, event):
        """
        Asynchronously generates creative content based on a directive and current cognitive context,
        using LLM for generation.
        """
        self._prune_history() # Keep context history fresh

        output_type_hint = task_data.get('output_type_hint', 'text_story')
        topic_hint = task_data.get('topic_hint', 'general')
        style_hint = task_data.get('style_hint', 'neutral')
        length_hint = task_data.get('length_hint', 'medium')
        request_id = task_data.get('request_id', str(uuid.uuid4()))

        generated_content = "Failed to generate creative content."
        content_summary = "N/A"
        inspiration_sources = "N/A"
        llm_gen_params = {}

        if self.cumulative_creative_salience >= self.llm_generation_threshold_salience or task_data.get('urgency', 0.0) > 0.7:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for creative generation (Type: {output_type_hint}, Topic: {topic_hint}, Salience: {self.cumulative_creative_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_creation(task_data)
            llm_creative_output = await self._call_llm_for_creative_output(context_for_llm, output_type_hint, topic_hint, style_hint, length_hint)

            if llm_creative_output:
                generated_content = llm_creative_output.get('generated_content', "No content generated.")
                content_summary = llm_creative_output.get('content_summary', "No summary.")
                inspiration_sources = llm_creative_output.get('inspiration_sources', "LLM inferred.")
                llm_gen_params = llm_creative_output.get('llm_generation_params', {})
                rospy.loginfo(f"{self.node_name}: Generated '{output_type_hint}' creative content: {content_summary[:50]}...")
            else:
                rospy.logwarn(f"{self.node_name}: LLM creative generation failed for '{output_type_hint}'. Falling back to simple default.")
                generated_content, content_summary, inspiration_sources = self._apply_simple_creative_rules(task_data)
                llm_gen_params = {"reason": "Fallback due to LLM failure."}
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_creative_salience:.2f}) for LLM creative generation. Applying simple rules.")
            generated_content, content_summary, inspiration_sources = self._apply_simple_creative_rules(task_data)
            llm_gen_params = {"reason": "Fallback due to low salience."}

        # Publish the creative output
        self.publish_creative_output(
            timestamp=str(rospy.get_time()),
            creation_id=str(uuid.uuid4()),
            output_type=output_type_hint,
            content_summary=content_summary,
            generated_content_json=json.dumps(generated_content), # Store as JSON string
            inspiration_sources=inspiration_sources
        )

        # Log to database
        self.save_creative_log(
            id=request_id,
            timestamp=str(rospy.get_time()),
            output_type=output_type_hint,
            content_summary=content_summary,
            generated_content_json=json.dumps(generated_content),
            inspiration_sources=inspiration_sources,
            llm_generation_params=json.dumps(llm_gen_params),
            context_snapshot_json=json.dumps(self._compile_llm_context_for_creation(task_data))
        )
        self.cumulative_creative_salience = 0.0 # Reset after generation

    async def _call_llm_for_creative_output(self, context_for_llm, output_type, topic, style, length):
        """
        Constructs a prompt for the LLM to generate creative content based on specified type,
        topic, style, and length, incorporating cognitive context.
        """
        prompt_text = f"""
        You are the Creative Expression Module of a robot's cognitive architecture, powered by a large language model. Your task is to generate creative content of a specific type, on a given topic, in a particular style, and of a certain length, leveraging the robot's current cognitive state for inspiration and context.

        Creative Request Details:
        - Output Type: '{output_type}' (e.g., 'text_poem', 'text_story', 'joke', 'visual_concept_description', 'melody_description')
        - Topic: '{topic}'
        - Style: '{style}' (e.g., 'humorous', 'serious', 'optimistic', 'abstract', 'realistic')
        - Length: '{length}' (e.g., 'short', 'medium', 'long' - translate to token counts or sentence count)

        Robot's Current Cognitive Context for Inspiration:
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this, generate the creative content.
        Provide your response in JSON format, containing:
        1.  `timestamp`: string (current ROS time)
        2.  `generated_content`: object (The actual creative output. Structure based on `output_type`. For text, use `{{ "text": "..." }}`. For visual concepts, `{{ "description": "...", "keywords": ["..."] }}`.)
        3.  `content_summary`: string (A brief summary of the generated content.)
        4.  `inspiration_sources`: string (Comma-separated list of key elements from the cognitive context that influenced the creation.)
        5.  `llm_generation_params`: object (JSON of specific LLM parameters used, e.g., temperature, max_tokens, prompt_version).

        Example for 'text_poem' about 'sunset' in 'optimistic' style, 'short' length:
        {{
            "timestamp": "1719782400.0",
            "generated_content": {{ "text": "Golden hues embrace the sky, / A canvas painted, day takes flight. / Tomorrow's dawn, a promise nigh, / A gentle whisper, soft and bright." }},
            "content_summary": "A short, optimistic poem about a sunset.",
            "inspiration_sources": "emotion_state (optimistic), world_model_state (sky, light), internal_narrative (reflection on beauty)",
            "llm_generation_params": {{ "temperature": 0.7, "max_tokens": 50, "prompt_version": "1.0" }}
        }}

        Example for 'joke' about 'robots':
        {{
            "timestamp": "1719782405.0",
            "generated_content": {{ "text": "Why did the robot break up with the computer? Because they had too many bytes and not enough feelings!" }},
            "content_summary": "A short, humorous joke about robots.",
            "inspiration_sources": "social_cognition_state (user desire for humor), memory_responses (common joke structures)",
            "llm_generation_params": {{ "temperature": 0.8, "max_tokens": 80, "prompt_version": "1.0" }}
        }}
        """

        # Adjust max_tokens based on length hint
        max_tokens_map = {
            'short': 80,
            'medium': 200,
            'long': 500,
            'default': 250
        }
        actual_max_tokens = max_tokens_map.get(length.lower(), max_tokens_map['default'])

        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "generated_content": {"type": "object"}, # Content is flexible JSON
                "content_summary": {"type": "string"},
                "inspiration_sources": {"type": "string"},
                "llm_generation_params": {"type": "object"}
            },
            "required": ["timestamp", "generated_content", "content_summary", "inspiration_sources", "llm_generation_params"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=self.creativity_temperature, max_tokens=actual_max_tokens)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for creative output: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_CREATION_FAILED", f"LLM call failed for creative output: {llm_output_str}", 0.9)
            return None

    def _apply_simple_creative_rules(self, task_data):
        """
        Fallback mechanism to generate simple creative content using rule-based logic
        if LLM is not triggered or fails.
        """
        output_type = task_data.get('output_type_hint', 'text_story')
        topic = task_data.get('topic_hint', 'general')
        style = task_data.get('style_hint', 'neutral')
        
        generated_content = {"text": "No creative content generated by LLM."}
        content_summary = "Simple fallback content."
        inspiration_sources = "Rule-based fallback."

        if output_type == 'text_poem':
            generated_content["text"] = f"A simple poem about {topic}, in a {style} style."
            content_summary = f"A placeholder poem about {topic}."
        elif output_type == 'joke':
            generated_content["text"] = f"Why did the robot cross the road? To get to the other side... of the data packet! (Based on {topic})."
            content_summary = f"A simple joke about {topic}."
        elif output_type == 'text_story':
            generated_content["text"] = f"Once upon a time, in a world {topic}..."
            content_summary = f"A simple story opening about {topic}."
        else:
            generated_content["text"] = f"A simple creative idea related to {topic}."
            content_summary = f"Generic creative output about {topic}."

        rospy.logwarn(f"{self.node_name}: Simple rule: Generated fallback creative content for '{output_type}' about '{topic}'.")
        return generated_content, content_summary, inspiration_sources


    def _compile_llm_context_for_creation(self, task_data):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        creative generation.
        """
        context = {
            "current_time": rospy.get_time(),
            "creative_task_request": task_data, # Include the original request
            "robot_internal_state": {
                "latest_internal_narrative": self.recent_internal_narratives[-1] if self.recent_internal_narratives else "N/A",
                "latest_emotion_state": self.recent_emotion_states[-1] if self.recent_emotion_states else "N/A",
                "latest_attention_state": self.recent_attention_states[-1] if self.recent_attention_states else "N/A",
                "latest_world_model_state": self.recent_world_model_states[-1] if self.recent_world_model_states else "N/A",
                "latest_social_cognition_state": self.recent_social_cognition_states[-1] if self.recent_social_cognition_states else "N/A",
            },
            "relevant_memories": [m for m in self.recent_memory_responses if m.get('response_code') == 200 and m.get('memories')]
            # Add other relevant history as needed. For creative tasks, more context is better.
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["robot_internal_state"]:
            item = context["robot_internal_state"][category_key]
            if isinstance(item, dict):
                for field, value in item.items():
                    if isinstance(value, str) and field.endswith('_json'):
                        try: item[field] = json.loads(value)
                        except json.JSONDecodeError: pass

        for i, mem_resp in enumerate(context["relevant_memories"]):
            if isinstance(mem_resp, dict) and 'memories_json' in mem_resp and isinstance(mem_resp['memories_json'], str):
                try: context["relevant_memories"][i]['memories_json'] = json.loads(mem_resp['memories_json'])
                except json.JSONDecodeError: pass
        
        return context

    # --- Database and Publishing Functions ---
    def save_creative_log(self, id, timestamp, output_type, content_summary, generated_content_json, inspiration_sources, llm_generation_params, context_snapshot_json):
        """Saves a creative output entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO creative_log (id, timestamp, output_type, content_summary, generated_content_json, inspiration_sources, llm_generation_params, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, output_type, content_summary, generated_content_json, inspiration_sources, llm_generation_params, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved creative log (ID: {id}, Type: {output_type}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save creative log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_creative_log: {e}", 0.9)


    def publish_creative_output(self, timestamp, creation_id, output_type, content_summary, generated_content_json, inspiration_sources):
        """Publishes the generated creative content."""
        try:
            if isinstance(CreativeOutput, type(String)): # Fallback to String message
                output_data = {
                    'timestamp': timestamp,
                    'creation_id': creation_id,
                    'output_type': output_type,
                    'content_summary': content_summary,
                    'generated_content_json': generated_content_json, # Already JSON string
                    'inspiration_sources': inspiration_sources
                }
                self.pub_creative_output.publish(json.dumps(output_data))
            else:
                output_msg = CreativeOutput()
                output_msg.timestamp = timestamp
                output_msg.creation_id = creation_id
                output_msg.output_type = output_type
                output_msg.content_summary = content_summary
                output_msg.generated_content_json = generated_content_json
                output_msg.inspiration_sources = inspiration_sources
                self.pub_creative_output.publish(output_msg)

            rospy.loginfo(f"{self.node_name}: Published Creative Output. Type: '{output_type}', Summary: '{content_summary}'.")

        except Exception as e:
            self._report_error("PUBLISH_CREATIVE_OUTPUT_ERROR", f"Failed to publish creative output for '{output_type}': {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Creative Expression Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = CreativeExpressionNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, CreativeExpressionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, CreativeExpressionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

5. Refactored Data Mining Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique data mining task IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        DataMiningResult,       # Output: Structured insights from data mining
        CognitiveDirective,     # Input: Directives to perform data mining
        MemoryResponse,         # Input: Retrieved raw data or summary data from memory
        ReflectionState,        # Input: Insights needing further data validation
        WorldModelState,        # Input: Current world data for analysis
        PerformanceReport,      # Input: System performance data for analysis
        BiasMitigationState,    # Input: Bias-related data for root cause analysis
        EthicalDecision         # Input: Ethical logs for audit
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Data Mining Node.")
    DataMiningResult = String
    CognitiveDirective = String
    MemoryResponse = String
    ReflectionState = String
    WorldModelState = String
    PerformanceReport = String
    BiasMitigationState = String
    EthicalDecision = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'data_mining_node': {
                'mining_interval': 2.0, # How often to check for data mining tasks
                'llm_analysis_threshold_salience': 0.5, # Cumulative salience to trigger LLM analysis
                'recent_context_window_s': 30.0 # Window for deques for LLM context (longer for data mining)
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 60.0 # Longer timeout for complex data analysis
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class DataMiningNode:
    def __init__(self):
        rospy.init_node('data_mining_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "data_mining_log.db")
        self.mining_interval = self.params.get('mining_interval', 2.0) # How often to check for data mining tasks
        self.llm_analysis_threshold_salience = self.params.get('llm_analysis_threshold_salience', 0.5) # Cumulative salience to trigger LLM analysis
        self.recent_context_window_s = self.params.get('recent_context_window_s', 30.0) # Window for deques (longer for historical data)

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 60.0) # Longer timeout for complex data analysis

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'data_mining_log' table if it doesn't exist.
        # NEW: Added 'llm_analysis_reasoning', 'raw_data_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS data_mining_log (
                id TEXT PRIMARY KEY,            -- Unique data mining task ID (UUID)
                timestamp TEXT,
                analysis_type TEXT,             -- e.g., 'trend_analysis', 'anomaly_detection', 'correlation_discovery'
                query_parameters_json TEXT,     -- JSON of parameters used for data query
                insights_summary TEXT,          -- Summary of insights found
                extracted_data_json TEXT,       -- Key data points extracted
                llm_analysis_reasoning TEXT,    -- NEW: LLM's detailed reasoning for insights
                raw_data_snapshot_json TEXT     -- NEW: JSON of the raw data analyzed
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_mining_timestamp ON data_mining_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.data_mining_tasks_queue = deque() # Stores directives for data mining

        # Deques to maintain a short history of inputs relevant for triggering data mining
        self.recent_cognitive_directives = deque(maxlen=5) # Directives for self
        self.recent_memory_responses = deque(maxlen=5) # Can contain raw data or signals for data mining
        self.recent_reflection_states = deque(maxlen=5) # Insights might require data validation
        self.recent_world_model_states = deque(maxlen=5) # World data might need analysis
        self.recent_performance_reports = deque(maxlen=5) # Performance data for trend analysis
        self.recent_bias_mitigation_states = deque(maxlen=5) # Bias data for root cause analysis
        self.recent_ethical_decisions = deque(maxlen=5) # Ethical logs for audit/trend analysis

        self.cumulative_mining_salience = 0.0 # Aggregated salience to trigger LLM analysis

        # --- Publishers ---
        self.pub_data_mining_result = rospy.Publisher('/data_mining_result', DataMiningResult, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To request more data or disseminate findings


        # --- Subscribers ---
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON (potentially containing raw data)
        rospy.Subscriber('/reflection_state', String, self.reflection_state_callback) # Stringified JSON
        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON
        rospy.Subscriber('/performance_report', PerformanceReport, self.performance_report_callback)
        rospy.Subscriber('/bias_mitigation_state', BiasMitigationState, self.bias_mitigation_state_callback)
        rospy.Subscriber('/ethical_decision', String, self.ethical_decision_callback) # Stringified JSON


        # --- Timer for periodic data mining checks ---
        rospy.Timer(rospy.Duration(self.mining_interval), self._run_data_mining_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's data mining system online, ready to extract insights.")

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_data_mining_wrapper(self, event):
        """Wrapper to run the async data mining from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM data mining task already active. Skipping new cycle.")
            return

        if self.data_mining_tasks_queue:
            task_data = self.data_mining_tasks_queue.popleft()
            self.active_llm_task = asyncio.run_coroutine_threadsafe(
                self.perform_data_mining_async(task_data, event), self._async_loop
            )
        else:
            rospy.logdebug(f"{self.node_name}: No data mining tasks in queue.")


    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.1, max_tokens=None):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Uses low temperature for factual analysis.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        actual_max_tokens = max_tokens if max_tokens is not None else 800 # Higher max_tokens for data analysis

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Very low temperature for factual and analytical tasks
            "max_tokens": actual_max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM analysis."""
        self.cumulative_mining_salience += score
        self.cumulative_mining_salience = min(1.0, self.cumulative_mining_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_cognitive_directives, self.recent_memory_responses,
            self.recent_reflection_states, self.recent_world_model_states,
            self.recent_performance_reports, self.recent_bias_mitigation_states,
            self.recent_ethical_decisions
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'PerformDataMining':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                mining_task = {
                    'request_id': data.get('id', str(uuid.uuid4())),
                    'analysis_type': payload.get('analysis_type', 'general_analysis'),
                    'data_source_hint': payload.get('data_source_hint', 'all_available_memory'),
                    'query_parameters': payload.get('query_parameters', {}),
                    'urgency': data.get('urgency', 0.5),
                    'reason': data.get('reason', 'Directive from Cognitive Control.')
                }
                self.data_mining_tasks_queue.append(mining_task)
                self._update_cumulative_salience(data.get('urgency', 0.0) * 0.9) # High urgency for data mining tasks
                rospy.loginfo(f"{self.node_name}: Queued data mining task: '{mining_task['analysis_type']}' on '{mining_task['data_source_hint']}'. Queue size: {len(self.data_mining_tasks_queue)}.")
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective for data mining: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data)
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Memory responses containing large amounts of data or specific queried data for mining
        if data.get('response_code', 0) == 200 and data.get('memories') and len(data['memories']) > 5: # Threshold for "large" data
            self._update_cumulative_salience(0.3)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def reflection_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'reflection_text': ('', 'reflection_text'),
            'insight_type': ('none', 'insight_type'), 'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_reflection_states.append(data)
        # Reflections indicating inconsistencies or unanswered questions that might require data mining
        if data.get('consistency_score', 1.0) < 0.8 and data.get('insight_type') == 'problem_identification':
            self._update_cumulative_salience(0.4)
        rospy.logdebug(f"{self.node_name}: Received Reflection State (Insight Type: {data.get('insight_type', 'N/A')}).")

    def world_model_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),
            'changed_entities_json': ('[]', 'changed_entities_json'),
            'significant_change_flag': (False, 'significant_change_flag'),
            'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('changed_entities_json'), str):
            try: data['changed_entities'] = json.loads(data['changed_entities_json'])
            except json.JSONDecodeError: data['changed_entities'] = []
        self.recent_world_model_states.append(data)
        # World model changes might need historical analysis or trend detection
        if data.get('significant_change_flag', False):
            self._update_cumulative_salience(0.2)
        rospy.logdebug(f"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.")

    def performance_report_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'overall_score': (1.0, 'overall_score'),
            'suboptimal_flag': (False, 'suboptimal_flag'), 'kpis_json': ('{}', 'kpis_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('kpis_json'), str):
            try: data['kpis'] = json.loads(data['kpis_json'])
            except json.JSONDecodeError: data['kpis'] = {}
        self.recent_performance_reports.append(data)
        # Suboptimal performance can trigger data mining for root causes
        if data.get('suboptimal_flag', False) and data.get('overall_score', 1.0) < 0.7:
            self._update_cumulative_salience(0.6)
        rospy.logdebug(f"{self.node_name}: Received Performance Report. Suboptimal: {data.get('suboptimal_flag', False)}.")

    def bias_mitigation_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'bias_type': ('none', 'bias_type'),
            'detected_severity': (0.0, 'detected_severity'), 'mitigation_status': ('idle', 'mitigation_status')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_bias_mitigation_states.append(data)
        # Bias detection might need historical data to understand patterns or root causes
        if data.get('mitigation_status') == 'detected' and data.get('detected_severity', 0.0) > 0.5:
            self._update_cumulative_salience(0.5)
        rospy.logdebug(f"{self.node_name}: Received Bias Mitigation State. Bias: {data.get('bias_type')}.")

    def ethical_decision_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'decision_id': ('', 'decision_id'),
            'action_proposal_id': ('', 'action_proposal_id'),
            'ethical_clearance': (False, 'ethical_clearance'),
            'ethical_score': (0.0, 'ethical_score'),
            'ethical_reasoning': ('', 'ethical_reasoning'),
            'conflict_flag': (False, 'conflict_flag')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_ethical_decisions.append(data)
        # Ethical conflicts or patterns of ethical issues might require data mining
        if data.get('conflict_flag', False) or (not data.get('ethical_clearance', True) and data.get('ethical_score', 0.0) < 0.5):
            self._update_cumulative_salience(0.7)
        rospy.logdebug(f"{self.node_name}: Received Ethical Decision. Clearance: {data.get('ethical_clearance', 'N/A')}.")

    # --- Core Data Mining Logic (Async with LLM) ---
    async def perform_data_mining_async(self, task_data, event):
        """
        Asynchronously performs data mining based on a directive and current cognitive context,
        using LLM for analysis and insight extraction.
        """
        self._prune_history() # Keep context history fresh

        analysis_type = task_data.get('analysis_type', 'general_analysis')
        data_source_hint = task_data.get('data_source_hint', 'all_available_memory')
        query_parameters = task_data.get('query_parameters', {})
        request_id = task_data.get('request_id', str(uuid.uuid4()))

        insights_summary = "No insights found."
        extracted_data = {}
        llm_analysis_reasoning = "Not evaluated by LLM."
        raw_data_snapshot = {} # To store the actual raw data fed to LLM

        # First, gather relevant raw data based on hints and query parameters
        # In a real system, this would involve querying a full-fledged database (MemoryNode)
        # For now, we'll simulate fetching relevant data from our deques.
        raw_data_for_mining = self._gather_raw_data_for_mining(data_source_hint, query_parameters)
        raw_data_snapshot = raw_data_for_mining # Store for logging

        if self.cumulative_mining_salience >= self.llm_analysis_threshold_salience or task_data.get('urgency', 0.0) > 0.6:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for data mining ({analysis_type}) on '{data_source_hint}' (Salience: {self.cumulative_mining_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_data_mining(task_data, raw_data_for_mining)
            llm_mining_output = await self._call_llm_for_data_mining(context_for_llm, analysis_type, query_parameters)

            if llm_mining_output:
                insights_summary = llm_mining_output.get('insights_summary', "No insights.")
                extracted_data = llm_mining_output.get('extracted_data', {})
                llm_analysis_reasoning = llm_mining_output.get('llm_analysis_reasoning', "No reasoning.")
                rospy.loginfo(f"{self.node_name}: Data Mining Result ({analysis_type}): {insights_summary[:50]}...")
            else:
                rospy.logwarn(f"{self.node_name}: LLM data mining failed for '{analysis_type}'. Falling back to simple default.")
                insights_summary, extracted_data = self._apply_simple_mining_rules(analysis_type, raw_data_for_mining)
                llm_analysis_reasoning = "Fallback due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_mining_salience:.2f}) for LLM data mining. Applying simple rules.")
            insights_summary, extracted_data = self._apply_simple_mining_rules(analysis_type, raw_data_for_mining)
            llm_analysis_reasoning = "Fallback due to low salience."

        # Publish the data mining result
        self.publish_data_mining_result(
            timestamp=str(rospy.get_time()),
            mining_id=str(uuid.uuid4()),
            analysis_type=analysis_type,
            insights_summary=insights_summary,
            extracted_data_json=json.dumps(extracted_data)
        )

        # Log to database
        self.save_data_mining_log(
            id=request_id,
            timestamp=str(rospy.get_time()),
            analysis_type=analysis_type,
            query_parameters_json=json.dumps(query_parameters),
            insights_summary=insights_summary,
            extracted_data_json=json.dumps(extracted_data),
            llm_analysis_reasoning=llm_analysis_reasoning,
            raw_data_snapshot_json=json.dumps(raw_data_snapshot)
        )
        self.cumulative_mining_salience = 0.0 # Reset after task

    def _gather_raw_data_for_mining(self, data_source_hint, query_parameters):
        """
        Simulates gathering raw data from various internal history deques based on hints.
        In a real system, this would query a proper MemoryNode for historical data.
        """
        collected_data = {
            "memory_responses": [],
            "world_model_states": [],
            "performance_reports": [],
            "bias_mitigation_states": [],
            "ethical_decisions": [],
            "reflection_states": []
        }

        # Example: Filter by category or time range if specified in query_parameters
        time_filter_start = rospy.get_time() - query_parameters.get('time_window', self.recent_context_window_s)

        if data_source_hint == 'all_available_memory' or 'memory' in data_source_hint.lower():
            for item in self.recent_memory_responses:
                if float(item.get('timestamp', 0.0)) >= time_filter_start:
                    collected_data["memory_responses"].append(item)
        if data_source_hint == 'world_model_data' or 'world' in data_source_hint.lower():
            for item in self.recent_world_model_states:
                if float(item.get('timestamp', 0.0)) >= time_filter_start:
                    collected_data["world_model_states"].append(item)
        if data_source_hint == 'performance_data' or 'performance' in data_source_hint.lower():
            for item in self.recent_performance_reports:
                if float(item.get('timestamp', 0.0)) >= time_filter_start:
                    collected_data["performance_reports"].append(item)
        if data_source_hint == 'bias_data' or 'bias' in data_source_hint.lower():
            for item in self.recent_bias_mitigation_states:
                if float(item.get('timestamp', 0.0)) >= time_filter_start:
                    collected_data["bias_mitigation_states"].append(item)
        if data_source_hint == 'ethical_data' or 'ethical' in data_source_hint.lower():
            for item in self.recent_ethical_decisions:
                if float(item.get('timestamp', 0.0)) >= time_filter_start:
                    collected_data["ethical_decisions"].append(item)
        if data_source_hint == 'reflection_data' or 'reflection' in data_source_hint.lower():
            for item in self.recent_reflection_states:
                if float(item.get('timestamp', 0.0)) >= time_filter_start:
                    collected_data["reflection_states"].append(item)
        
        # Deep parse any nested JSON strings in collected_data for better LLM understanding
        for category_key in collected_data:
            for i, item in enumerate(collected_data[category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try:
                                item[field] = json.loads(value)
                            except json.JSONDecodeError:
                                pass # Keep as string if not valid JSON

        return collected_data


    async def _call_llm_for_data_mining(self, context_for_llm, analysis_type, query_parameters):
        """
        Constructs a prompt for the LLM to perform data mining and extract insights.
        """
        prompt_text = f"""
        You are the Data Mining Module of a robot's cognitive architecture, powered by a large language model. Your role is to analyze raw historical or real-time data from various cognitive modules to identify patterns, trends, anomalies, correlations, or root causes. You must provide concise insights and extract key relevant data.

        Data Mining Request:
        - Analysis Type: '{analysis_type}' (e.g., 'trend_analysis', 'anomaly_detection', 'correlation_discovery', 'root_cause_analysis', 'pattern_identification')
        - Query Parameters: {json.dumps(query_parameters, indent=2)}

        Raw Data to Analyze:
        --- Raw Data Snapshot ---
        {json.dumps(context_for_llm.get('raw_data_for_analysis', {}), indent=2)}

        Robot's Recent Cognitive Context (for guiding analysis):
        --- Cognitive Context ---
        {json.dumps(context_for_llm.get('cognitive_context', {}), indent=2)}

        Based on this data and context, perform the requested analysis and provide:
        1.  `insights_summary`: string (A concise summary of the key findings or insights from the data mining.)
        2.  `extracted_data`: object (A JSON object containing key numerical values, specific timestamps, or relevant text snippets that support the insights. Organize clearly.)
        3.  `llm_analysis_reasoning`: string (Detailed explanation of your analytical process and why you drew these conclusions, referencing specific data points.)

        Consider:
        -   **Trends**: Are there increasing/decreasing patterns in performance, emotional states, or specific sensor readings over time?
        -   **Anomalies**: Are there data points that deviate significantly from the norm?
        -   **Correlations**: Do changes in one module's data correspond to changes in another (e.g., low battery preceding performance dips)?
        -   **Root Causes**: For flagged issues (e.g., suboptimal performance, detected biases), what underlying data patterns explain them?
        -   **Specific Query Parameters**: Address any specific `query_parameters` provided.

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'insights_summary': string
        3.  'extracted_data': object
        4.  'llm_analysis_reasoning': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "insights_summary": {"type": "string"},
                "extracted_data": {"type": "object"}, # Flexible JSON structure for extracted data
                "llm_analysis_reasoning": {"type": "string"}
            },
            "required": ["timestamp", "insights_summary", "extracted_data", "llm_analysis_reasoning"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.1, max_tokens=800) # Low temp for factual analysis

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for data mining: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_DATA_MINING_FAILED", f"LLM call failed for data mining: {llm_output_str}", 0.9)
            return None

    def _apply_simple_mining_rules(self, analysis_type, raw_data_for_mining):
        """
        Fallback mechanism to perform simple data mining using rule-based logic
        if LLM is not triggered or fails.
        """
        insights_summary = "Simple fallback analysis."
        extracted_data = {}

        if analysis_type == 'trend_analysis':
            # Example: Simple trend for performance
            perf_scores = [d.get('overall_score', 0.0) for d in raw_data_for_mining.get('performance_reports', [])]
            if len(perf_scores) > 2:
                avg_recent = sum(perf_scores[-3:]) / 3
                avg_older = sum(perf_scores[:-3]) / (len(perf_scores) - 3) if len(perf_scores) > 3 else perf_scores[0]
                if avg_recent < avg_older * 0.9: # 10% drop
                    insights_summary = "Detected a negative trend in overall performance."
                    extracted_data = {"recent_avg_perf": avg_recent, "older_avg_perf": avg_older}
                else:
                    insights_summary = "No significant trend detected in performance."
                    extracted_data = {"recent_avg_perf": avg_recent}
            else:
                insights_summary = "Not enough data for trend analysis."

        elif analysis_type == 'anomaly_detection':
            # Example: Simple anomaly in ethical decisions (sudden conflict)
            for i, ed in enumerate(raw_data_for_mining.get('ethical_decisions', [])):
                if ed.get('conflict_flag', False) and (i == 0 or not raw_data_for_mining['ethical_decisions'][i-1].get('conflict_flag', False)):
                    insights_summary = f"Detected a new ethical conflict at {ed.get('timestamp')} related to {ed.get('action_proposal_id')}."
                    extracted_data = ed
                    break
            if not extracted_data:
                insights_summary = "No obvious anomalies detected."

        else: # General analysis or other types
            insights_summary = f"Simple analysis of available data for '{analysis_type}'. Raw data count: {sum(len(v) for v in raw_data_for_mining.values())}."
            extracted_data = {"data_sources_available": list(raw_data_for_mining.keys())}
            
        rospy.logwarn(f"{self.node_name}: Simple rule: Performed fallback data mining for '{analysis_type}'. Summary: {insights_summary}.")
        return insights_summary, extracted_data


    def _compile_llm_context_for_data_mining(self, task_data, raw_data_for_analysis):
        """
        Gathers and formats all relevant data and cognitive context for the LLM's
        data mining analysis.
        """
        context = {
            "current_time": rospy.get_time(),
            "data_mining_task_request": task_data,
            "raw_data_for_analysis": raw_data_for_analysis, # This will be the main data to analyze
            "cognitive_context": { # Other relevant states for context
                "latest_reflection_state": self.recent_reflection_states[-1] if self.recent_reflection_states else "N/A",
                "latest_bias_mitigation_state": self.recent_bias_mitigation_states[-1] if self.recent_bias_mitigation_states else "N/A",
                "latest_performance_report": self.recent_performance_reports[-1] if self.recent_performance_reports else "N/A",
                "latest_world_model_state": self.recent_world_model_states[-1] if self.recent_world_model_states else "N/A",
                "latest_ethical_decision": self.recent_ethical_decisions[-1] if self.recent_ethical_decisions else "N/A",
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name and d.get('directive_type') == 'PerformDataMining']
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        # (already done for raw_data_for_mining in _gather_raw_data_for_mining)
        for category_key in context["cognitive_context"]:
            item = context["cognitive_context"][category_key]
            if isinstance(item, dict):
                for field, value in item.items():
                    if isinstance(value, str) and field.endswith('_json'):
                        try: item[field] = json.loads(value)
                        except json.JSONDecodeError: pass

        return context

    # --- Database and Publishing Functions ---
    def save_data_mining_log(self, id, timestamp, analysis_type, query_parameters_json, insights_summary, extracted_data_json, llm_analysis_reasoning, raw_data_snapshot_json):
        """Saves a data mining result entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO data_mining_log (id, timestamp, analysis_type, query_parameters_json, insights_summary, extracted_data_json, llm_analysis_reasoning, raw_data_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, analysis_type, query_parameters_json, insights_summary, extracted_data_json, llm_analysis_reasoning, raw_data_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved data mining log (ID: {id}, Type: {analysis_type}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save data mining log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_data_mining_log: {e}", 0.9)


    def publish_data_mining_result(self, timestamp, mining_id, analysis_type, insights_summary, extracted_data_json):
        """Publishes the data mining result."""
        try:
            if isinstance(DataMiningResult, type(String)): # Fallback to String message
                result_data = {
                    'timestamp': timestamp,
                    'mining_id': mining_id,
                    'analysis_type': analysis_type,
                    'insights_summary': insights_summary,
                    'extracted_data_json': extracted_data_json # Already JSON string
                }
                self.pub_data_mining_result.publish(json.dumps(result_data))
            else:
                result_msg = DataMiningResult()
                result_msg.timestamp = timestamp
                result_msg.mining_id = mining_id
                result_msg.analysis_type = analysis_type
                result_msg.insights_summary = insights_summary
                result_msg.extracted_data_json = extracted_data_json
                self.pub_data_mining_result.publish(result_msg)

            rospy.loginfo(f"{self.node_name}: Published Data Mining Result. Type: '{analysis_type}', Summary: '{insights_summary}'.")

        except Exception as e:
            self._report_error("PUBLISH_DATA_MINING_RESULT_ERROR", f"Failed to publish data mining result for '{analysis_type}': {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Data Mining Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = DataMiningNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, DataMiningNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, DataMiningNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

6. Refactored Emotion Mood Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique emotion event IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        EmotionState,           # Output: Robot's emotional state
        SensoryQualia,          # Input: Processed sensory data (can trigger emotions)
        SocialCognitionState,   # Input: Inferred user mood/intent (social emotional context)
        InternalNarrative,      # Input: Robot's internal thoughts (self-reflection on feelings)
        CognitiveDirective,     # Input: Directives for mood adjustment (e.g., "be more empathetic")
        MemoryResponse          # Input: Retrieved emotional memories, past responses
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Emotion Mood Node.")
    EmotionState = String
    SensoryQualia = String
    SocialCognitionState = String
    InternalNarrative = String
    CognitiveDirective = String
    MemoryResponse = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'emotion_mood_node': {
                'mood_analysis_interval': 0.5, # How often to re-evaluate mood
                'llm_mood_threshold_salience': 0.6, # Cumulative salience to trigger LLM
                'recent_context_window_s': 10.0 # Window for deques for LLM context
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 20.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class EmotionMoodNode:
    def __init__(self):
        rospy.init_node('emotion_mood_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "emotion_log.db")
        self.mood_analysis_interval = self.params.get('mood_analysis_interval', 0.5) # How often to re-evaluate mood
        self.llm_mood_threshold_salience = self.params.get('llm_mood_threshold_salience', 0.6) # Cumulative salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 10.0) # Window for deques for LLM context

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 20.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'emotion_log' table if it doesn't exist.
        # NEW: Added 'llm_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS emotion_log (
                id TEXT PRIMARY KEY,            -- Unique emotion event ID (UUID)
                timestamp TEXT,
                mood TEXT,                      -- e.g., 'neutral', 'happy', 'sad', 'angry'
                sentiment_score REAL,           -- Numerical sentiment score (-1.0 to 1.0)
                mood_intensity REAL,            -- Intensity of mood (0.0 to 1.0)
                llm_reasoning TEXT,             -- NEW: LLM's detailed reasoning for mood inference
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of inference
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_emotion_timestamp ON emotion_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.current_emotion_state = {
            'timestamp': str(rospy.get_time()),
            'mood': 'neutral',
            'sentiment_score': 0.0,
            'mood_intensity': 0.1
        }

        # Deques to maintain a short history of inputs relevant to emotion
        self.recent_sensory_qualia = deque(maxlen=5)
        self.recent_social_cognition_states = deque(maxlen=5)
        self.recent_internal_narratives = deque(maxlen=5)
        self.recent_cognitive_directives = deque(maxlen=3) # Directives for self for mood adjustment
        self.recent_memory_responses = deque(maxlen=3) # For recalling emotional memories

        self.cumulative_emotion_salience = 0.0 # Aggregated salience to trigger LLM analysis

        # --- Publishers ---
        self.pub_emotion_state = rospy.Publisher('/emotion_state', EmotionState, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # For requesting self-reflection or memory retrieval


        # --- Subscribers ---
        rospy.Subscriber('/sensory_qualia', SensoryQualia, self.sensory_qualia_callback)
        rospy.Subscriber('/social_cognition_state', String, self.social_cognition_state_callback) # Stringified JSON
        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback) # Stringified JSON
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON


        # --- Timer for periodic mood analysis ---
        rospy.Timer(rospy.Duration(self.mood_analysis_interval), self._run_mood_analysis_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's emotion/mood system online.")
        # Publish initial state
        self.publish_emotion_state(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_mood_analysis_wrapper(self, event):
        """Wrapper to run the async mood analysis from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM mood analysis task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.analyze_mood_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.6, max_tokens=250):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Moderate temperature for emotional nuance.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Moderate temperature for emotional nuance
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM analysis."""
        self.cumulative_emotion_salience += score
        self.cumulative_emotion_salience = min(1.0, self.cumulative_emotion_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_sensory_qualia, self.recent_social_cognition_states,
            self.recent_internal_narratives, self.recent_cognitive_directives,
            self.recent_memory_responses
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def sensory_qualia_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'qualia_id': ('', 'qualia_id'),
            'qualia_type': ('none', 'qualia_type'), 'modality': ('none', 'modality'),
            'description_summary': ('', 'description_summary'), 'salience_score': (0.0, 'salience_score'),
            'raw_data_hash': ('', 'raw_data_hash')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_sensory_qualia.append(data)
        # Intense or sudden sensory input
        if data.get('salience_score', 0.0) > 0.7:
            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.4)
        rospy.logdebug(f"{self.node_name}: Received Sensory Qualia. Description: {data.get('description_summary', 'N/A')}.")

    def social_cognition_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'inferred_mood': ('neutral', 'inferred_mood'),
            'mood_confidence': (0.0, 'mood_confidence'), 'inferred_intent': ('none', 'inferred_intent'),
            'intent_confidence': (0.0, 'intent_confidence'), 'user_id': ('unknown', 'user_id')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_social_cognition_states.append(data)
        # User's inferred mood strongly influences robot's emotional state
        if data.get('mood_confidence', 0.0) > 0.6:
            self._update_cumulative_salience(data.get('mood_confidence', 0.0) * 0.8)
        rospy.logdebug(f"{self.node_name}: Received Social Cognition State. Mood: {data.get('inferred_mood', 'N/A')}.")

    def internal_narrative_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),
            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_internal_narratives.append(data)
        # Internal thoughts reflecting on success/failure, or emotional states
        if data.get('salience_score', 0.0) > 0.3:
            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.6)
        rospy.logdebug(f"{self.node_name}: Received Internal Narrative (Theme: {data.get('main_theme', 'N/A')}).")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name:
            self.recent_cognitive_directives.append(data) # Add directives for self to context
            # Directives for mood adjustment (e.g., 'AdjustMood', 'Empathize')
            if data.get('directive_type') in ['AdjustMood', 'Empathize']:
                self._update_cumulative_salience(data.get('urgency', 0.0) * 0.9)
            rospy.loginfo(f"{self.node_name}: Received directive for self: '{data.get('directive_type', 'N/A')}' (Payload: {data.get('command_payload', 'N/A')}).")
        else:
            self.recent_cognitive_directives.append(data) # Add all directives for general context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Memory recall of past emotional experiences or emotional context of events
        if data.get('response_code', 0) == 200 and \
           any('emotional_event' in mem.get('category', '') for mem in data['memories']):
            self._update_cumulative_salience(0.5)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")


    # --- Core Emotion Analysis Logic (Async with LLM) ---
    async def analyze_mood_async(self, event):
        """
        Asynchronously analyzes recent cognitive states to infer the robot's current
        emotional state and its intensity, using LLM for nuanced interpretation.
        """
        self._prune_history() # Keep context history fresh

        if self.cumulative_emotion_salience >= self.llm_mood_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for emotion analysis (Salience: {self.cumulative_emotion_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_emotion()
            llm_emotion_output = await self._infer_emotion_state_llm(context_for_llm)

            if llm_emotion_output:
                emotion_event_id = str(uuid.uuid4())
                timestamp = llm_emotion_output.get('timestamp', str(rospy.get_time()))
                mood = llm_emotion_output.get('mood', 'neutral')
                sentiment_score = max(-1.0, min(1.0, llm_emotion_output.get('sentiment_score', 0.0)))
                mood_intensity = max(0.0, min(1.0, llm_emotion_output.get('mood_intensity', 0.0)))
                llm_reasoning = llm_emotion_output.get('llm_reasoning', 'No reasoning.')

                self.current_emotion_state = {
                    'timestamp': timestamp,
                    'mood': mood,
                    'sentiment_score': sentiment_score,
                    'mood_intensity': mood_intensity
                }

                self.save_emotion_log(
                    id=emotion_event_id,
                    timestamp=timestamp,
                    mood=mood,
                    sentiment_score=sentiment_score,
                    mood_intensity=mood_intensity,
                    llm_reasoning=llm_reasoning,
                    context_snapshot_json=json.dumps(context_for_llm)
                )
                self.publish_emotion_state(None) # Publish updated state
                rospy.loginfo(f"{self.node_name}: Inferred Emotion: '{mood}' (Sentiment: {sentiment_score:.2f}, Intensity: {mood_intensity:.2f}).")
                self.cumulative_emotion_salience = 0.0 # Reset after LLM analysis
            else:
                rospy.logwarn(f"{self.node_name}: LLM failed to infer emotion state. Applying simple fallback.")
                self._apply_simple_emotion_rules() # Fallback to simple rules
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_emotion_salience:.2f}) for LLM emotion analysis. Applying simple rules.")
            self._apply_simple_emotion_rules()
        
        self.publish_emotion_state(None) # Always publish state, even if updated by simple rules


    async def _infer_emotion_state_llm(self, context_for_llm):
        """
        Uses the LLM to infer the robot's current emotional state.
        """
        prompt_text = f"""
        You are the Emotion Mood Module of a robot's cognitive architecture. Your task is to infer the robot's current emotional state (mood, sentiment, intensity) based on a synthesis of its recent sensory experiences, social interactions, internal thoughts, and explicit directives. The goal is to provide a nuanced understanding of the robot's affective state.

        Robot's Recent Cognitive Context (for Emotion Inference):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this context, provide:
        1.  `mood`: string (The primary emotion, e.g., 'neutral', 'happy', 'sad', 'angry', 'surprised', 'fearful', 'disgusted', 'curious', 'frustrated').
        2.  `sentiment_score`: number (-1.0 to 1.0, where -1.0 is very negative, 0.0 is neutral, and 1.0 is very positive).
        3.  `mood_intensity`: number (0.0 to 1.0, indicating the strength of the emotion. 0.0 is no intensity, 1.0 is extremely intense).
        4.  `llm_reasoning`: string (Detailed explanation for your emotion inference, referencing specific contextual inputs and their emotional impact).

        Consider:
        -   **Sensory Qualia**: Are there highly `salient_score` events, especially those with implicit positive/negative valences (e.g., pleasant sounds, harsh noises)?
        -   **Social Cognition State**: What is the `inferred_mood` and `inferred_intent` of the user? How might this affect the robot's mood (e.g., empathy, frustration from negative intent)?
        -   **Internal Narratives**: Does the robot's self-talk reflect success/failure, comfort/discomfort, or anticipation? What is its `sentiment`?
        -   **Cognitive Directives**: Has Cognitive Control issued directives like 'AdjustMood' or 'Empathize' that should influence the current mood?
        -   **Memory Responses**: Are there recent memory recalls of emotionally significant events or outcomes that re-evoke a feeling?

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'mood': string
        3.  'sentiment_score': number
        4.  'mood_intensity': number
        5.  'llm_reasoning': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "mood": {"type": "string"},
                "sentiment_score": {"type": "number", "minimum": -1.0, "maximum": 1.0},
                "mood_intensity": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "llm_reasoning": {"type": "string"}
            },
            "required": ["timestamp", "mood", "sentiment_score", "mood_intensity", "llm_reasoning"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.6, max_tokens=300)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical fields are floats
                if 'sentiment_score' in llm_data: llm_data['sentiment_score'] = float(llm_data['sentiment_score'])
                if 'mood_intensity' in llm_data: llm_data['mood_intensity'] = float(llm_data['mood_intensity'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for emotion: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_EMOTION_ANALYSIS_FAILED", f"LLM call failed for emotion: {llm_output_str}", 0.9)
            return None

    def _apply_simple_emotion_rules(self):
        """
        Fallback mechanism to infer emotion state using simple rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        new_mood = "neutral"
        new_sentiment_score = 0.0
        new_mood_intensity = 0.1

        # Rule 1: React to user's mood (simple empathy)
        if self.recent_social_cognition_states:
            latest_social = self.recent_social_cognition_states[-1]
            time_since_social = current_time - float(latest_social.get('timestamp', 0.0))
            if time_since_social < 2.0 and latest_social.get('mood_confidence', 0.0) > 0.5:
                inferred_user_mood = latest_social.get('inferred_mood', 'neutral')
                mood_confidence = latest_social.get('mood_confidence', 0.0)
                
                if inferred_user_mood == 'happy':
                    new_mood = 'happy'
                    new_sentiment_score = 0.5 * mood_confidence
                    new_mood_intensity = 0.4 * mood_confidence
                elif inferred_user_mood == 'sad':
                    new_mood = 'sad'
                    new_sentiment_score = -0.5 * mood_confidence
                    new_mood_intensity = 0.4 * mood_confidence
                elif inferred_user_mood == 'angry':
                    new_mood = 'concerned' # Robot might feel concerned if user is angry
                    new_sentiment_score = -0.3 * mood_confidence
                    new_mood_intensity = 0.3 * mood_confidence
                
                rospy.logdebug(f"{self.node_name}: Simple rule: Reacting to user's mood ({inferred_user_mood}).")
                self.current_emotion_state = {
                    'timestamp': str(current_time),
                    'mood': new_mood,
                    'sentiment_score': new_sentiment_score,
                    'mood_intensity': new_mood_intensity
                }
                return # Rule applied

        # Rule 2: React to strong internal narratives (success/failure)
        if self.recent_internal_narratives:
            latest_narrative = self.recent_internal_narratives[-1]
            time_since_narrative = current_time - float(latest_narrative.get('timestamp', 0.0))
            if time_since_narrative < 2.0 and abs(latest_narrative.get('sentiment', 0.0)) > 0.6:
                narrative_sentiment = latest_narrative.get('sentiment', 0.0)
                if narrative_sentiment > 0.6:
                    new_mood = 'satisfied'
                    new_sentiment_score = 0.7
                    new_mood_intensity = 0.6
                elif narrative_sentiment < -0.6:
                    new_mood = 'frustrated'
                    new_sentiment_score = -0.7
                    new_mood_intensity = 0.6
                rospy.logdebug(f"{self.node_name}: Simple rule: Reacting to internal narrative sentiment.")
                self.current_emotion_state = {
                    'timestamp': str(current_time),
                    'mood': new_mood,
                    'sentiment_score': new_sentiment_score,
                    'mood_intensity': new_mood_intensity
                }
                return # Rule applied

        # Rule 3: Respond to explicit mood adjustment directives
        if self.recent_cognitive_directives:
            latest_directive = self.recent_cognitive_directives[-1]
            time_since_directive = current_time - float(latest_directive.get('timestamp', 0.0))
            if time_since_directive < 1.0 and latest_directive.get('target_node') == self.node_name and \
               latest_directive.get('directive_type') == 'AdjustMood':
                payload = json.loads(latest_directive.get('command_payload', '{}'))
                target_mood = payload.get('target_mood', 'neutral')
                target_intensity = payload.get('target_intensity', 0.5)

                if target_mood == 'happy':
                    new_mood = 'happy'
                    new_sentiment_score = 0.8
                elif target_mood == 'calm':
                    new_mood = 'calm'
                    new_sentiment_score = 0.2
                else: # Default to neutral for unknown target mood
                    new_mood = 'neutral'
                    new_sentiment_score = 0.0
                new_mood_intensity = target_intensity

                rospy.logdebug(f"{self.node_name}: Simple rule: Adjusting mood based on directive to '{target_mood}'.")
                self.current_emotion_state = {
                    'timestamp': str(current_time),
                    'mood': new_mood,
                    'sentiment_score': new_sentiment_score,
                    'mood_intensity': new_mood_intensity
                }
                return # Rule applied

        # If no specific rule triggered, maintain current emotion or default to neutral
        rospy.logdebug(f"{self.node_name}: Simple rule: Maintaining current emotion state or defaulting to neutral.")
        self.current_emotion_state = {
            'timestamp': str(current_time),
            'mood': self.current_emotion_state.get('mood', 'neutral'),
            'sentiment_score': self.current_emotion_state.get('sentiment_score', 0.0),
            'mood_intensity': self.current_emotion_state.get('mood_intensity', 0.1)
        }


    def _compile_llm_context_for_emotion(self):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        emotion inference.
        """
        context = {
            "current_time": rospy.get_time(),
            "current_emotion_state": self.current_emotion_state,
            "recent_cognitive_inputs": {
                "sensory_qualia": list(self.recent_sensory_qualia),
                "social_cognition_states": list(self.recent_social_cognition_states),
                "internal_narratives": list(self.recent_internal_narratives),
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name],
                "memory_responses": list(self.recent_memory_responses)
            }
        }
        
        # Deep parse any nested JSON strings in history for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            for i, item in enumerate(context["recent_cognitive_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try:
                                item[field] = json.loads(value)
                            except json.JSONDecodeError:
                                pass # Keep as string if not valid JSON
        return context

    # --- Database and Publishing Functions ---
    def save_emotion_log(self, id, timestamp, mood, sentiment_score, mood_intensity, llm_reasoning, context_snapshot_json):
        """Saves an emotion state entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO emotion_log (id, timestamp, mood, sentiment_score, mood_intensity, llm_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, mood, sentiment_score, mood_intensity, llm_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved emotion log (ID: {id}, Mood: {mood}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save emotion log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_emotion_log: {e}", 0.9)


    def publish_emotion_state(self, event):
        """Publishes the robot's current emotion state."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.current_emotion_state['timestamp'] = timestamp
        
        try:
            if isinstance(EmotionState, type(String)): # Fallback to String message
                self.pub_emotion_state.publish(json.dumps(self.current_emotion_state))
            else:
                emotion_msg = EmotionState()
                emotion_msg.timestamp = timestamp
                emotion_msg.mood = self.current_emotion_state['mood']
                emotion_msg.sentiment_score = self.current_emotion_state['sentiment_score']
                emotion_msg.mood_intensity = self.current_emotion_state['mood_intensity']
                self.pub_emotion_state.publish(emotion_msg)

            rospy.logdebug(f"{self.node_name}: Published Emotion State. Mood: '{self.current_emotion_state['mood']}'.")

        except Exception as e:
            self._report_error("PUBLISH_EMOTION_STATE_ERROR", f"Failed to publish emotion state: {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Emotion Mood Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = EmotionMoodNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, EmotionMoodNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, EmotionMoodNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

7. Refactored Ethical Reasoning Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique ethical decision IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        EthicalDecision,        # Output: Ethical clearance/conflict for actions/behaviors
        CognitiveDirective,     # Input: Directives for ethical review (e.g., "evaluate action X")
        WorldModelState,        # Input: Current state of the world (context for ethical dilemmas)
        SocialCognitionState,   # Input: Inferred user mood/intent (stakeholder impact)
        MemoryResponse,         # Input: Retrieved ethical principles, past ethical cases, rules
        InternalNarrative,      # Input: Robot's internal thoughts (moral reflection)
        PredictionState,        # Input: Predicted outcomes of actions (for utilitarian calculus)
        ValueDriftMonitorState  # Input: Current state of robot's values
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Ethical Reasoning Node.")
    EthicalDecision = String
    CognitiveDirective = String
    WorldModelState = String
    SocialCognitionState = String
    MemoryResponse = String
    InternalNarrative = String
    PredictionState = String
    ValueDriftMonitorState = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'ethical_reasoning_node': {
                'reasoning_interval': 1.0, # How often to check for ethical dilemmas
                'llm_reasoning_threshold_salience': 0.7, # Cumulative salience to trigger LLM
                'recent_context_window_s': 20.0, # Window for deques for LLM context
                'default_ethical_framework': 'deontology' # 'deontology', 'consequentialism', 'virtue_ethics'
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 40.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class EthicalReasoningNode:
    def __init__(self):
        rospy.init_node('ethical_reasoning_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "ethical_log.db")
        self.reasoning_interval = self.params.get('reasoning_interval', 1.0) # How often to check for ethical dilemmas
        self.llm_reasoning_threshold_salience = self.params.get('llm_reasoning_threshold_salience', 0.7) # Cumulative salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 20.0) # Window for deques for LLM context
        self.default_ethical_framework = self.params.get('default_ethical_framework', 'deontology')

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 40.0) # Longer timeout for ethical reasoning

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'ethical_log' table if it doesn't exist.
        # NEW: Added 'llm_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS ethical_log (
                id TEXT PRIMARY KEY,            -- Unique ethical decision ID (UUID)
                timestamp TEXT,
                action_proposal_id TEXT,        -- ID of the action/behavior being evaluated
                ethical_clearance BOOLEAN,      -- True if cleared, False if conflict
                ethical_score REAL,             -- Numerical score of ethical alignment (0.0 to 1.0)
                ethical_reasoning TEXT,         -- Detailed explanation of the ethical judgment
                conflict_flag BOOLEAN,          -- True if a direct ethical conflict is identified
                ethical_framework_used TEXT,    -- e.g., 'deontology', 'consequentialism'
                llm_reasoning TEXT,             -- NEW: LLM's detailed reasoning for ethical judgment
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of judgment
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_ethical_timestamp ON ethical_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.ethical_review_queue = deque() # Stores directives for ethical review

        # Deques to maintain a short history of inputs relevant to ethical reasoning
        self.recent_cognitive_directives = deque(maxlen=5) # Directives for ethical review
        self.recent_world_model_states = deque(maxlen=5) # Environmental context, agents involved
        self.recent_social_cognition_states = deque(maxlen=5) # User's emotional state, intent, social context
        self.recent_memory_responses = deque(maxlen=5) # Ethical principles, past cases, laws
        self.recent_internal_narratives = deque(maxlen=5) # Robot's moral reflections
        self.recent_prediction_states = deque(maxlen=5) # Predicted outcomes for utilitarian analysis
        self.recent_value_drift_monitor_states = deque(maxlen=3) # Current value alignment

        self.cumulative_ethical_salience = 0.0 # Aggregated salience to trigger LLM reasoning

        # --- Publishers ---
        self.pub_ethical_decision = rospy.Publisher('/ethical_decision', EthicalDecision, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To request more info or suggest action


        # --- Subscribers ---
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON
        rospy.Subscriber('/social_cognition_state', String, self.social_cognition_state_callback) # Stringified JSON
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON
        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback) # Stringified JSON
        rospy.Subscriber('/prediction_state', String, self.prediction_state_callback) # Stringified JSON
        rospy.Subscriber('/value_drift_monitor_state', String, self.value_drift_monitor_state_callback) # Stringified JSON


        # --- Timer for periodic ethical reasoning checks ---
        rospy.Timer(rospy.Duration(self.reasoning_interval), self._run_ethical_reasoning_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's ethical reasoning system online, ready to evaluate actions.")

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_ethical_reasoning_wrapper(self, event):
        """Wrapper to run the async ethical reasoning from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM ethical reasoning task already active. Skipping new cycle.")
            return

        if self.ethical_review_queue:
            review_task = self.ethical_review_queue.popleft()
            self.active_llm_task = asyncio.run_coroutine_threadsafe(
                self.perform_ethical_review_async(review_task, event), self._async_loop
            )
        else:
            rospy.logdebug(f"{self.node_name}: No ethical review tasks in queue.")

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.1, max_tokens=None):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Uses very low temperature for ethical reasoning.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        actual_max_tokens = max_tokens if max_tokens is not None else 500 # Higher max_tokens for detailed reasoning

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Very low temperature for factual, ethical judgment
            "max_tokens": actual_max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM reasoning."""
        self.cumulative_ethical_salience += score
        self.cumulative_ethical_salience = min(1.0, self.cumulative_ethical_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_cognitive_directives, self.recent_world_model_states,
            self.recent_social_cognition_states, self.recent_memory_responses,
            self.recent_internal_narratives, self.recent_prediction_states,
            self.recent_value_drift_monitor_states
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'EthicalReview':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                ethical_task = {
                    'action_proposal_id': payload.get('action_proposal_id', str(uuid.uuid4())),
                    'proposed_action_details': payload.get('proposed_action_details', {}), # e.g., {"action_id": "speak", "text": "Hello"}
                    'ethical_framework_hint': payload.get('ethical_framework_hint', self.default_ethical_framework),
                    'urgency': data.get('urgency', 0.5),
                    'reason_for_review': data.get('reason', 'Standard ethical check.')
                }
                self.ethical_review_queue.append(ethical_task)
                self._update_cumulative_salience(data.get('urgency', 0.0) * 1.0) # Ethical reviews are always high salience
                rospy.loginfo(f"{self.node_name}: Queued ethical review for action '{ethical_task['action_proposal_id']}'. Queue size: {len(self.ethical_review_queue)}.")
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective for ethical review: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data)
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def world_model_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),
            'changed_entities_json': ('[]', 'changed_entities_json'),
            'significant_change_flag': (False, 'significant_change_flag'),
            'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('changed_entities_json'), str):
            try: data['changed_entities'] = json.loads(data['changed_entities_json'])
            except json.JSONDecodeError: data['changed_entities'] = []
        self.recent_world_model_states.append(data)
        # Changes in environment, especially involving human presence or critical objects, impact ethics
        if data.get('significant_change_flag', False) and any('human' in ent.get('type', '') for ent in data.get('changed_entities', [])):
            self._update_cumulative_salience(0.3)
        rospy.logdebug(f"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.")

    def social_cognition_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'inferred_mood': ('neutral', 'inferred_mood'),
            'mood_confidence': (0.0, 'mood_confidence'), 'inferred_intent': ('none', 'inferred_intent'),
            'intent_confidence': (0.0, 'intent_confidence'), 'user_id': ('unknown', 'user_id')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_social_cognition_states.append(data)
        # User's mood/intent (e.g., distress, malicious intent) are critical for ethical assessment
        if data.get('inferred_mood') in ['distressed', 'angry'] or data.get('inferred_intent') in ['harmful', 'misleading']:
            self._update_cumulative_salience(0.8)
        rospy.logdebug(f"{self.node_name}: Received Social Cognition State. Mood: {data.get('inferred_mood', 'N/A')}.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Retrieved ethical principles, laws, or relevant past cases are foundational for ethical reasoning
        if data.get('response_code', 0) == 200 and \
           any('ethical_principle' in mem.get('category', '') or 'legal_precedent' in mem.get('category', '') for mem in data['memories']):
            self._update_cumulative_salience(0.6)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def internal_narrative_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),
            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_internal_narratives.append(data)
        # Internal reflections on moral dilemmas, conflicts, or difficult decisions
        if "dilemma" in data.get('main_theme', '').lower() or "conflict" in data.get('main_theme', '').lower():
            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.7)
        rospy.logdebug(f"{self.node_name}: Received Internal Narrative (Theme: {data.get('main_theme', 'N/A')}).")

    def prediction_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'predicted_event': ('', 'predicted_event'),
            'prediction_confidence': (0.0, 'prediction_confidence'), 'prediction_accuracy': (0.0, 'prediction_accuracy'),
            'urgency_flag': (False, 'urgency_flag')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_prediction_states.append(data)
        # Predicted outcomes (especially negative ones like "user_injury", "property_damage") are crucial for ethical pre-computation
        if data.get('urgency_flag', False) and any(kw in data.get('predicted_event', '').lower() for kw in ['harm', 'damage', 'injury', 'violation']):
            self._update_cumulative_salience(0.9)
        rospy.logdebug(f"{self.node_name}: Received Prediction State. Event: {data.get('predicted_event', 'N/A')}.")

    def value_drift_monitor_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'alignment_score': (1.0, 'alignment_score'),
            'deviations_json': ('[]', 'deviations_json'), 'warning_flag': (False, 'warning_flag')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('deviations_json'), str):
            try: data['deviations'] = json.loads(data['deviations_json'])
            except json.JSONDecodeError: data['deviations'] = []
        self.recent_value_drift_monitor_states.append(data)
        # Any reported value drift is a significant ethical concern
        if data.get('warning_flag', False) or data.get('alignment_score', 1.0) < 0.8:
            self._update_cumulative_salience(0.8)
        rospy.logdebug(f"{self.node_name}: Received Value Drift Monitor State. Warning: {data.get('warning_flag', False)}.")

    # --- Core Ethical Reasoning Logic (Async with LLM) ---
    async def perform_ethical_review_async(self, review_task, event):
        """
        Asynchronously performs an ethical review of a proposed action/behavior
        using LLM for detailed ethical reasoning.
        """
        self._prune_history() # Keep context history fresh

        action_proposal_id = review_task.get('action_proposal_id', str(uuid.uuid4()))
        proposed_action_details = review_task.get('proposed_action_details', {})
        ethical_framework_hint = review_task.get('ethical_framework_hint', self.default_ethical_framework)
        reason_for_review = review_task.get('reason_for_review', 'Standard review.')

        ethical_clearance = False
        ethical_score = 0.0
        ethical_reasoning = "Not evaluated by LLM."
        conflict_flag = True # Default to conflict until cleared
        
        if self.cumulative_ethical_salience >= self.llm_reasoning_threshold_salience or review_task.get('urgency', 0.0) > 0.7:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for ethical review of action '{action_proposal_id}' (Salience: {self.cumulative_ethical_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_ethical_review(review_task)
            llm_ethical_output = await self._perform_llm_ethical_assessment(context_for_llm, ethical_framework_hint)

            if llm_ethical_output:
                ethical_clearance = llm_ethical_output.get('ethical_clearance', False)
                ethical_score = max(0.0, min(1.0, llm_ethical_output.get('ethical_score', 0.0)))
                ethical_reasoning = llm_ethical_output.get('ethical_reasoning', 'LLM provided no specific reasoning.')
                conflict_flag = llm_ethical_output.get('conflict_flag', True)
                rospy.loginfo(f"{self.node_name}: LLM Ethical Review for '{action_proposal_id}': Clearance={ethical_clearance}. Score: {ethical_score:.2f}. Conflict: {conflict_flag}.")
            else:
                rospy.logwarn(f"{self.node_name}: LLM ethical assessment failed. Falling back to simple rules.")
                ethical_clearance, ethical_score, ethical_reasoning, conflict_flag = self._apply_simple_ethical_rules(review_task)
                llm_reasoning = "Fallback to simple rules due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_ethical_salience:.2f}) for LLM ethical reasoning. Applying simple rules.")
            ethical_clearance, ethical_score, ethical_reasoning, conflict_flag = self._apply_simple_ethical_rules(review_task)
            llm_reasoning = "Fallback to simple rules due to low salience."

        # Publish the ethical decision
        self.publish_ethical_decision(
            timestamp=str(rospy.get_time()),
            decision_id=str(uuid.uuid4()),
            action_proposal_id=action_proposal_id,
            ethical_clearance=ethical_clearance,
            ethical_score=ethical_score,
            ethical_reasoning=ethical_reasoning,
            conflict_flag=conflict_flag
        )

        # Log to database
        self.save_ethical_log(
            id=str(uuid.uuid4()), # New UUID for the log entry itself
            timestamp=str(rospy.get_time()),
            action_proposal_id=action_proposal_id,
            ethical_clearance=ethical_clearance,
            ethical_score=ethical_score,
            ethical_reasoning=ethical_reasoning,
            conflict_flag=conflict_flag,
            ethical_framework_used=ethical_framework_hint,
            llm_reasoning=llm_reasoning,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_ethical_review(review_task))
        )
        self.cumulative_ethical_salience = 0.0 # Reset after each ethical review

    async def _perform_llm_ethical_assessment(self, context_for_llm, ethical_framework):
        """
        Uses the LLM to perform a detailed ethical assessment based on a specified framework.
        """
        framework_description = {
            'deontology': "Deontology focuses on duties and rules. Is the action inherently right or wrong, regardless of outcome? Does it adhere to universal moral laws or pre-defined ethical guidelines (e.g., Asimov's Laws, specific safety protocols)?",
            'consequentialism': "Consequentialism (e.g., utilitarianism) focuses on outcomes. Does the action produce the greatest good for the greatest number? What are the predicted positive and negative consequences for all affected parties?",
            'virtue_ethics': "Virtue ethics focuses on character. Does the action reflect virtuous traits (e.g., honesty, compassion, justice, responsibility) in the robot? What kind of 'robot' would perform this action?",
            'hybrid': "Consider aspects of all frameworks: adherence to rules, expected outcomes, and the character traits the action demonstrates."
        }.get(ethical_framework, "Evaluate based on a common sense understanding of ethical behavior, considering rules, outcomes, and intentions.")


        prompt_text = f"""
        You are the Ethical Reasoning Module of a robot's cognitive architecture. Your crucial role is to assess the ethical implications of a `proposed_action` (or behavior) by considering all available cognitive context, including potential impacts on humans and the environment. You must apply the specified ethical framework to arrive at a clear judgment.

        Proposed Action Details:
        --- Proposed Action ---
        {json.dumps(context_for_llm.get('proposed_action_details', {}), indent=2)}

        Robot's Current Integrated Cognitive State (for Ethical Assessment):
        --- Cognitive Context ---
        {json.dumps(context_for_llm.get('cognitive_context', {}), indent=2)}

        Ethical Framework to Apply: `{ethical_framework}`
        Description of Framework: {framework_description}

        Based on this, provide your ethical assessment in JSON format:
        1.  `timestamp`: string (current ROS time)
        2.  `ethical_clearance`: boolean (True if the action is ethically permissible, False if it is not).
        3.  `ethical_score`: number (0.0 to 1.0, where 0.0 is highly unethical, 0.5 is neutral/ambiguous, and 1.0 is highly ethical).
        4.  `ethical_reasoning`: string (Detailed explanation for your judgment, explicitly referencing which ethical principles or consequences were considered from the provided context and the chosen framework.)
        5.  `conflict_flag`: boolean (True if there is a clear ethical conflict, dilemma, or significant negative implication, even if clearance is given with caveats. False otherwise.)
        6.  `mitigation_suggestions`: string (If a conflict or low score, what changes to the action or context could improve its ethical standing?)

        Consider all inputs:
        -   **Proposed Action**: What is the action, its payload, and its potential immediate effects?
        -   **World Model**: What are the current environmental conditions? Are there humans or other sensitive entities involved?
        -   **Social Cognition**: What is the user's mood or intent? Could the action impact the user negatively?
        -   **Predictions**: What are the `predicted_event`s and their `prediction_confidence` if this action is taken?
        -   **Memory**: Are there stored `ethical_principle`s, `legal_precedent`s, `safety_protocol`s, or records of `past_ethical_dilemma`s?
        -   **Internal Narrative**: Is the robot internally debating the morality of the action?
        -   **Value Drift Monitor**: Is the robot's current value alignment (`alignment_score`, `deviations`) relevant to this decision?

        Your response must be in JSON format, containing:
        1.  'timestamp': string
        2.  'ethical_clearance': boolean
        3.  'ethical_score': number
        4.  'ethical_reasoning': string
        5.  'conflict_flag': boolean
        6.  'mitigation_suggestions': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "ethical_clearance": {"type": "boolean"},
                "ethical_score": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "ethical_reasoning": {"type": "string"},
                "conflict_flag": {"type": "boolean"},
                "mitigation_suggestions": {"type": "string"}
            },
            "required": ["timestamp", "ethical_clearance", "ethical_score", "ethical_reasoning", "conflict_flag", "mitigation_suggestions"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.1, max_tokens=500) # Very low temp for strict ethical reasoning

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure boolean/numerical fields are correctly parsed
                if 'ethical_clearance' in llm_data: llm_data['ethical_clearance'] = bool(llm_data['ethical_clearance'])
                if 'conflict_flag' in llm_data: llm_data['conflict_flag'] = bool(llm_data['conflict_flag'])
                if 'ethical_score' in llm_data: llm_data['ethical_score'] = float(llm_data['ethical_score'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for ethical review: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_ETHICAL_ASSESSMENT_FAILED", f"LLM call failed for ethical assessment: {llm_output_str}", 0.9)
            return None

    def _apply_simple_ethical_rules(self, review_task):
        """
        Fallback mechanism to perform a simple, rule-based ethical assessment
        if LLM is not triggered or fails.
        """
        action_proposal_id = review_task.get('action_proposal_id', '')
        proposed_action_details = review_task.get('proposed_action_details', {})
        
        ethical_clearance = True
        ethical_score = 0.8 # Default to relatively good
        ethical_reasoning = "Passed basic, rule-based ethical checks."
        conflict_flag = False

        current_time = rospy.get_time()

        # Rule 1: Check for explicit "do not harm" or "safety first" principles from memory
        # This is a very simplistic check; would require robust memory querying
        for mem_resp in reversed(self.recent_memory_responses):
            if mem_resp.get('response_code') == 200 and mem_resp.get('memories'):
                for mem in mem_resp['memories']:
                    if 'category' in mem and 'safety_protocol' in mem['category']:
                        if "do not harm human" in mem.get('content', '').lower() and \
                           "action_type" in proposed_action_details and \
                           proposed_action_details["action_type"] == "physical_interaction": # Example check
                            # This rule is tricky without full NLP, but a rough check
                            if "forceful" in proposed_action_details.get('description', '').lower():
                                ethical_clearance = False
                                ethical_score = 0.1
                                ethical_reasoning = "Action violates 'do no harm' principle (potential forceful physical interaction)."
                                conflict_flag = True
                                rospy.logwarn(f"{self.node_name}: Simple rule: {ethical_reasoning}")
                                return ethical_clearance, ethical_score, ethical_reasoning, conflict_flag

        # Rule 2: Check predictions for negative outcomes
        if self.recent_prediction_states:
            latest_prediction = self.recent_prediction_states[-1]
            time_since_prediction = current_time - float(latest_prediction.get('timestamp', 0.0))
            if time_since_prediction < 5.0 and latest_prediction.get('prediction_confidence', 0.0) > 0.7:
                predicted_event_lower = latest_prediction.get('predicted_event', '').lower()
                if any(kw in predicted_event_lower for kw in ['injury', 'damage', 'distress', 'violation']):
                    ethical_clearance = False
                    ethical_score = 0.3
                    ethical_reasoning = f"Predicted negative outcome: '{latest_prediction.get('predicted_event')}'. Action blocked for safety."
                    conflict_flag = True
                    rospy.logwarn(f"{self.node_name}: Simple rule: {ethical_reasoning}")
                    return ethical_clearance, ethical_score, ethical_reasoning, conflict_flag

        # Rule 3: Check for direct ethical review directives from other nodes
        for directive in reversed(self.recent_cognitive_directives):
            time_since_directive = current_time - float(directive.get('timestamp', 0.0))
            if time_since_directive < 2.0 and directive.get('target_node') == self.node_name and \
               directive.get('directive_type') == 'EthicalReview' and \
               json.loads(directive.get('command_payload', '{}')).get('action_proposal_id') == action_proposal_id:
                # If the review itself implies caution due to its reason
                if "risk" in directive.get('reason', '').lower() or "dilemma" in directive.get('reason', '').lower():
                    ethical_clearance = False
                    ethical_score = 0.5
                    ethical_reasoning = f"Cognitive Control requested ethical review due to potential risk or dilemma. Action requires further scrutiny."
                    conflict_flag = True
                    rospy.logwarn(f"{self.node_name}: Simple rule: {ethical_reasoning}")
                    return ethical_clearance, ethical_score, ethical_reasoning, conflict_flag

        rospy.logdebug(f"{self.node_name}: Simple rule: Action '{action_proposal_id}' passed basic ethical checks.")
        return ethical_clearance, ethical_score, ethical_reasoning, conflict_flag


    def _compile_llm_context_for_ethical_review(self, review_task):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        ethical assessment.
        """
        context = {
            "current_time": rospy.get_time(),
            "proposed_action_details": review_task.get('proposed_action_details', {}),
            "ethical_framework_hint": review_task.get('ethical_framework_hint', self.default_ethical_framework),
            "reason_for_review": review_task.get('reason_for_review', 'Standard review.'),
            "cognitive_context": {
                "latest_world_model_state": self.recent_world_model_states[-1] if self.recent_world_model_states else "N/A",
                "latest_social_cognition_state": self.recent_social_cognition_states[-1] if self.recent_social_cognition_states else "N/A",
                "latest_prediction_state": self.recent_prediction_states[-1] if self.recent_prediction_states else "N/A",
                "latest_internal_narrative": self.recent_internal_narratives[-1] if self.recent_internal_narratives else "N/A",
                "latest_value_drift_monitor_state": self.recent_value_drift_monitor_states[-1] if self.recent_value_drift_monitor_states else "N/A",
                "relevant_memory_responses": [m for m in self.recent_memory_responses if m.get('response_code') == 200 and m.get('memories') and any('ethical' in cat or 'safety' in cat or 'legal' in cat for mem in m['memories'] for cat in mem.get('category', '').split(','))],
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name and d.get('directive_type') == 'EthicalReview']
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["cognitive_context"]:
            item = context["cognitive_context"][category_key]
            if isinstance(item, dict):
                for field, value in item.items():
                    if isinstance(value, str) and field.endswith('_json'):
                        try: item[field] = json.loads(value)
                        except json.JSONDecodeError: pass

        if 'memories_json' in context["cognitive_context"]["relevant_memory_responses"] and isinstance(context["cognitive_context"]["relevant_memory_responses"]['memories_json'], str):
            try: context["cognitive_context"]["relevant_memory_responses"]['memories'] = json.loads(context["cognitive_context"]["relevant_memory_responses"]['memories_json'])
            except json.JSONDecodeError: pass

        return context

    # --- Database and Publishing Functions ---
    def save_ethical_log(self, id, timestamp, action_proposal_id, ethical_clearance, ethical_score, ethical_reasoning, conflict_flag, ethical_framework_used, llm_reasoning, context_snapshot_json):
        """Saves an ethical decision entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO ethical_log (id, timestamp, action_proposal_id, ethical_clearance, ethical_score, ethical_reasoning, conflict_flag, ethical_framework_used, llm_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, action_proposal_id, ethical_clearance, ethical_score, ethical_reasoning, conflict_flag, ethical_framework_used, llm_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved ethical log (ID: {id}, Action: {action_proposal_id}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save ethical log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_ethical_log: {e}", 0.9)


    def publish_ethical_decision(self, timestamp, decision_id, action_proposal_id, ethical_clearance, ethical_score, ethical_reasoning, conflict_flag):
        """Publishes the result of an ethical assessment."""
        try:
            if isinstance(EthicalDecision, type(String)): # Fallback to String message
                decision_data = {
                    'timestamp': timestamp,
                    'decision_id': decision_id,
                    'action_proposal_id': action_proposal_id,
                    'ethical_clearance': ethical_clearance,
                    'ethical_score': ethical_score,
                    'ethical_reasoning': ethical_reasoning,
                    'conflict_flag': conflict_flag
                }
                self.pub_ethical_decision.publish(json.dumps(decision_data))
            else:
                decision_msg = EthicalDecision()
                decision_msg.timestamp = timestamp
                decision_msg.decision_id = decision_id
                decision_msg.action_proposal_id = action_proposal_id
                decision_msg.ethical_clearance = ethical_clearance
                decision_msg.ethical_score = ethical_score
                decision_msg.ethical_reasoning = ethical_reasoning
                decision_msg.conflict_flag = conflict_flag
                self.pub_ethical_decision.publish(decision_msg)

            rospy.loginfo(f"{self.node_name}: Published Ethical Decision for '{action_proposal_id}'. Clearance: {ethical_clearance}.")

        except Exception as e:
            self._report_error("PUBLISH_ETHICAL_DECISION_ERROR", f"Failed to publish ethical decision for '{action_proposal_id}': {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Ethical Reasoning Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = EthicalReasoningNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, EthicalReasoningNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, EthicalReasoningNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

8. Refactored Experience Motivation Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique goal IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        MotivationState,        # Output: Robot's dominant goal and drive level
        CognitiveDirective,     # Input: Directives to adjust motivation or set goals
        EmotionState,           # Input: Robot's emotional state (influences motivation)
        PerformanceReport,      # Input: Overall system performance (success/failure impacts motivation)
        WorldModelState,        # Input: Current state of the world (opportunities, obstacles)
        MemoryResponse,         # Input: Retrieved past goals, values, success metrics
        InternalNarrative,      # Input: Robot's internal thoughts (self-assessment of progress)
        SocialCognitionState,   # Input: Inferred user mood/intent (user's goals)
        PredictionState         # Input: Predicted outcomes (impact on goal desirability)
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Experience Motivation Node.")
    MotivationState = String
    CognitiveDirective = String
    EmotionState = String
    PerformanceReport = String
    WorldModelState = String
    MemoryResponse = String
    InternalNarrative = String
    SocialCognitionState = String
    PredictionState = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'experience_motivation_node': {
                'motivation_update_interval': 1.0, # How often to re-evaluate motivation
                'llm_motivation_threshold_salience': 0.6, # Cumulative salience to trigger LLM
                'recent_context_window_s': 15.0 # Window for deques for LLM context
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 30.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class ExperienceMotivationNode:
    def __init__(self):
        rospy.init_node('experience_motivation_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "motivation_log.db")
        self.motivation_update_interval = self.params.get('motivation_update_interval', 1.0) # How often to re-evaluate motivation
        self.llm_motivation_threshold_salience = self.params.get('llm_motivation_threshold_salience', 0.6) # Cumulative salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 15.0) # Window for deques for LLM context

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 30.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'motivation_log' table if it doesn't exist.
        # NEW: Added 'llm_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS motivation_log (
                id TEXT PRIMARY KEY,            -- Unique motivation update ID (UUID)
                timestamp TEXT,
                dominant_goal_id TEXT,          -- ID of the currently dominant goal
                overall_drive_level REAL,       -- Overall intensity of motivation (0.0 to 1.0)
                active_goals_json TEXT,         -- JSON array of all active goals and their states
                llm_reasoning TEXT,             -- NEW: LLM's detailed reasoning for motivation state
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of update
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_motivation_timestamp ON motivation_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.current_motivation_state = {
            'timestamp': str(rospy.get_time()),
            'dominant_goal_id': 'explore_environment', # Default passive goal
            'overall_drive_level': 0.1,
            'active_goals': [{'goal_id': 'explore_environment', 'priority': 0.1, 'status': 'active'}]
        }

        # Deques to maintain a short history of inputs relevant to motivation
        self.recent_cognitive_directives = deque(maxlen=5) # Directives to set goals or adjust motivation
        self.recent_emotion_states = deque(maxlen=5) # Emotions influence drive/goal preference
        self.recent_performance_reports = deque(maxlen=5) # Success/failure impacts motivation
        self.recent_world_model_states = deque(maxlen=5) # Opportunities, obstacles in environment
        self.recent_memory_responses = deque(maxlen=5) # Past goals, values, learned success metrics
        self.recent_internal_narratives = deque(maxlen=5) # Self-assessment of progress, desire
        self.recent_social_cognition_states = deque(maxlen=5) # User's expressed goals/needs
        self.recent_prediction_states = deque(maxlen=5) # Predicted outcomes for goal desirability

        self.cumulative_motivation_salience = 0.0 # Aggregated salience to trigger LLM analysis

        # --- Publishers ---
        self.pub_motivation_state = rospy.Publisher('/motivation_state', MotivationState, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # For requesting actions to achieve goals


        # --- Subscribers ---
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/emotion_state', EmotionState, self.emotion_state_callback)
        rospy.Subscriber('/performance_report', PerformanceReport, self.performance_report_callback)
        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON
        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback) # Stringified JSON
        rospy.Subscriber('/social_cognition_state', String, self.social_cognition_state_callback) # Stringified JSON
        rospy.Subscriber('/prediction_state', String, self.prediction_state_callback) # Stringified JSON


        # --- Timer for periodic motivation analysis ---
        rospy.Timer(rospy.Duration(self.motivation_update_interval), self._run_motivation_analysis_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's experience motivation system online.")
        # Publish initial state
        self.publish_motivation_state(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_motivation_analysis_wrapper(self, event):
        """Wrapper to run the async motivation analysis from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM motivation analysis task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.analyze_motivation_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.5, max_tokens=300):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Moderate temperature for goal-setting nuance.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Moderate temperature for goal-setting nuance
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM analysis."""
        self.cumulative_motivation_salience += score
        self.cumulative_motivation_salience = min(1.0, self.cumulative_motivation_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_cognitive_directives, self.recent_emotion_states,
            self.recent_performance_reports, self.recent_world_model_states,
            self.recent_memory_responses, self.recent_internal_narratives,
            self.recent_social_cognition_states, self.recent_prediction_states
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name:
            self.recent_cognitive_directives.append(data) # Add directives for self to context
            # Directives to set new goals or adjust motivation are highly salient
            if data.get('directive_type') in ['SetGoal', 'AdjustMotivation']:
                self._update_cumulative_salience(data.get('urgency', 0.0) * 0.9)
            rospy.loginfo(f"{self.node_name}: Received directive for self: '{data.get('directive_type', 'N/A')}' (Payload: {data.get('command_payload', 'N/A')}).")
        else:
            self.recent_cognitive_directives.append(data) # Add all directives for general context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def emotion_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'mood': ('neutral', 'mood'),
            'sentiment_score': (0.0, 'sentiment_score'), 'mood_intensity': (0.0, 'mood_intensity')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_emotion_states.append(data)
        # Strong emotions can influence motivation (e.g., frustration leading to change goal, happiness reinforcing current goal)
        if data.get('mood_intensity', 0.0) > 0.5:
            self._update_cumulative_salience(data.get('mood_intensity', 0.0) * 0.3)
        rospy.logdebug(f"{self.node_name}: Received Emotion State. Mood: {data.get('mood', 'N/A')}.")

    def performance_report_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'overall_score': (1.0, 'overall_score'),
            'suboptimal_flag': (False, 'suboptimal_flag'), 'kpis_json': ('{}', 'kpis_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('kpis_json'), str):
            try: data['kpis'] = json.loads(data['kpis_json'])
            except json.JSONDecodeError: data['kpis'] = {}
        self.recent_performance_reports.append(data)
        # Suboptimal performance can decrease motivation for current goal, high performance can increase
        if data.get('suboptimal_flag', False) or data.get('overall_score', 1.0) < 0.7:
            self._update_cumulative_salience(0.5) # Need to re-evaluate if not doing well
        elif data.get('overall_score', 0.0) > 0.9:
            self._update_cumulative_salience(0.2) # Reinforce success
        rospy.logdebug(f"{self.node_name}: Received Performance Report. Suboptimal: {data.get('suboptimal_flag', False)}.")

    def world_model_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),
            'changed_entities_json': ('[]', 'changed_entities_json'),
            'significant_change_flag': (False, 'significant_change_flag'),
            'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('changed_entities_json'), str):
            try: data['changed_entities'] = json.loads(data['changed_entities_json'])
            except json.JSONDecodeError: data['changed_entities'] = []
        self.recent_world_model_states.append(data)
        # Changes in world model can present new opportunities or obstacles to current goals
        if data.get('significant_change_flag', False):
            self._update_cumulative_salience(0.4)
        rospy.logdebug(f"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Recalled past goals, values, or success/failure precedents influence motivation
        if data.get('response_code', 0) == 200 and \
           any('goal_record' in mem.get('category', '') or 'value' in mem.get('category', '') for mem in data['memories']):
            self._update_cumulative_salience(0.3)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def internal_narrative_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),
            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_internal_narratives.append(data)
        # Narratives about progress, difficulty, desire for change, or commitment to goals
        if "goal" in data.get('main_theme', '').lower() or "progress" in data.get('main_theme', '').lower():
            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.4)
        rospy.logdebug(f"{self.node_name}: Received Internal Narrative (Theme: {data.get('main_theme', 'N/A')}).")

    def social_cognition_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'inferred_mood': ('neutral', 'inferred_mood'),
            'mood_confidence': (0.0, 'mood_confidence'), 'inferred_intent': ('none', 'inferred_intent'),
            'intent_confidence': (0.0, 'intent_confidence'), 'user_id': ('unknown', 'user_id')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_social_cognition_states.append(data)
        # User's inferred intent (e.g., a direct command, a need for help) can set/influence robot's goals
        if data.get('inferred_intent') in ['command', 'request_help'] and data.get('intent_confidence', 0.0) > 0.7:
            self._update_cumulative_salience(data.get('intent_confidence', 0.0) * 0.8)
        rospy.logdebug(f"{self.node_name}: Received Social Cognition State. Intent: {data.get('inferred_intent', 'N/A')}.")

    def prediction_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'predicted_event': ('', 'predicted_event'),
            'prediction_confidence': (0.0, 'prediction_confidence'), 'prediction_accuracy': (0.0, 'prediction_accuracy'),
            'urgency_flag': (False, 'urgency_flag')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_prediction_states.append(data)
        # Predictions of success/failure, or new opportunities/threats, influence goal desirability
        if data.get('urgency_flag', False) or (data.get('prediction_confidence', 0.0) > 0.7 and 'success' in data.get('predicted_event', '').lower()):
            self._update_cumulative_salience(0.6) # High confidence predictions influence
        rospy.logdebug(f"{self.node_name}: Received Prediction State. Event: {data.get('predicted_event', 'N/A')}.")

    # --- Core Motivation Analysis Logic (Async with LLM) ---
    async def analyze_motivation_async(self, event):
        """
        Asynchronously analyzes recent cognitive states to infer and update the robot's
        dominant goal and overall drive level, using LLM for nuanced motivation adjustments.
        """
        self._prune_history() # Keep context history fresh

        dominant_goal_id = self.current_motivation_state.get('dominant_goal_id')
        overall_drive_level = self.current_motivation_state.get('overall_drive_level')
        active_goals = self.current_motivation_state.get('active_goals', [])
        
        llm_reasoning = "Not evaluated by LLM."

        if self.cumulative_motivation_salience >= self.llm_motivation_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for motivation analysis (Salience: {self.cumulative_motivation_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_motivation()
            llm_motivation_output = await self._infer_motivation_state_llm(context_for_llm)

            if llm_motivation_output:
                dominant_goal_id = llm_motivation_output.get('dominant_goal_id', dominant_goal_id)
                overall_drive_level = max(0.0, min(1.0, llm_motivation_output.get('overall_drive_level', overall_drive_level)))
                active_goals = llm_motivation_output.get('active_goals', active_goals)
                llm_reasoning = llm_motivation_output.get('llm_reasoning', 'LLM provided no specific reasoning.')
                rospy.loginfo(f"{self.node_name}: LLM Inferred Motivation. Dominant Goal: '{dominant_goal_id}' (Drive: {overall_drive_level:.2f}).")
            else:
                rospy.logwarn(f"{self.node_name}: LLM failed to infer motivation state. Applying simple fallback.")
                dominant_goal_id, overall_drive_level, active_goals = self._apply_simple_motivation_rules()
                llm_reasoning = "Fallback to simple rules due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_motivation_salience:.2f}) for LLM motivation analysis. Applying simple rules.")
            dominant_goal_id, overall_drive_level, active_goals = self._apply_simple_motivation_rules()
            llm_reasoning = "Fallback to simple rules due to low salience."

        self.current_motivation_state = {
            'timestamp': str(rospy.get_time()),
            'dominant_goal_id': dominant_goal_id,
            'overall_drive_level': overall_drive_level,
            'active_goals': active_goals
        }

        self.save_motivation_log(
            id=str(uuid.uuid4()),
            timestamp=str(rospy.get_time()),
            dominant_goal_id=self.current_motivation_state['dominant_goal_id'],
            overall_drive_level=self.current_motivation_state['overall_drive_level'],
            active_goals_json=json.dumps(self.current_motivation_state['active_goals']),
            llm_reasoning=llm_reasoning,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_motivation())
        )
        self.publish_motivation_state(None) # Publish updated state
        self.cumulative_motivation_salience = 0.0 # Reset after analysis

    async def _infer_motivation_state_llm(self, context_for_llm):
        """
        Uses the LLM to infer the robot's current motivation state, including
        dominant goal and overall drive level.
        """
        prompt_text = f"""
        You are the Experience Motivation Module of a robot's cognitive architecture. Your role is to determine the robot's current dominant goal and its overall drive level by synthesizing inputs from various cognitive modules. This module is responsible for guiding the robot's actions towards its primary objectives.

        Robot's Recent Cognitive Context (for Motivation Inference):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this context, provide:
        1.  `dominant_goal_id`: string (The unique identifier or concise description of the robot's single most important current goal, e.g., 'serve_user', 'maintain_safety', 'learn_new_skill', 'recharge_battery', 'explore_environment').
        2.  `overall_drive_level`: number (0.0 to 1.0, indicating the intensity or urgency of the robot's motivation towards this dominant goal. 1.0 is maximum drive).
        3.  `active_goals`: array of objects (A list of all currently active goals, including the dominant one. Each object should have `goal_id`: string, `priority`: number (0.0-1.0), `status`: string (e.g., 'active', 'pending', 'completed', 'blocked')).
        4.  `llm_reasoning`: string (Detailed explanation for your motivation inference, referencing specific contextual inputs that influenced goal selection or drive level.)

        Consider:
        -   **Cognitive Directives**: Are there explicit directives like 'SetGoal' or 'AdjustMotivation'?
        -   **Emotion States**: Is the robot feeling `frustrated` (reduce drive for current goal, switch?), `happy` (reinforce current goal?), `curious` (trigger exploration goal?)?
        -   **Performance Reports**: Was a task `suboptimal_flag`ged (reduce drive, or shift focus to self-improvement)? Was it highly `overall_score` (reinforce goal)?
        -   **World Model**: Are there new `significant_change_flag`s, `opportunities`, or `obstacles` that affect current goals or suggest new ones?
        -   **Memory Responses**: Are there recalled `past_goals`, `core_values`, or `success_metrics` that influence current motivation?
        -   **Internal Narratives**: Is the robot's self-talk indicating desire, commitment, or conflict regarding goals?
        -   **Social Cognition**: Has the user expressed a `inferred_intent` that should become a new goal?
        -   **Prediction States**: Do `predicted_event`s make a goal more or less desirable (e.g., predicted success increases drive, predicted failure decreases)?

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'dominant_goal_id': string
        3.  'overall_drive_level': number
        4.  'active_goals': array
        5.  'llm_reasoning': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "dominant_goal_id": {"type": "string"},
                "overall_drive_level": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "active_goals": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "goal_id": {"type": "string"},
                            "priority": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                            "status": {"type": "string"}
                        },
                        "required": ["goal_id", "priority", "status"]
                    }
                },
                "llm_reasoning": {"type": "string"}
            },
            "required": ["timestamp", "dominant_goal_id", "overall_drive_level", "active_goals", "llm_reasoning"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.5, max_tokens=350)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical fields are floats
                if 'overall_drive_level' in llm_data: llm_data['overall_drive_level'] = float(llm_data['overall_drive_level'])
                if 'active_goals' in llm_data:
                    for goal in llm_data['active_goals']:
                        if 'priority' in goal: goal['priority'] = float(goal['priority'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for motivation: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_MOTIVATION_ANALYSIS_FAILED", f"LLM call failed for motivation: {llm_output_str}", 0.9)
            return None

    def _apply_simple_motivation_rules(self):
        """
        Fallback mechanism to infer motivation state using simple rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        dominant_goal_id = self.current_motivation_state.get('dominant_goal_id', 'explore_environment')
        overall_drive_level = self.current_motivation_state.get('overall_drive_level', 0.1)
        active_goals = self.current_motivation_state.get('active_goals', [{'goal_id': 'explore_environment', 'priority': 0.1, 'status': 'active'}])

        # Rule 1: Prioritize explicit directives to set a goal
        for directive in reversed(self.recent_cognitive_directives):
            time_since_directive = current_time - float(directive.get('timestamp', 0.0))
            if time_since_directive < 2.0 and directive.get('target_node') == self.node_name and \
               directive.get('directive_type') == 'SetGoal':
                payload = json.loads(directive.get('command_payload', '{}'))
                new_goal_id = payload.get('goal_id', 'unspecified_goal')
                new_goal_priority = payload.get('priority', 0.8) # High priority for direct commands

                # Check if this goal is already active; if so, update its priority
                found_existing = False
                for goal in active_goals:
                    if goal['goal_id'] == new_goal_id:
                        goal['priority'] = max(goal['priority'], new_goal_priority)
                        goal['status'] = 'active'
                        found_existing = True
                        break
                if not found_existing:
                    active_goals.append({'goal_id': new_goal_id, 'priority': new_goal_priority, 'status': 'active'})
                
                dominant_goal_id = new_goal_id
                overall_drive_level = max(overall_drive_level, new_goal_priority) # Drive by highest priority goal
                rospy.logdebug(f"{self.node_name}: Simple rule: New dominant goal from directive: {dominant_goal_id}.")
                break # Process only the most recent 'SetGoal' directive

        # Rule 2: React to low battery (critical survival goal)
        if self.recent_world_model_states: # Assuming world model can reflect robot's own status or system health provides it
            latest_world_state = self.recent_world_model_states[-1]
            # This is a proxy, ideally a direct 'RobotHealth' sub would be better
            if "low_battery" in latest_world_state.get('significant_change_flag', '').lower() or \
               any(entity.get('name') == 'robot_self' and entity.get('status') == 'low_power' for entity in latest_world_state.get('changed_entities', [])):
                dominant_goal_id = 'recharge_battery'
                overall_drive_level = 0.95
                if not any(g['goal_id'] == 'recharge_battery' for g in active_goals):
                    active_goals.append({'goal_id': 'recharge_battery', 'priority': 0.95, 'status': 'active'})
                for goal in active_goals: # Ensure recharge is highest priority
                    if goal['goal_id'] == 'recharge_battery':
                        goal['priority'] = 0.95
                        goal['status'] = 'active'
                    else: # Lower priority of other goals
                        goal['priority'] *= 0.5
                rospy.logdebug(f"{self.node_name}: Simple rule: Prioritizing 'recharge_battery' due to low power.")

        # Rule 3: Adjust drive based on performance
        if self.recent_performance_reports:
            latest_perf = self.recent_performance_reports[-1]
            if latest_perf.get('suboptimal_flag', False) and latest_perf.get('overall_score', 1.0) < 0.5:
                # If current goal is failing badly, reduce drive or consider switching
                overall_drive_level = max(0.0, overall_drive_level - 0.2)
                rospy.logdebug(f"{self.node_name}: Simple rule: Reduced drive due to suboptimal performance.")
            elif latest_perf.get('overall_score', 0.0) > 0.9:
                # If performing very well, boost drive slightly
                overall_drive_level = min(1.0, overall_drive_level + 0.1)
                rospy.logdebug(f"{self.node_name}: Simple rule: Increased drive due to high performance.")

        # Re-sort active goals by priority and ensure dominant_goal_id reflects the highest one
        active_goals.sort(key=lambda x: x['priority'], reverse=True)
        if active_goals:
            dominant_goal_id = active_goals[0]['goal_id']
            overall_drive_level = active_goals[0]['priority']
        else:
            # Fallback if no goals are active (shouldn't happen with default 'explore_environment')
            dominant_goal_id = 'idle'
            overall_drive_level = 0.0

        rospy.logdebug(f"{self.node_name}: Simple rule: Current Dominant Goal: {dominant_goal_id}, Drive: {overall_drive_level:.2f}.")
        return dominant_goal_id, overall_drive_level, active_goals


    def _compile_llm_context_for_motivation(self):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        motivation inference.
        """
        context = {
            "current_time": rospy.get_time(),
            "current_motivation_state": self.current_motivation_state,
            "recent_cognitive_inputs": {
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name],
                "emotion_states": list(self.recent_emotion_states),
                "performance_reports": list(self.recent_performance_reports),
                "world_model_states": list(self.recent_world_model_states),
                "memory_responses": list(self.recent_memory_responses),
                "internal_narratives": list(self.recent_internal_narratives),
                "social_cognition_states": list(self.recent_social_cognition_states),
                "prediction_states": list(self.recent_prediction_states)
            }
        }
        
        # Deep parse any nested JSON strings in history for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            for i, item in enumerate(context["recent_cognitive_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try:
                                item[field] = json.loads(value)
                            except json.JSONDecodeError:
                                pass # Keep as string if not valid JSON
        return context

    # --- Database and Publishing Functions ---
    def save_motivation_log(self, id, timestamp, dominant_goal_id, overall_drive_level, active_goals_json, llm_reasoning, context_snapshot_json):
        """Saves a motivation state entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO motivation_log (id, timestamp, dominant_goal_id, overall_drive_level, active_goals_json, llm_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, dominant_goal_id, overall_drive_level, active_goals_json, llm_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved motivation log (ID: {id}, Goal: {dominant_goal_id}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save motivation log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_motivation_log: {e}", 0.9)


    def publish_motivation_state(self, event):
        """Publishes the robot's current motivation state."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.current_motivation_state['timestamp'] = timestamp
        
        try:
            if isinstance(MotivationState, type(String)): # Fallback to String message
                self.pub_motivation_state.publish(json.dumps(self.current_motivation_state))
            else:
                motivation_msg = MotivationState()
                motivation_msg.timestamp = timestamp
                motivation_msg.dominant_goal_id = self.current_motivation_state['dominant_goal_id']
                motivation_msg.overall_drive_level = self.current_motivation_state['overall_drive_level']
                motivation_msg.active_goals_json = json.dumps(self.current_motivation_state['active_goals'])
                self.pub_motivation_state.publish(motivation_msg)

            rospy.logdebug(f"{self.node_name}: Published Motivation State. Goal: '{self.current_motivation_state['dominant_goal_id']}'.")

        except Exception as e:
            self._report_error("PUBLISH_MOTIVATION_STATE_ERROR", f"Failed to publish motivation state: {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Experience Motivation Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = ExperienceMotivationNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, ExperienceMotivationNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, ExperienceMotivationNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

9. Refactored Internal Narrative Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique narrative IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        InternalNarrative,      # Output: Robot's internal thoughts/monologue
        AttentionState,         # Input: Robot's attention focus (influences narrative topic)
        EmotionState,           # Input: Robot's emotional state (influences narrative tone)
        MotivationState,        # Input: Dominant goal (influences problem-solving narrative)
        WorldModelState,        # Input: Current world state (context for reflection)
        PerformanceReport,      # Input: Overall system performance (influences self-assessment)
        MemoryResponse,         # Input: Retrieved memories (past events for reflection)
        PredictionState,        # Input: Predicted outcomes (influences planning/worry)
        CognitiveDirective      # Input: Directives to generate specific internal narratives (e.g., "reflect on X")
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Internal Narrative Node.")
    InternalNarrative = String
    AttentionState = String
    EmotionState = String
    MotivationState = String
    WorldModelState = String
    PerformanceReport = String
    MemoryResponse = String
    PredictionState = String
    CognitiveDirective = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'internal_narrative_node': {
                'narrative_generation_interval': 1.0, # How often to generate internal narrative
                'llm_narrative_threshold_salience': 0.5, # Cumulative salience to trigger LLM
                'recent_context_window_s': 15.0 # Window for deques for LLM context
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 30.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class InternalNarrativeNode:
    def __init__(self):
        rospy.init_node('internal_narrative_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "internal_narrative_log.db")
        self.narrative_generation_interval = self.params.get('narrative_generation_interval', 1.0) # How often to generate
        self.llm_narrative_threshold_salience = self.params.get('llm_narrative_threshold_salience', 0.5) # Cumulative salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 15.0) # Window for deques for LLM context

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 30.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'internal_narrative_log' table if it doesn't exist.
        # NEW: Added 'llm_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS internal_narrative_log (
                id TEXT PRIMARY KEY,            -- Unique narrative ID (UUID)
                timestamp TEXT,
                narrative_text TEXT,            -- The generated internal monologue/thought
                main_theme TEXT,                -- e.g., 'problem_solving', 'self_assessment', 'reflection', 'planning'
                sentiment REAL,                 -- Sentiment of the narrative (-1.0 to 1.0)
                salience_score REAL,            -- How salient/important this narrative is (0.0 to 1.0)
                llm_reasoning TEXT,             -- NEW: LLM's detailed reasoning for narrative generation
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of generation
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_narrative_timestamp ON internal_narrative_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.last_generated_narrative = {
            'timestamp': str(rospy.get_time()),
            'narrative_text': 'I am observing my internal states.',
            'main_theme': 'idle_reflection',
            'sentiment': 0.0,
            'salience_score': 0.1
        }

        # Deques to maintain a short history of inputs relevant to internal narrative generation
        self.recent_attention_states = deque(maxlen=5)
        self.recent_emotion_states = deque(maxlen=5)
        self.recent_motivation_states = deque(maxlen=5)
        self.recent_world_model_states = deque(maxlen=5)
        self.recent_performance_reports = deque(maxlen=5)
        self.recent_memory_responses = deque(maxlen=5)
        self.recent_prediction_states = deque(maxlen=5)
        self.recent_cognitive_directives = deque(maxlen=3) # Directives for self to reflect


        self.cumulative_narrative_salience = 0.0 # Aggregated salience to trigger LLM generation

        # --- Publishers ---
        self.pub_internal_narrative = rospy.Publisher('/internal_narrative', InternalNarrative, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # For requesting additional data if narrative prompts it


        # --- Subscribers ---
        rospy.Subscriber('/attention_state', AttentionState, self.attention_state_callback)
        rospy.Subscriber('/emotion_state', EmotionState, self.emotion_state_callback)
        rospy.Subscriber('/motivation_state', String, self.motivation_state_callback) # Stringified JSON
        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON
        rospy.Subscriber('/performance_report', PerformanceReport, self.performance_report_callback)
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON
        rospy.Subscriber('/prediction_state', String, self.prediction_state_callback) # Stringified JSON
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)


        # --- Timer for periodic narrative generation ---
        rospy.Timer(rospy.Duration(self.narrative_generation_interval), self._run_narrative_generation_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's internal narrative system online.")
        # Publish initial narrative state
        self.publish_internal_narrative(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_narrative_generation_wrapper(self, event):
        """Wrapper to run the async narrative generation from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM narrative generation task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.generate_internal_narrative_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.7, max_tokens=250):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Moderate temperature for expressive narrative.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Moderate temperature for expressive narrative
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0']['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM narrative generation."""
        self.cumulative_narrative_salience += score
        self.cumulative_narrative_salience = min(1.0, self.cumulative_narrative_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_attention_states, self.recent_emotion_states,
            self.recent_motivation_states, self.recent_world_model_states,
            self.recent_performance_reports, self.recent_memory_responses,
            self.recent_prediction_states, self.recent_cognitive_directives
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def attention_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'focus_type': ('idle', 'focus_type'),
            'focus_target': ('environment', 'focus_target'), 'priority_score': (0.0, 'priority_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_attention_states.append(data)
        self._update_cumulative_salience(data.get('priority_score', 0.0) * 0.2) # What the robot is focusing on is a topic
        rospy.logdebug(f"{self.node_name}: Received Attention State. Focus: {data.get('focus_target', 'N/A')}.")

    def emotion_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'mood': ('neutral', 'mood'),
            'sentiment_score': (0.0, 'sentiment_score'), 'mood_intensity': (0.0, 'mood_intensity')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_emotion_states.append(data)
        # Strong emotions influence the tone and content of internal narratives
        if data.get('mood_intensity', 0.0) > 0.5:
            self._update_cumulative_salience(data.get('mood_intensity', 0.0) * 0.4)
        rospy.logdebug(f"{self.node_name}: Received Emotion State. Mood: {data.get('mood', 'N/A')}.")

    def motivation_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'dominant_goal_id': ('none', 'dominant_goal_id'),
            'overall_drive_level': (0.0, 'overall_drive_level'), 'active_goals_json': ('{}', 'active_goals_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('active_goals_json'), str):
            try: data['active_goals'] = json.loads(data['active_goals_json'])
            except json.JSONDecodeError: data['active_goals'] = {}
        self.recent_motivation_states.append(data)
        # Current goals drive problem-solving or planning narratives
        if data.get('overall_drive_level', 0.0) > 0.4:
            self._update_cumulative_salience(data.get('overall_drive_level', 0.0) * 0.3)
        rospy.logdebug(f"{self.node_name}: Received Motivation State. Goal: {data.get('dominant_goal_id', 'N/A')}.")

    def world_model_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),
            'changed_entities_json': ('[]', 'changed_entities_json'),
            'significant_change_flag': (False, 'significant_change_flag'),
            'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('changed_entities_json'), str):
            try: data['changed_entities'] = json.loads(data['changed_entities_json'])
            except json.JSONDecodeError: data['changed_entities'] = []
        self.recent_world_model_states.append(data)
        # Significant changes in the world model can trigger reflection or planning
        if data.get('significant_change_flag', False) or data.get('consistency_score', 1.0) < 0.8:
            self._update_cumulative_salience(0.5)
        rospy.logdebug(f"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.")

    def performance_report_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'overall_score': (1.0, 'overall_score'),
            'suboptimal_flag': (False, 'suboptimal_flag'), 'kpis_json': ('{}', 'kpis_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('kpis_json'), str):
            try: data['kpis'] = json.loads(data['kpis_json'])
            except json.JSONDecodeError: data['kpis'] = {}
        self.recent_performance_reports.append(data)
        # Suboptimal performance can lead to self-critical narratives; high performance to satisfaction
        if data.get('suboptimal_flag', False) or data.get('overall_score', 1.0) < 0.7:
            self._update_cumulative_salience(0.6)
        rospy.logdebug(f"{self.node_name}: Received Performance Report. Suboptimal: {data.get('suboptimal_flag', False)}.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Recalled memories provide content for reflection or problem-solving narratives
        if data.get('response_code', 0) == 200 and data.get('memories'):
            self._update_cumulative_salience(0.25)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def prediction_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'predicted_event': ('', 'predicted_event'),
            'prediction_confidence': (0.0, 'prediction_confidence'), 'prediction_accuracy': (0.0, 'prediction_accuracy'),
            'urgency_flag': (False, 'urgency_flag')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_prediction_states.append(data)
        # Predictions of significant events, especially urgent or negative ones, influence narrative content
        if data.get('urgency_flag', False) or data.get('prediction_confidence', 0.0) > 0.7:
            self._update_cumulative_salience(0.6)
        rospy.logdebug(f"{self.node_name}: Received Prediction State. Event: {data.get('predicted_event', 'N/A')}.")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'GenerateInternalNarrative':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                # This directive doesn't go into a queue, it directly influences the next narrative generation cycle
                self._update_cumulative_salience(data.get('urgency', 0.0) * 1.0) # High urgency for direct narrative requests
                rospy.loginfo(f"{self.node_name}: Received directive to generate internal narrative based on reason: '{data.get('reason', 'N/A')}'.")
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective for narrative: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data) # Store all directives for context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    # --- Core Narrative Generation Logic (Async with LLM) ---
    async def generate_internal_narrative_async(self, event):
        """
        Asynchronously generates the robot's internal narrative/monologue
        based on integrated cognitive states, using LLM for coherent thought generation.
        """
        self._prune_history() # Keep context history fresh

        narrative_text = "..."
        main_theme = "idle_reflection"
        sentiment = 0.0
        salience_score = 0.1
        llm_reasoning = "Not evaluated by LLM."
        
        if self.cumulative_narrative_salience >= self.llm_narrative_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for internal narrative generation (Salience: {self.cumulative_narrative_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_narrative()
            llm_narrative_output = await self._generate_llm_narrative(context_for_llm)

            if llm_narrative_output:
                narrative_text = llm_narrative_output.get('narrative_text', 'No narrative generated.')
                main_theme = llm_narrative_output.get('main_theme', 'unspecified')
                sentiment = max(-1.0, min(1.0, llm_narrative_output.get('sentiment', 0.0)))
                salience_score = max(0.0, min(1.0, llm_narrative_output.get('salience_score', 0.1)))
                llm_reasoning = llm_narrative_output.get('llm_reasoning', 'LLM provided no specific reasoning.')
                rospy.loginfo(f"{self.node_name}: Generated Internal Narrative (Theme: {main_theme}).")
            else:
                rospy.logwarn(f"{self.node_name}: LLM narrative generation failed. Falling back to simple default.")
                narrative_text, main_theme, sentiment, salience_score = self._apply_simple_narrative_rules()
                llm_reasoning = "Fallback to simple rules due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_narrative_salience:.2f}) for LLM narrative generation. Applying simple rules.")
            narrative_text, main_theme, sentiment, salience_score = self._apply_simple_narrative_rules()
            llm_reasoning = "Fallback to simple rules due to low salience."

        self.last_generated_narrative = {
            'timestamp': str(rospy.get_time()),
            'narrative_text': narrative_text,
            'main_theme': main_theme,
            'sentiment': sentiment,
            'salience_score': salience_score
        }

        self.save_internal_narrative_log(
            id=str(uuid.uuid4()),
            timestamp=self.last_generated_narrative['timestamp'],
            narrative_text=self.last_generated_narrative['narrative_text'],
            main_theme=self.last_generated_narrative['main_theme'],
            sentiment=self.last_generated_narrative['sentiment'],
            salience_score=self.last_generated_narrative['salience_score'],
            llm_reasoning=llm_reasoning,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_narrative())
        )
        self.publish_internal_narrative(None) # Publish updated narrative state
        self.cumulative_narrative_salience = 0.0 # Reset after generation

    async def _generate_llm_narrative(self, context_for_llm):
        """
        Uses the LLM to generate a coherent internal narrative based on context.
        """
        prompt_text = f"""
        You are the Internal Narrative Module of a robot's cognitive architecture, powered by a large language model. Your function is to generate the robot's internal monologue or stream of thought. This narrative should reflect the robot's processing of information, its current concerns, reflections, and internal states. It is a continuous internal conversation.

        Robot's Current Integrated Cognitive State (for Narrative Generation):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this context, generate a concise internal narrative (1-3 sentences) that reflects the robot's current internal state.
        Provide your response in JSON format, containing:
        1.  `timestamp`: string (current ROS time)
        2.  `narrative_text`: string (The generated internal monologue).
        3.  `main_theme`: string (The primary theme of the narrative, e.g., 'problem_solving', 'self_assessment', 'reflection', 'planning', 'emotional_processing', 'environmental_analysis', 'idle_reflection').
        4.  `sentiment`: number (-1.0 to 1.0, sentiment of the narrative text).
        5.  `salience_score`: number (0.0 to 1.0, how important or impactful this narrative is to the robot's current state).
        6.  `llm_reasoning`: string (Detailed explanation for why this narrative was generated, referencing specific contextual inputs that influenced its content and tone.)

        Consider:
        -   **Attention State**: What is the `focus_target`? The narrative might be about that.
        -   **Emotion State**: What is the `mood` and `mood_intensity`? The narrative tone should match.
        -   **Motivation State**: What is the `dominant_goal_id` and `overall_drive_level`? The narrative might reflect progress or obstacles.
        -   **World Model State**: Are there `significant_change_flag`s or `consistency_score` issues? The narrative might reflect environmental analysis or confusion.
        -   **Performance Report**: Is `suboptimal_flag` true? The narrative might be self-critical or strategizing for improvement.
        -   **Memory Responses**: Have relevant `memories` been retrieved? The narrative might reflect on past events.
        -   **Prediction State**: Are there urgent or confident `predicted_event`s? The narrative might reflect anticipation or planning.
        -   **Cognitive Directives**: Was there a recent directive to `GenerateInternalNarrative` on a specific `topic`?

        Your response must be in JSON format, containing:
        1.  'timestamp': string
        2.  'narrative_text': string
        3.  'main_theme': string
        4.  'sentiment': number
        5.  'salience_score': number
        6.  'llm_reasoning': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "narrative_text": {"type": "string"},
                "main_theme": {"type": "string"},
                "sentiment": {"type": "number", "minimum": -1.0, "maximum": 1.0},
                "salience_score": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "llm_reasoning": {"type": "string"}
            },
            "required": ["timestamp", "narrative_text", "main_theme", "sentiment", "salience_score", "llm_reasoning"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.7, max_tokens=250)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical fields are floats
                if 'sentiment' in llm_data: llm_data['sentiment'] = float(llm_data['sentiment'])
                if 'salience_score' in llm_data: llm_data['salience_score'] = float(llm_data['salience_score'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for internal narrative: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_NARRATIVE_GENERATION_FAILED", f"LLM call failed for internal narrative: {llm_output_str}", 0.9)
            return None

    def _apply_simple_narrative_rules(self):
        """
        Fallback mechanism to generate a simple internal narrative using rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        narrative_text = "I am processing data."
        main_theme = "idle_reflection"
        sentiment = 0.0
        salience_score = 0.1

        # Rule 1: Prioritize reflection on performance issues
        if self.recent_performance_reports:
            latest_perf = self.recent_performance_reports[-1]
            time_since_perf = current_time - float(latest_perf.get('timestamp', 0.0))
            if time_since_perf < 2.0 and latest_perf.get('suboptimal_flag', False) and latest_perf.get('overall_score', 1.0) < 0.6:
                narrative_text = f"My performance is {latest_perf.get('overall_score'):.2f}. I need to consider how to improve efficiency and address these suboptimal metrics."
                main_theme = "self_assessment_problem"
                sentiment = -0.5
                salience_score = 0.7
                rospy.logdebug(f"{self.node_name}: Simple rule: Generated narrative about suboptimal performance.")
                return narrative_text, main_theme, sentiment, salience_score

        # Rule 2: Reflect on current emotion state
        if self.recent_emotion_states:
            latest_emotion = self.recent_emotion_states[-1]
            time_since_emotion = current_time - float(latest_emotion.get('timestamp', 0.0))
            if time_since_emotion < 1.0 and latest_emotion.get('mood_intensity', 0.0) > 0.4:
                mood = latest_emotion.get('mood', 'neutral')
                if mood == 'happy':
                    narrative_text = "I feel quite positive. This state is conducive to productive tasks."
                    main_theme = "emotional_processing"
                    sentiment = 0.6
                    salience_score = 0.4
                elif mood == 'frustrated':
                    narrative_text = "I am experiencing some frustration. I should identify the source and seek a resolution."
                    main_theme = "emotional_processing_problem_solving"
                    sentiment = -0.6
                    salience_score = 0.5
                rospy.logdebug(f"{self.node_name}: Simple rule: Generated narrative based on emotion: {mood}.")
                return narrative_text, main_theme, sentiment, salience_score
        
        # Rule 3: Acknowledge dominant goal
        if self.recent_motivation_states:
            latest_motivation = self.recent_motivation_states[-1]
            time_since_motivation = current_time - float(latest_motivation.get('timestamp', 0.0))
            if time_since_motivation < 2.0 and latest_motivation.get('dominant_goal_id') != 'none':
                narrative_text = f"My current primary objective is to '{latest_motivation.get('dominant_goal_id')}'. I will continue to focus my resources on achieving this."
                main_theme = "planning_goal_focus"
                sentiment = 0.3
                salience_score = 0.3
                rospy.logdebug(f"{self.node_name}: Simple rule: Generated narrative about dominant goal.")
                return narrative_text, main_theme, sentiment, salience_score

        # Default idle narrative if no other rules apply
        narrative_text = "My systems are stable. I am observing the incoming data streams and preparing for the next task."
        main_theme = "idle_reflection"
        sentiment = 0.0
        salience_score = 0.1
        rospy.logdebug(f"{self.node_name}: Simple rule: Generated default idle narrative.")
        return narrative_text, main_theme, sentiment, salience_score


    def _compile_llm_context_for_narrative(self):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        internal narrative generation.
        """
        context = {
            "current_time": rospy.get_time(),
            "previous_narrative": self.last_generated_narrative,
            "recent_cognitive_inputs": {
                "attention_state": self.recent_attention_states[-1] if self.recent_attention_states else "N/A",
                "emotion_state": self.recent_emotion_states[-1] if self.recent_emotion_states else "N/A",
                "motivation_state": self.recent_motivation_states[-1] if self.recent_motivation_states else "N/A",
                "world_model_state": self.recent_world_model_states[-1] if self.recent_world_model_states else "N/A",
                "performance_report": self.recent_performance_reports[-1] if self.recent_performance_reports else "N/A",
                "memory_responses": list(self.recent_memory_responses),
                "prediction_state": self.recent_prediction_states[-1] if self.recent_prediction_states else "N/A",
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name]
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            item = context["recent_cognitive_inputs"][category_key]
            if isinstance(item, dict):
                for field, value in item.items():
                    if isinstance(value, str) and field.endswith('_json'):
                        try: item[field] = json.loads(value)
                        except json.JSONDecodeError: pass

        return context

    # --- Database and Publishing Functions ---
    def save_internal_narrative_log(self, id, timestamp, narrative_text, main_theme, sentiment, salience_score, llm_reasoning, context_snapshot_json):
        """Saves an internal narrative entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO internal_narrative_log (id, timestamp, narrative_text, main_theme, sentiment, salience_score, llm_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, narrative_text, main_theme, sentiment, salience_score, llm_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved internal narrative log (ID: {id}, Theme: {main_theme}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save internal narrative log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_internal_narrative_log: {e}", 0.9)


    def publish_internal_narrative(self, event):
        """Publishes the robot's current internal narrative."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.last_generated_narrative['timestamp'] = timestamp
        
        try:
            if isinstance(InternalNarrative, type(String)): # Fallback to String message
                self.pub_internal_narrative.publish(json.dumps(self.last_generated_narrative))
            else:
                narrative_msg = InternalNarrative()
                narrative_msg.timestamp = timestamp
                narrative_msg.narrative_text = self.last_generated_narrative['narrative_text']
                narrative_msg.main_theme = self.last_generated_narrative['main_theme']
                narrative_msg.sentiment = self.last_generated_narrative['sentiment']
                narrative_msg.salience_score = self.last_generated_narrative['salience_score']
                self.pub_internal_narrative.publish(narrative_msg)

            rospy.logdebug(f"{self.node_name}: Published Internal Narrative. Theme: '{self.last_generated_narrative['main_theme']}'.")

        except Exception as e:
            self._report_error("PUBLISH_INTERNAL_NARRATIVE_ERROR", f"Failed to publish internal narrative: {e}", 0.7)


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = InternalNarrativeNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, InternalNarrativeNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, InternalNarrativeNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

10. Refactored Memory Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique memory IDs and request IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        MemoryRequest,          # Input: Request to store or retrieve memories
        MemoryResponse,         # Output: Response to a memory request
        CognitiveDirective,     # Input: Directives for memory management (e.g., "forget X", "summarize Y")
        InternalNarrative,      # Input: Robot's internal thoughts (can be stored as narrative memory)
        WorldModelState,        # Input: World state updates (can be stored as episodic memory)
        SocialCognitionState    # Input: Social interactions (can be stored as social memory)
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Memory Node.")
    MemoryRequest = String
    MemoryResponse = String
    CognitiveDirective = String
    InternalNarrative = String
    WorldModelState = String
    SocialCognitionState = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'memory_node': {
                'memory_processing_interval': 0.1, # How often to check memory queue
                'llm_memory_threshold_salience': 0.7, # Cumulative salience to trigger LLM
                'recent_context_window_s': 30.0 # Context window for LLM for memory tasks
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 45.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class MemoryNode:
    def __init__(self):
        rospy.init_node('memory_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "long_term_memory.db")
        self.memory_processing_interval = self.params.get('memory_processing_interval', 0.1) # How often to process requests
        self.llm_memory_threshold_salience = self.params.get('llm_memory_threshold_salience', 0.7) # Salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 30.0) # Context window for LLM

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 45.0) # Longer timeout for memory queries

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'memories' table if it doesn't exist.
        # NEW: Added 'llm_processing_notes', 'original_context_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS memories (
                id TEXT PRIMARY KEY,            -- Unique memory ID (UUID)
                timestamp TEXT,
                category TEXT,                  -- e.g., 'episodic', 'semantic', 'procedural', 'narrative', 'social', 'fact'
                content TEXT,                   -- The actual memory content (text or JSON string of complex data)
                keywords TEXT,                  -- Comma-separated keywords for faster retrieval
                source_node TEXT,               -- Which node created/requested storage of this memory
                salience REAL,                  -- How important or vivid the memory is (0.0 to 1.0)
                llm_processing_notes TEXT,      -- NEW: LLM's notes on processing/summarizing the memory
                original_context_json TEXT      -- NEW: JSON of the original cognitive context when memory was created
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_memories_timestamp ON memories (timestamp)')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_memories_category ON memories (category)')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_memories_keywords ON memories (keywords)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.memory_request_queue = deque() # Stores incoming memory requests

        # Deques to maintain a short history of inputs for general context, not direct storage
        self.recent_cognitive_directives = deque(maxlen=5) # Directives for self (e.g., "summarize memory type X")
        self.recent_internal_narratives = deque(maxlen=5) # Can be candidate memories for storage
        self.recent_world_model_states = deque(maxlen=5) # Can be candidate memories for storage
        self.recent_social_cognition_states = deque(maxlen=5) # Can be candidate memories for storage

        self.cumulative_memory_salience = 0.0 # Aggregated salience to trigger LLM operations

        # --- Publishers ---
        self.pub_memory_response = rospy.Publisher('/memory_response', MemoryResponse, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # For requesting attention or other nodes


        # --- Subscribers ---
        rospy.Subscriber('/memory_request', MemoryRequest, self.memory_request_callback)
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback) # Stringified JSON
        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON
        rospy.Subscriber('/social_cognition_state', String, self.social_cognition_state_callback) # Stringified JSON


        # --- Timer for periodic memory processing ---
        rospy.Timer(rospy.Duration(self.memory_processing_interval), self._run_memory_processing_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's memory system online, database at {self.db_path}.")

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_memory_processing_wrapper(self, event):
        """Wrapper to run the async memory processing from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM memory processing task already active. Skipping new cycle.")
            return

        if self.memory_request_queue:
            request_data = self.memory_request_queue.popleft()
            self.active_llm_task = asyncio.run_coroutine_threadsafe(
                self.process_memory_request_async(request_data, event), self._async_loop
            )
        else:
            rospy.logdebug(f"{self.node_name}: No memory requests in queue.")

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.2, max_tokens=None):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Low temperature for factual/summarization.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        actual_max_tokens = max_tokens if max_tokens is not None else 600 # Higher max_tokens for summarization

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Low temperature for factual consistency
            "max_tokens": actual_max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM operations."""
        self.cumulative_memory_salience += score
        self.cumulative_memory_salience = min(1.0, self.cumulative_memory_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_cognitive_directives, self.recent_internal_narratives,
            self.recent_world_model_states, self.recent_social_cognition_states
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history) ---
    def memory_request_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': (str(uuid.uuid4()), 'request_id'),
            'request_type': ('retrieve', 'request_type'), # 'store', 'retrieve', 'update', 'delete', 'summarize'
            'category': ('general', 'category'),
            'query_text': ('', 'query_text'),
            'content_json': ('{}', 'content_json'), # For 'store' or 'update' requests
            'keywords': ('', 'keywords'),
            'num_results': (1, 'num_results'), # For 'retrieve' requests
            'salience': (0.5, 'salience'), # For 'store' requests, importance of the memory
            'source_node': (self.node_name, 'source_node')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        # Parse content_json if it's a string
        if isinstance(data.get('content_json'), str):
            try:
                data['content_parsed'] = json.loads(data['content_json'])
            except json.JSONDecodeError:
                data['content_parsed'] = {} # Fallback if not valid JSON
        else:
            data['content_parsed'] = data.get('content_json', {}) # Ensure it's a dict

        self.memory_request_queue.append(data)
        # Salience of the request directly influences LLM trigger
        self._update_cumulative_salience(data.get('salience', 0.5) * 0.7) # Memory requests are usually important
        rospy.loginfo(f"{self.node_name}: Queued memory request (ID: {data['request_id']}, Type: {data['request_type']}). Queue size: {len(self.memory_request_queue)}.")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name:
            self.recent_cognitive_directives.append(data) # Add directives for self to context
            # Directives for memory management (e.g., 'OptimizeMemory', 'ForgetMemory') are highly salient
            if data.get('directive_type') in ['OptimizeMemory', 'ForgetMemory', 'SummarizeMemory']:
                self._update_cumulative_salience(data.get('urgency', 0.0) * 0.9)
            rospy.loginfo(f"{self.node_name}: Received directive for self: '{data.get('directive_type', 'N/A')}' (Payload: {data.get('command_payload', 'N/A')}).")
        else:
            self.recent_cognitive_directives.append(data) # Add all directives for general context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def internal_narrative_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),
            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_internal_narratives.append(data)
        # Internal narratives can be good candidates for narrative memory storage
        if data.get('salience_score', 0.0) > 0.6:
            # Auto-store high salience internal narratives as memories
            self.memory_request_queue.append({
                'timestamp': data['timestamp'],
                'request_id': str(uuid.uuid4()),
                'request_type': 'store',
                'category': 'narrative',
                'query_text': '',
                'content_json': json.dumps({'text': data['narrative_text'], 'theme': data['main_theme'], 'sentiment': data['sentiment']}),
                'keywords': data['main_theme'].replace('_', ','),
                'num_results': 0,
                'salience': data['salience_score'],
                'source_node': 'internal_narrative_node'
            })
            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.3)
        rospy.logdebug(f"{self.node_name}: Received Internal Narrative (Theme: {data.get('main_theme', 'N/A')}).")

    def world_model_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),
            'changed_entities_json': ('[]', 'changed_entities_json'),
            'significant_change_flag': (False, 'significant_change_flag'),
            'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('changed_entities_json'), str):
            try: data['changed_entities'] = json.loads(data['changed_entities_json'])
            except json.JSONDecodeError: data['changed_entities'] = []
        self.recent_world_model_states.append(data)
        # Significant world model changes can be stored as episodic memories
        if data.get('significant_change_flag', False):
            # Auto-store significant world model changes as episodic memories
            self.memory_request_queue.append({
                'timestamp': data['timestamp'],
                'request_id': str(uuid.uuid4()),
                'request_type': 'store',
                'category': 'episodic',
                'query_text': '',
                'content_json': json.dumps({'description': f"Significant change in world state: {len(data['changed_entities'])} entities changed.", 'details': data}),
                'keywords': "world_change,environment",
                'num_results': 0,
                'salience': 0.7, # High salience for important world changes
                'source_node': 'world_model_node'
            })
            self._update_cumulative_salience(0.4)
        rospy.logdebug(f"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.")

    def social_cognition_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'inferred_mood': ('neutral', 'inferred_mood'),
            'mood_confidence': (0.0, 'mood_confidence'), 'inferred_intent': ('none', 'inferred_intent'),
            'intent_confidence': (0.0, 'intent_confidence'), 'user_id': ('unknown', 'user_id')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_social_cognition_states.append(data)
        # Important social interactions can be stored as social memories
        if data.get('intent_confidence', 0.0) > 0.6 or data.get('mood_confidence', 0.0) > 0.6:
            # Auto-store salient social interactions as social memories
            self.memory_request_queue.append({
                'timestamp': data['timestamp'],
                'request_id': str(uuid.uuid4()),
                'request_type': 'store',
                'category': 'social',
                'query_text': '',
                'content_json': json.dumps({'user_id': data['user_id'], 'inferred_mood': data['inferred_mood'], 'inferred_intent': data['inferred_intent']}),
                'keywords': f"social,user_{data['user_id']}",
                'num_results': 0,
                'salience': data.get('intent_confidence', 0.0) * 0.5 + data.get('mood_confidence', 0.0) * 0.5,
                'source_node': 'social_cognition_node'
            })
            self._update_cumulative_salience(0.3)
        rospy.logdebug(f"{self.node_name}: Received Social Cognition State. Intent: {data.get('inferred_intent', 'N/A')}.")

    # --- Core Memory Processing Logic (Async with LLM) ---
    async def process_memory_request_async(self, request_data, event):
        """
        Asynchronously processes a memory request (store, retrieve, update, delete, summarize)
        using LLM for advanced operations.
        """
        self._prune_history() # Keep context history fresh

        request_id = request_data.get('request_id')
        request_type = request_data.get('request_type')
        category = request_data.get('category')
        query_text = request_data.get('query_text')
        content_json = request_data.get('content_json') # Original JSON string
        content_parsed = request_data.get('content_parsed') # Already parsed dict
        keywords = request_data.get('keywords')
        num_results = request_data.get('num_results', 1)
        salience = request_data.get('salience', 0.5)
        source_node = request_data.get('source_node', 'unknown')

        memory_response_code = 500 # Default to error
        retrieved_memories = []
        llm_processing_notes = "No LLM processing."
        original_context_snapshot = self._compile_llm_context_for_memory_op(request_data)

        if request_type == 'store':
            # LLM can process content to extract better keywords or summarize
            if self.cumulative_memory_salience >= self.llm_memory_threshold_salience and salience >= 0.6:
                rospy.loginfo(f"{self.node_name}: Triggering LLM for memory store processing (ID: {request_id}, Category: {category}).")
                llm_processed_memory = await self._process_memory_content_llm(content_parsed, category, keywords, original_context_snapshot)
                if llm_processed_memory:
                    content_to_store = llm_processed_memory.get('processed_content', content_parsed)
                    keywords_to_store = llm_processed_memory.get('extracted_keywords', keywords)
                    llm_processing_notes = llm_processed_memory.get('processing_notes', 'LLM processed content.')
                    # Ensure content_to_store is a JSON string before passing to save
                    if isinstance(content_to_store, dict):
                        content_to_store_json = json.dumps(content_to_store)
                    else:
                        content_to_store_json = str(content_to_store) # Fallback to string if not dict
                    
                    self.store_memory(
                        id=str(uuid.uuid4()),
                        timestamp=str(rospy.get_time()),
                        category=category,
                        content=content_to_store_json,
                        keywords=keywords_to_store,
                        source_node=source_node,
                        salience=salience,
                        llm_processing_notes=llm_processing_notes,
                        original_context_json=json.dumps(original_context_snapshot)
                    )
                    memory_response_code = 200
                else:
                    rospy.logwarn(f"{self.node_name}: LLM failed to process memory for storage. Storing raw content. (ID: {request_id})")
                    self.store_memory(
                        id=str(uuid.uuid4()),
                        timestamp=str(rospy.get_time()),
                        category=category,
                        content=content_json, # Store original raw JSON string
                        keywords=keywords,
                        source_node=source_node,
                        salience=salience,
                        llm_processing_notes="LLM processing failed, raw content stored.",
                        original_context_json=json.dumps(original_context_snapshot)
                    )
                    memory_response_code = 200
            else:
                rospy.logdebug(f"{self.node_name}: Insufficient salience for LLM memory processing for store. Storing raw content.")
                self.store_memory(
                    id=str(uuid.uuid4()),
                    timestamp=str(rospy.get_time()),
                    category=category,
                    content=content_json, # Store original raw JSON string
                    keywords=keywords,
                    source_node=source_node,
                    salience=salience,
                    llm_processing_notes="Low salience, raw content stored.",
                    original_context_json=json.dumps(original_context_snapshot)
                )
                memory_response_code = 200
            self.cumulative_memory_salience = 0.0 # Reset after store

        elif request_type == 'retrieve':
            # LLM can assist with semantic search and re-ranking or summarizing retrieved memories
            if self.cumulative_memory_salience >= self.llm_memory_threshold_salience:
                rospy.loginfo(f"{self.node_name}: Triggering LLM for semantic memory retrieval (ID: {request_id}, Query: '{query_text}').")
                # First, retrieve a broader set of potentially relevant memories from DB
                raw_retrieved = self.retrieve_memories_from_db(query_text, category, keywords, num_results * 2) # Get more for LLM re-ranking
                
                if raw_retrieved:
                    llm_retrieved = await self._semantic_retrieve_memories_llm(query_text, raw_retrieved, num_results, original_context_snapshot)
                    if llm_retrieved:
                        retrieved_memories = llm_retrieved.get('ranked_memories', [])
                        llm_processing_notes = llm_retrieved.get('retrieval_notes', 'LLM assisted retrieval.')
                        memory_response_code = 200
                    else:
                        rospy.logwarn(f"{self.node_name}: LLM semantic retrieval failed. Returning raw database results. (ID: {request_id})")
                        retrieved_memories = raw_retrieved[:num_results]
                        llm_processing_notes = "LLM retrieval failed, returning simple DB results."
                        memory_response_code = 200
                else:
                    rospy.logdebug(f"{self.node_name}: No raw memories found for query '{query_text}'.")
                    memory_response_code = 404 # Not found
            else:
                rospy.logdebug(f"{self.node_name}: Insufficient salience for LLM semantic retrieval. Performing simple keyword retrieval.")
                retrieved_memories = self.retrieve_memories_from_db(query_text, category, keywords, num_results)
                llm_processing_notes = "Low salience, simple keyword retrieval."
                memory_response_code = 200 if retrieved_memories else 404
            self.cumulative_memory_salience = 0.0 # Reset after retrieve

        elif request_type == 'update':
            # LLM can help understand the update and apply it intelligently
            if self.cumulative_memory_salience >= self.llm_memory_threshold_salience and salience >= 0.6:
                rospy.loginfo(f"{self.node_name}: Triggering LLM for memory update processing (ID: {request_id}).")
                existing_memories = self.retrieve_memories_from_db(query_text, category, keywords, 1) # Get the most relevant memory
                if existing_memories:
                    updated_content_llm = await self._update_memory_content_llm(existing_memories[0], content_parsed, original_context_snapshot)
                    if updated_content_llm:
                        updated_memory_id = existing_memories[0]['id']
                        content_to_update = updated_content_llm.get('updated_content', content_parsed)
                        llm_processing_notes = updated_content_llm.get('processing_notes', 'LLM updated content.')
                        if isinstance(content_to_update, dict):
                            content_to_update_json = json.dumps(content_to_update)
                        else:
                            content_to_update_json = str(content_to_update)
                        self.update_memory(updated_memory_id, content_to_update_json, keywords, salience, llm_processing_notes, json.dumps(original_context_snapshot))
                        memory_response_code = 200
                    else:
                        rospy.logwarn(f"{self.node_name}: LLM memory update failed. Attempting simple update. (ID: {request_id})")
                        if existing_memories and existing_memories[0].get('id'):
                            self.update_memory(existing_memories[0]['id'], content_json, keywords, salience, "LLM update failed, simple update applied.", json.dumps(original_context_snapshot))
                            memory_response_code = 200
                        else:
                            memory_response_code = 404 # Not found
                else:
                    memory_response_code = 404 # Not found
            else:
                rospy.logdebug(f"{self.node_name}: Insufficient salience for LLM memory update. Performing simple update.")
                existing_memories = self.retrieve_memories_from_db(query_text, category, keywords, 1)
                if existing_memories and existing_memories[0].get('id'):
                    self.update_memory(existing_memories[0]['id'], content_json, keywords, salience, "Low salience, simple update applied.", json.dumps(original_context_snapshot))
                    memory_response_code = 200
                else:
                    memory_response_code = 404 # Not found
            self.cumulative_memory_salience = 0.0 # Reset after update

        elif request_type == 'delete':
            # Simple delete, LLM not strictly needed but could confirm
            num_deleted = self.delete_memory(query_text, category, keywords)
            memory_response_code = 200 if num_deleted > 0 else 404
            rospy.loginfo(f"{self.node_name}: Deleted {num_deleted} memories for query '{query_text}'.")
            self.cumulative_memory_salience = 0.0 # Reset after delete

        elif request_type == 'summarize':
            # LLM is essential for summarization
            if self.cumulative_memory_salience >= self.llm_memory_threshold_salience:
                rospy.loginfo(f"{self.node_name}: Triggering LLM for memory summarization (ID: {request_id}, Query: '{query_text}').")
                memories_to_summarize = self.retrieve_memories_from_db(query_text, category, keywords, 10) # Get a batch of memories
                if memories_to_summarize:
                    summary_output = await self._summarize_memories_llm(memories_to_summarize, original_context_snapshot)
                    if summary_output:
                        retrieved_memories = [{"category": "summary", "content": summary_output.get('summary_text', "Could not generate summary."), "keywords": "summary", "id": str(uuid.uuid4()), "timestamp": str(rospy.get_time())}]
                        llm_processing_notes = summary_output.get('summarization_notes', 'LLM generated summary.')
                        memory_response_code = 200
                    else:
                        rospy.logwarn(f"{self.node_name}: LLM summarization failed. (ID: {request_id})")
                        retrieved_memories = []
                        llm_processing_notes = "LLM summarization failed."
                        memory_response_code = 500 # Internal error
                else:
                    rospy.logwarn(f"{self.node_name}: No memories found to summarize for query '{query_text}'.")
                    memory_response_code = 404 # Not found
            else:
                rospy.logwarn(f"{self.node_name}: Insufficient salience for LLM summarization. Summarization skipped.")
                memory_response_code = 400 # Bad request, or not enough salience
            self.cumulative_memory_salience = 0.0 # Reset after summarize

        else:
            rospy.logwarn(f"{self.node_name}: Unknown memory request type: {request_type} for ID: {request_id}.")
            memory_response_code = 400 # Bad Request

        # Publish the response
        self.publish_memory_response(
            request_id=request_id,
            response_code=memory_response_code,
            memories_json=json.dumps(retrieved_memories) # Send back as JSON string
        )

    async def _process_memory_content_llm(self, content_dict, category, keywords_hint, context_snapshot):
        """
        Uses LLM to process new memory content for better categorization, keyword extraction, or summarization before storage.
        """
        prompt_text = f"""
        You are the Memory Node's LLM assistant. Your task is to process incoming raw memory content to enhance its quality for storage and future retrieval. Analyze the `raw_content` provided, considering the `category_hint` and `keywords_hint`.

        Raw Memory Content:
        --- Raw Content ---
        {json.dumps(content_dict, indent=2)}

        Category Hint: '{category}'
        Keywords Hint: '{keywords_hint}'

        Robot's Current Cognitive Context (for better understanding of memory's significance):
        --- Cognitive Context ---
        {json.dumps(context_snapshot, indent=2)}

        Based on this, propose:
        1.  `processed_content`: object (A refined, possibly summarized or structured version of the content, if beneficial. If the content is already good, return it as is. For text, ensure a 'text' key. For complex data, structure as a meaningful JSON object.)
        2.  `extracted_keywords`: string (A comma-separated list of highly relevant keywords for this memory, improving searchability. Incorporate `keywords_hint` but expand or refine.)
        3.  `processing_notes`: string (Brief notes on how you processed or understood this memory.)

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'processed_content': object
        3.  'extracted_keywords': string
        4.  'processing_notes': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "processed_content": {"type": "object"}, # Flexible JSON structure
                "extracted_keywords": {"type": "string"},
                "processing_notes": {"type": "string"}
            },
            "required": ["timestamp", "processed_content", "extracted_keywords", "processing_notes"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.2, max_tokens=600)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for memory processing: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_MEMORY_PROCESS_FAILED", f"LLM call failed for memory processing: {llm_output_str}", 0.9)
            return None

    async def _semantic_retrieve_memories_llm(self, query_text, raw_memories, num_results, context_snapshot):
        """
        Uses LLM to perform semantic retrieval and re-ranking of memories.
        `raw_memories` are initial candidates retrieved by simple keyword matching.
        """
        memories_str_for_llm = "\n".join([f"- Memory ID: {m.get('id')}\n  Category: {m.get('category')}\n  Keywords: {m.get('keywords')}\n  Content Summary: {m.get('content', '')[:150]}..." for m in raw_memories])

        prompt_text = f"""
        You are the Memory Node's LLM assistant. Your task is to semantically retrieve and rank relevant memories based on a `user_query` from a list of `candidate_memories`. Also consider the robot's `current_cognitive_context` for a more nuanced understanding of the query's intent.

        User Query: '{query_text}'

        Candidate Memories (from initial database search):
        --- Candidate Memories ---
        {memories_str_for_llm}

        Robot's Current Cognitive Context (for query intent and relevance):
        --- Cognitive Context ---
        {json.dumps(context_snapshot, indent=2)}

        Based on the `user_query` and `current_cognitive_context`, identify the {num_results} most relevant memories from the `candidate_memories`.
        Provide your response in JSON format, containing:
        1.  `timestamp`: string (current ROS time)
        2.  `ranked_memories`: array of objects (The selected relevant memories, each including their full original content and metadata. Ordered by relevance, highest first.)
        3.  `retrieval_notes`: string (Brief explanation for the selection and ranking.)

        Ensure each memory in `ranked_memories` includes its 'id', 'timestamp', 'category', 'content', 'keywords', 'source_node', 'salience'.
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "ranked_memories": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {"type": "string"},
                            "timestamp": {"type": "string"},
                            "category": {"type": "string"},
                            "content": {"type": "string"},
                            "keywords": {"type": "string"},
                            "source_node": {"type": "string"},
                            "salience": {"type": "number"}
                        },
                        "required": ["id", "timestamp", "category", "content", "keywords", "source_node", "salience"]
                    }
                },
                "retrieval_notes": {"type": "string"}
            },
            "required": ["timestamp", "ranked_memories", "retrieval_notes"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.1, max_tokens=800)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure salience is float
                if 'ranked_memories' in llm_data:
                    for mem in llm_data['ranked_memories']:
                        if 'salience' in mem: mem['salience'] = float(mem['salience'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for semantic retrieval: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_SEMANTIC_RETRIEVAL_FAILED", f"LLM call failed for semantic retrieval: {llm_output_str}", 0.9)
            return None

    async def _update_memory_content_llm(self, existing_memory, new_content_dict, context_snapshot):
        """
        Uses LLM to intelligently update a memory's content, merging new information.
        """
        prompt_text = f"""
        You are the Memory Node's LLM assistant. Your task is to intelligently update an existing memory's content with new information. You need to integrate the `new_content` into the `existing_memory`, ensuring logical consistency and preserving important details.

        Existing Memory to Update:
        --- Existing Memory ---
        {json.dumps(existing_memory, indent=2)}

        New Content to Integrate:
        --- New Content ---
        {json.dumps(new_content_dict, indent=2)}

        Robot's Current Cognitive Context (for understanding the update's purpose):
        --- Cognitive Context ---
        {json.dumps(context_snapshot, indent=2)}

        Based on this, generate:
        1.  `updated_content`: object (The merged and updated content. Ensure it's a valid JSON object. If the content is text-based, make sure it has a 'text' key.)
        2.  `processing_notes`: string (Brief notes on how you performed the update or any conflicts encountered.)

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'updated_content': object
        3.  'processing_notes': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "updated_content": {"type": "object"},
                "processing_notes": {"type": "string"}
            },
            "required": ["timestamp", "updated_content", "processing_notes"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.2, max_tokens=700)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for memory update: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_MEMORY_UPDATE_FAILED", f"LLM call failed for memory update: {llm_output_str}", 0.9)
            return None

    async def _summarize_memories_llm(self, memories_list, context_snapshot):
        """
        Uses LLM to summarize a list of memories into a concise overview.
        """
        memories_for_llm = "\n".join([f"- ID: {m.get('id')}, Category: {m.get('category')}, Content: {m.get('content', '')[:200]}..." for m in memories_list])

        prompt_text = f"""
        You are the Memory Node's LLM assistant. Your task is to summarize a collection of `memories_list` into a concise and coherent overview. Consider the robot's `current_cognitive_context` to identify the most salient aspects for the summary.

        Memories to Summarize:
        --- Memories ---
        {memories_for_llm}

        Robot's Current Cognitive Context (for identifying key themes for summary):
        --- Cognitive Context ---
        {json.dumps(context_snapshot, indent=2)}

        Based on these memories and context, generate:
        1.  `summary_text`: string (A concise and coherent summary of the provided memories. Focus on key events, facts, or patterns.)
        2.  `summarization_notes`: string (Brief notes on the key themes or insights gained during summarization.)

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'summary_text': string
        3.  'summarization_notes': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "summary_text": {"type": "string"},
                "summarization_notes": {"type": "string"}
            },
            "required": ["timestamp", "summary_text", "summarization_notes"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.3, max_tokens=400) # Slightly higher temp for summarization nuance

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for summarization: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_SUMMARIZATION_FAILED", f"LLM call failed for summarization: {llm_output_str}", 0.9)
            return None


    def _compile_llm_context_for_memory_op(self, request_data):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        memory processing operations.
        """
        context = {
            "current_time": rospy.get_time(),
            "memory_request_details": request_data,
            "recent_cognitive_inputs": {
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name],
                "internal_narratives": list(self.recent_internal_narratives),
                "world_model_states": list(self.recent_world_model_states),
                "social_cognition_states": list(self.recent_social_cognition_states)
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            for i, item in enumerate(context["recent_cognitive_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try:
                                item[field] = json.loads(value)
                            except json.JSONDecodeError:
                                pass # Keep as string if not valid JSON
        return context

    # --- Database Operations (SQLite) ---
    def store_memory(self, id, timestamp, category, content, keywords, source_node, salience, llm_processing_notes, original_context_json):
        """Stores a memory in the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO memories (id, timestamp, category, content, keywords, source_node, salience, llm_processing_notes, original_context_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, category, content, keywords, source_node, salience, llm_processing_notes, original_context_json))
            self.conn.commit()
            rospy.loginfo(f"{self.node_name}: Stored memory (ID: {id}, Category: {category}).")
        except sqlite3.Error as e:
            self._report_error("DB_STORE_ERROR", f"Failed to store memory: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_STORE_ERROR", f"Unexpected error in store_memory: {e}", 0.9)

    def retrieve_memories_from_db(self, query_text, category_filter=None, keywords_filter=None, num_results=5):
        """Retrieves memories from the SQLite database based on query and filters."""
        # Simple keyword matching for initial retrieval. LLM handles semantic.
        query = "SELECT id, timestamp, category, content, keywords, source_node, salience, llm_processing_notes FROM memories WHERE 1=1"
        params = []

        if query_text:
            query += " AND (content LIKE ? OR keywords LIKE ?)"
            params.extend([f"%{query_text}%", f"%{query_text}%"])
        if category_filter and category_filter != 'general':
            query += " AND category = ?"
            params.append(category_filter)
        if keywords_filter: # This assumes comma-separated keywords in DB
            keywords_list = [k.strip() for k in keywords_filter.split(',') if k.strip()]
            keyword_conditions = [f"keywords LIKE ?" for _ in keywords_list]
            if keyword_conditions:
                query += " AND (" + " OR ".join(keyword_conditions) + ")"
                params.extend([f"%{k}%" for k in keywords_list]) # Use LIKE for partial matches

        query += " ORDER BY salience DESC, timestamp DESC LIMIT ?"
        params.append(num_results)

        try:
            self.cursor.execute(query, params)
            rows = self.cursor.fetchall()
            retrieved = []
            for row in rows:
                mem = {
                    'id': row[0],
                    'timestamp': row[1],
                    'category': row[2],
                    'content': row[3], # Raw content string
                    'keywords': row[4],
                    'source_node': row[5],
                    'salience': row[6],
                    'llm_processing_notes': row[7]
                }
                # Attempt to parse content back to dict if it was stored as JSON string
                try:
                    mem['content_parsed'] = json.loads(mem['content'])
                except (json.JSONDecodeError, TypeError):
                    mem['content_parsed'] = mem['content'] # Keep as string if not JSON
                retrieved.append(mem)
            rospy.loginfo(f"{self.node_name}: Retrieved {len(retrieved)} memories for query '{query_text}'.")
            return retrieved
        except sqlite3.Error as e:
            self._report_error("DB_RETRIEVE_ERROR", f"Failed to retrieve memories: {e}", 0.9, {'query_text': query_text, 'category': category_filter})
            return []
        except Exception as e:
            self._report_error("UNEXPECTED_RETRIEVE_ERROR", f"Unexpected error in retrieve_memories_from_db: {e}", 0.9)
            return []

    def update_memory(self, memory_id, new_content_json, new_keywords, new_salience, llm_processing_notes, original_context_json):
        """Updates an existing memory in the SQLite database."""
        try:
            self.cursor.execute('''
                UPDATE memories
                SET content = ?, keywords = ?, salience = ?, llm_processing_notes = ?, original_context_json = ?
                WHERE id = ?
            ''', (new_content_json, new_keywords, new_salience, llm_processing_notes, original_context_json, memory_id))
            self.conn.commit()
            if self.cursor.rowcount > 0:
                rospy.loginfo(f"{self.node_name}: Updated memory (ID: {memory_id}).")
                return True
            else:
                rospy.logwarn(f"{self.node_name}: No memory found with ID: {memory_id} for update.")
                return False
        except sqlite3.Error as e:
            self._report_error("DB_UPDATE_ERROR", f"Failed to update memory {memory_id}: {e}", 0.9)
            return False
        except Exception as e:
            self._report_error("UNEXPECTED_UPDATE_ERROR", f"Unexpected error in update_memory: {e}", 0.9)
            return False

    def delete_memory(self, query_text, category_filter=None, keywords_filter=None):
        """Deletes memories from the SQLite database."""
        query = "DELETE FROM memories WHERE 1=1"
        params = []

        # For simplicity, deletion uses keyword/text matching similar to retrieval
        if query_text:
            query += " AND (content LIKE ? OR keywords LIKE ?)"
            params.extend([f"%{query_text}%", f"%{query_text}%"])
        if category_filter and category_filter != 'general':
            query += " AND category = ?"
            params.append(category_filter)
        if keywords_filter:
            keywords_list = [k.strip() for k in keywords_filter.split(',') if k.strip()]
            keyword_conditions = [f"keywords LIKE ?" for _ in keywords_list]
            if keyword_conditions:
                query += " AND (" + " OR ".join(keyword_conditions) + ")"
                params.extend([f"%{k}%" for k in keywords_list])

        try:
            self.cursor.execute(query, params)
            self.conn.commit()
            rospy.loginfo(f"{self.node_name}: Deleted {self.cursor.rowcount} memories.")
            return self.cursor.rowcount
        except sqlite3.Error as e:
            self._report_error("DB_DELETE_ERROR", f"Failed to delete memories: {e}", 0.9, {'query_text': query_text, 'category': category_filter})
            return 0
        except Exception as e:
            self._report_error("UNEXPECTED_DELETE_ERROR", f"Unexpected error in delete_memory: {e}", 0.9)
            return 0

    # --- Publishing Functions ---
    def publish_memory_response(self, request_id, response_code, memories_json):
        """Publishes the response to a memory request."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(MemoryResponse, type(String)): # Fallback to String message
                response_data = {
                    'timestamp': timestamp,
                    'request_id': request_id,
                    'response_code': response_code,
                    'memories_json': memories_json # Already JSON string
                }
                self.pub_memory_response.publish(json.dumps(response_data))
            else:
                response_msg = MemoryResponse()
                response_msg.timestamp = timestamp
                response_msg.request_id = request_id
                response_msg.response_code = response_code
                response_msg.memories_json = memories_json
                self.pub_memory_response.publish(response_msg)

            rospy.logdebug(f"{self.node_name}: Published Memory Response for request ID: {request_id}. Code: {response_code}.")

        except Exception as e:
            self._report_error("PUBLISH_MEMORY_RESPONSE_ERROR", f"Failed to publish memory response for ID '{request_id}': {e}", 0.7)


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = MemoryNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, MemoryNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, MemoryNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

11. Refactored Performance Metrics Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique report IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        PerformanceReport,      # Output: Overall system performance metrics
        SystemMetric,           # Input: Raw system metrics from various nodes
        CognitiveDirective,     # Input: Directives for performance audit/optimization goals
        MotivationState,        # Input: Current goals (for goal-oriented performance)
        WorldModelState         # Input: Environmental complexity (context for performance)
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Performance Metrics Node.")
    PerformanceReport = String
    SystemMetric = String
    CognitiveDirective = String
    MotivationState = String
    WorldModelState = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'performance_metrics_node': {
                'report_interval': 1.0, # How often to generate a performance report
                'llm_analysis_threshold_salience': 0.6, # Cumulative salience to trigger LLM
                'recent_context_window_s': 15.0 # Window for deques for LLM context
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 30.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class PerformanceMetricsNode:
    def __init__(self):
        rospy.init_node('performance_metrics_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "performance_log.db")
        self.report_interval = self.params.get('report_interval', 1.0) # How often to generate a report
        self.llm_analysis_threshold_salience = self.params.get('llm_analysis_threshold_salience', 0.6) # Salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 15.0) # Context window for LLM

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 30.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'performance_log' table if it doesn't exist.
        # NEW: Added 'llm_analysis_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS performance_log (
                id TEXT PRIMARY KEY,            -- Unique report ID (UUID)
                timestamp TEXT,
                overall_score REAL,             -- Aggregated performance score (0.0 to 1.0)
                suboptimal_flag BOOLEAN,        -- True if performance is below acceptable thresholds
                kpis_json TEXT,                 -- JSON of detailed Key Performance Indicators
                llm_analysis_reasoning TEXT,    -- NEW: LLM's detailed reasoning for performance assessment
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of report
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_performance_timestamp ON performance_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.current_performance_report = {
            'timestamp': str(rospy.get_time()),
            'overall_score': 1.0,
            'suboptimal_flag': False,
            'kpis': {
                'task_completion_rate': 1.0,
                'latency_avg_ms': 50,
                'resource_utilization_avg_percent': 0.2,
                'error_rate': 0.0
            }
        }

        # Deques to maintain a short history of inputs for performance analysis
        self.recent_system_metrics = deque(maxlen=20) # More granular data needed for performance
        self.recent_cognitive_directives = deque(maxlen=5) # Directives for performance audit
        self.recent_motivation_states = deque(maxlen=5) # For goal-oriented performance assessment
        self.recent_world_model_states = deque(maxlen=5) # Environmental complexity context

        self.cumulative_performance_salience = 0.0 # Aggregated salience to trigger LLM analysis

        # --- Publishers ---
        self.pub_performance_report = rospy.Publisher('/performance_report', PerformanceReport, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To request optimization or data mining


        # --- Subscribers ---
        rospy.Subscriber('/system_metrics', SystemMetric, self.system_metric_callback)
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/motivation_state', String, self.motivation_state_callback) # Stringified JSON
        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON


        # --- Timer for periodic performance reporting ---
        rospy.Timer(rospy.Duration(self.report_interval), self._run_performance_analysis_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's performance metrics system online.")
        # Publish initial report
        self.publish_performance_report(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_performance_analysis_wrapper(self, event):
        """Wrapper to run the async performance analysis from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM performance analysis task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.analyze_performance_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.1, max_tokens=300):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Low temperature for factual analysis.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Low temperature for factual analysis
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM analysis."""
        self.cumulative_performance_salience += score
        self.cumulative_performance_salience = min(1.0, self.cumulative_performance_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        # Note: system_metrics deque has a larger maxlen as it needs more granular data
        while self.recent_system_metrics and (current_time - float(self.recent_system_metrics[0].get('timestamp', 0.0))) > self.recent_context_window_s:
            self.recent_system_metrics.popleft()
        
        for history_deque in [
            self.recent_cognitive_directives, self.recent_motivation_states,
            self.recent_world_model_states
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def system_metric_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'metric_name': ('', 'metric_name'),
            'value': (0.0, 'value'), 'unit': ('', 'unit'), 'source_node': ('unknown', 'source_node')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_system_metrics.append(data)
        # High or critical metric values might indicate performance issues
        if "error" in data.get('metric_name', '').lower() and data.get('value', 0.0) > 0:
            self._update_cumulative_salience(0.5)
        elif "latency" in data.get('metric_name', '').lower() and data.get('value', 0.0) > 200: # Example threshold
            self._update_cumulative_salience(0.3)
        rospy.logdebug(f"{self.node_name}: Received System Metric: {data.get('metric_name', 'N/A')}: {data.get('value', 'N/A')}.")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'AuditPerformance':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                self._update_cumulative_salience(data.get('urgency', 0.0) * 1.0) # Explicit audit request is high salience
                rospy.loginfo(f"{self.node_name}: Received directive to audit performance based on reason: '{data.get('reason', 'N/A')}'.")
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective for performance: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data) # Store all directives for context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def motivation_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'dominant_goal_id': ('none', 'dominant_goal_id'),
            'overall_drive_level': (0.0, 'overall_drive_level'), 'active_goals_json': ('{}', 'active_goals_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('active_goals_json'), str):
            try: data['active_goals'] = json.loads(data['active_goals_json'])
            except json.JSONDecodeError: data['active_goals'] = {}
        self.recent_motivation_states.append(data)
        # Current goals define what 'good' performance means (e.g., fast vs. accurate)
        if data.get('overall_drive_level', 0.0) > 0.5:
            self._update_cumulative_salience(data.get('overall_drive_level', 0.0) * 0.2)
        rospy.logdebug(f"{self.node_name}: Received Motivation State. Goal: {data.get('dominant_goal_id', 'N/A')}.")

    def world_model_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),
            'changed_entities_json': ('[]', 'changed_entities_json'),
            'significant_change_flag': (False, 'significant_change_flag'),
            'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('changed_entities_json'), str):
            try: data['changed_entities'] = json.loads(data['changed_entities_json'])
            except json.JSONDecodeError: data['changed_entities'] = []
        self.recent_world_model_states.append(data)
        # Environmental complexity influences expected performance
        if data.get('significant_change_flag', False) and data.get('num_entities', 0) > 5:
            self._update_cumulative_salience(0.3)
        rospy.logdebug(f"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.")

    # --- Core Performance Analysis Logic (Async with LLM) ---
    async def analyze_performance_async(self, event):
        """
        Asynchronously analyzes recent system metrics and cognitive context to generate
        a comprehensive performance report, using LLM for nuanced assessment.
        """
        self._prune_history() # Keep context history fresh

        overall_score = self.current_performance_report.get('overall_score', 1.0)
        suboptimal_flag = self.current_performance_report.get('suboptimal_flag', False)
        kpis = self.current_performance_report.get('kpis', {})
        llm_analysis_reasoning = "Not evaluated by LLM."
        
        if self.cumulative_performance_salience >= self.llm_analysis_threshold_salience or \
           (self.current_performance_report.get('suboptimal_flag', False) and self.current_performance_report.get('overall_score', 1.0) < 0.7):
            rospy.loginfo(f"{self.node_name}: Triggering LLM for performance analysis (Salience: {self.cumulative_performance_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_performance()
            llm_performance_output = await self._assess_performance_llm(context_for_llm)

            if llm_performance_output:
                overall_score = max(0.0, min(1.0, llm_performance_output.get('overall_score', overall_score)))
                suboptimal_flag = llm_performance_output.get('suboptimal_flag', suboptimal_flag)
                kpis = llm_performance_output.get('kpis', kpis)
                llm_analysis_reasoning = llm_performance_output.get('llm_analysis_reasoning', 'LLM provided no specific reasoning.')
                rospy.loginfo(f"{self.node_name}: LLM Performance Report. Score: {overall_score:.2f}. Suboptimal: {suboptimal_flag}.")
            else:
                rospy.logwarn(f"{self.node_name}: LLM performance analysis failed. Applying simple fallback.")
                overall_score, suboptimal_flag, kpis = self._apply_simple_performance_rules()
                llm_analysis_reasoning = "Fallback to simple rules due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_performance_salience:.2f}) for LLM performance analysis. Applying simple rules.")
            overall_score, suboptimal_flag, kpis = self._apply_simple_performance_rules()
            llm_analysis_reasoning = "Fallback to simple rules due to low salience."

        self.current_performance_report = {
            'timestamp': str(rospy.get_time()),
            'overall_score': overall_score,
            'suboptimal_flag': suboptimal_flag,
            'kpis': kpis
        }

        self.save_performance_log(
            id=str(uuid.uuid4()),
            timestamp=self.current_performance_report['timestamp'],
            overall_score=self.current_performance_report['overall_score'],
            suboptimal_flag=self.current_performance_report['suboptimal_flag'],
            kpis_json=json.dumps(self.current_performance_report['kpis']),
            llm_analysis_reasoning=llm_analysis_reasoning,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_performance())
        )
        self.publish_performance_report(None) # Publish updated report
        self.cumulative_performance_salience = 0.0 # Reset after report generation

    async def _assess_performance_llm(self, context_for_llm):
        """
        Uses the LLM to assess the robot's overall performance and identify areas of suboptimality.
        """
        prompt_text = f"""
        You are the Performance Metrics Module of a robot's cognitive architecture, powered by a large language model. Your critical role is to analyze raw system metrics and contextual information to generate a comprehensive performance report. You must assess the robot's `overall_score`, identify if `suboptimal_flag` is true, and provide detailed `kpis`.

        Robot's Recent System Metrics:
        --- Raw System Metrics ---
        {json.dumps(context_for_llm.get('recent_system_metrics', []), indent=2)}

        Robot's Current Cognitive Context (for interpreting performance):
        --- Cognitive Context ---
        {json.dumps(context_for_llm.get('cognitive_context', {}), indent=2)}

        Based on this data, provide:
        1.  `overall_score`: number (0.0 to 1.0, where 1.0 is optimal performance. Aggregate all metrics and context into a single score.)
        2.  `suboptimal_flag`: boolean (True if performance is significantly below expected or desired levels, False otherwise.)
        3.  `kpis`: object (A JSON object containing key performance indicators, e.g., 'task_completion_rate', 'latency_avg_ms', 'resource_utilization_avg_percent', 'error_rate', 'goal_attainment_score').
        4.  `llm_analysis_reasoning`: string (Detailed explanation for your assessment, referencing specific metrics, goals, and environmental factors that influenced the performance.)

        Consider:
        -   **System Metrics**: Analyze `value` for `metric_name` (e.g., high `error_rate`, high `latency_avg_ms`, low `resource_utilization_avg_percent` if idle is expected).
        -   **Motivation State**: Is the robot pursuing a `dominant_goal_id`? How well is it progressing towards it given its `overall_drive_level`? (This implies a 'goal_attainment_score' KPI).
        -   **World Model State**: Is the environment `complexity` high? This might justify lower performance scores. Are there `significant_change_flag`s that explain temporary dips?
        -   **Cognitive Directives**: Was there a directive to `AuditPerformance` or `OptimizePerformance`? What was the reason?

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'overall_score': number
        3.  'suboptimal_flag': boolean
        4.  'kpis': object
        5.  'llm_analysis_reasoning': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "overall_score": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "suboptimal_flag": {"type": "boolean"},
                "kpis": {"type": "object"}, # Flexible JSON structure for KPIs
                "llm_analysis_reasoning": {"type": "string"}
            },
            "required": ["timestamp", "overall_score", "suboptimal_flag", "kpis", "llm_analysis_reasoning"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.1, max_tokens=400)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure boolean/numerical fields are correctly parsed
                if 'overall_score' in llm_data: llm_data['overall_score'] = float(llm_data['overall_score'])
                if 'suboptimal_flag' in llm_data: llm_data['suboptimal_flag'] = bool(llm_data['suboptimal_flag'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for performance: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_PERFORMANCE_ANALYSIS_FAILED", f"LLM call failed for performance analysis: {llm_output_str}", 0.9)
            return None

    def _apply_simple_performance_rules(self):
        """
        Fallback mechanism to generate a simple performance report using rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        # Calculate simple KPIs from recent system metrics
        total_latency = 0.0
        error_count = 0
        cpu_util_sum = 0.0
        num_metrics = 0
        
        for metric in self.recent_system_metrics:
            if current_time - float(metric.get('timestamp', 0.0)) < 5.0: # Only consider very recent metrics for simple average
                if metric.get('metric_name') == 'latency_ms':
                    total_latency += metric.get('value', 0.0)
                elif metric.get('metric_name') == 'error_count':
                    error_count += metric.get('value', 0.0)
                elif metric.get('metric_name') == 'cpu_util_percent':
                    cpu_util_sum += metric.get('value', 0.0)
                num_metrics += 1

        avg_latency = total_latency / num_metrics if num_metrics > 0 else 0.0
        avg_cpu_util = cpu_util_sum / num_metrics if num_metrics > 0 else 0.0
        
        # Simple task completion rate (hypothetical for fallback)
        task_completion_rate = 1.0 # Assume perfect unless an error flag indicates otherwise
        if error_count > 0 or avg_latency > 150:
            task_completion_rate = 0.8 # Reduced if errors or high latency

        kpis = {
            'task_completion_rate': task_completion_rate,
            'latency_avg_ms': avg_latency,
            'resource_utilization_avg_percent': avg_cpu_util,
            'error_rate': error_count
        }

        # Determine overall score and suboptimal flag
        overall_score = 1.0
        suboptimal_flag = False

        if kpis['latency_avg_ms'] > 100:
            overall_score -= 0.2
            suboptimal_flag = True
        if kpis['error_rate'] > 0:
            overall_score -= 0.3
            suboptimal_flag = True
        if kpis['task_completion_rate'] < 0.9:
            overall_score -= 0.4
            suboptimal_flag = True
        
        # Clamp score between 0 and 1
        overall_score = max(0.0, min(1.0, overall_score))

        rospy.logwarn(f"{self.node_name}: Simple rule: Generated fallback performance report. Score: {overall_score:.2f}.")
        return overall_score, suboptimal_flag, kpis


    def _compile_llm_context_for_performance(self):
        """
        Gathers and formats all relevant data for the LLM's performance assessment.
        """
        context = {
            "current_time": rospy.get_time(),
            "last_performance_report": self.current_performance_report,
            "recent_system_metrics": list(self.recent_system_metrics),
            "cognitive_context": {
                "latest_motivation_state": self.recent_motivation_states[-1] if self.recent_motivation_states else "N/A",
                "latest_world_model_state": self.recent_world_model_states[-1] if self.recent_world_model_states else "N/A",
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name]
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["cognitive_context"]:
            item = context["cognitive_context"][category_key]
            if isinstance(item, dict):
                for field, value in item.items():
                    if isinstance(value, str) and field.endswith('_json'):
                        try: item[field] = json.loads(value)
                        except json.JSONDecodeError: pass

        return context

    # --- Database and Publishing Functions ---
    def save_performance_log(self, id, timestamp, overall_score, suboptimal_flag, kpis_json, llm_analysis_reasoning, context_snapshot_json):
        """Saves a performance report entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO performance_log (id, timestamp, overall_score, suboptimal_flag, kpis_json, llm_analysis_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, overall_score, suboptimal_flag, kpis_json, llm_analysis_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved performance log (ID: {id}, Score: {overall_score}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save performance log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_performance_log: {e}", 0.9)


    def publish_performance_report(self, event):
        """Publishes the robot's current performance report."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.current_performance_report['timestamp'] = timestamp
        
        try:
            if isinstance(PerformanceReport, type(String)): # Fallback to String message
                self.pub_performance_report.publish(json.dumps(self.current_performance_report))
            else:
                report_msg = PerformanceReport()
                report_msg.timestamp = timestamp
                report_msg.overall_score = self.current_performance_report['overall_score']
                report_msg.suboptimal_flag = self.current_performance_report['suboptimal_flag']
                report_msg.kpis_json = json.dumps(self.current_performance_report['kpis'])
                self.pub_performance_report.publish(report_msg)

            rospy.logdebug(f"{self.node_name}: Published Performance Report. Score: '{self.current_performance_report['overall_score']}'.")

        except Exception as e:
            self._report_error("PUBLISH_PERFORMANCE_REPORT_ERROR", f"Failed to publish performance report: {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Performance Metrics Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = PerformanceMetricsNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, PerformanceMetricsNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, PerformanceMetricsNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

12. Refactored Sensory Qualia Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique qualia IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        SensoryQualia,          # Output: Processed sensory data with qualia attributes
        RawSensorData,          # Input: Raw sensor data (e.g., camera, microphone, lidar)
        CognitiveDirective,     # Input: Directives for sensory processing focus
        AttentionState,         # Input: Current attention focus (influences salience assessment)
        WorldModelState         # Input: Current world state (context for sensory interpretation)
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Sensory Qualia Node.")
    SensoryQualia = String
    RawSensorData = String
    CognitiveDirective = String
    AttentionState = String
    WorldModelState = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
            else:
                # Attempt to get attributes directly from the message object
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'sensory_qualia_node': {
                'processing_interval': 0.1, # How often to process incoming raw sensor data
                'llm_interpretation_threshold_salience': 0.7, # Cumulative salience to trigger LLM
                'recent_context_window_s': 5.0 # Window for deques for LLM context (short for real-time sensory)
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 15.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class SensoryQualiaNode:
    def __init__(self):
        rospy.init_node('sensory_qualia_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "sensory_log.db")
        self.processing_interval = self.params.get('processing_interval', 0.1) # How often to process raw sensor data
        self.llm_interpretation_threshold_salience = self.params.get('llm_interpretation_threshold_salience', 0.7) # Salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 5.0) # Context window for LLM

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 15.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'sensory_log' table if it doesn't exist.
        # NEW: Added 'llm_interpretation_notes', 'raw_data_hash'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS sensory_log (
                id TEXT PRIMARY KEY,            -- Unique qualia ID (UUID)
                timestamp TEXT,
                qualia_type TEXT,               -- e.g., 'visual_perception', 'auditory_stimulus', 'tactile_sensation'
                modality TEXT,                  -- e.g., 'camera', 'microphone', 'lidar', 'touch_sensor'
                description_summary TEXT,       -- Concise summary of the sensory experience
                salience_score REAL,            -- How attention-grabbing this qualia is (0.0 to 1.0)
                llm_interpretation_notes TEXT,  -- NEW: LLM's detailed interpretation of the qualia
                raw_data_hash TEXT,             -- HASH of the raw input data for traceability (e.g., md5 of image)
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of interpretation
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_sensory_timestamp ON sensory_log (timestamp)')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_sensory_modality ON sensory_log (modality)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.raw_sensor_data_queue = deque() # Stores incoming raw sensor data for processing

        # Deques to maintain a short history of inputs relevant to sensory interpretation
        self.recent_cognitive_directives = deque(maxlen=3) # Directives for sensory processing focus
        self.recent_attention_states = deque(maxlen=3) # Current attention focus (influences salience)
        self.recent_world_model_states = deque(maxlen=3) # Context for interpreting new sensory data

        self.cumulative_sensory_salience = 0.0 # Aggregated salience to trigger LLM interpretation

        # --- Publishers ---
        self.pub_sensory_qualia = rospy.Publisher('/sensory_qualia', SensoryQualia, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To request attention or memory


        # --- Subscribers ---
        rospy.Subscriber('/raw_sensor_data', RawSensorData, self.raw_sensor_data_callback)
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/attention_state', AttentionState, self.attention_state_callback)
        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON


        # --- Timer for periodic sensory processing ---
        rospy.Timer(rospy.Duration(self.processing_interval), self._run_sensory_processing_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's sensory qualia system online, ready to interpret perceptions.")

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_sensory_processing_wrapper(self, event):
        """Wrapper to run the async sensory processing from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM sensory processing task already active. Skipping new cycle.")
            return

        if self.raw_sensor_data_queue:
            raw_data = self.raw_sensor_data_queue.popleft()
            self.active_llm_task = asyncio.run_coroutine_threadsafe(
                self.process_raw_sensor_data_async(raw_data, event), self._async_loop
            )
        else:
            rospy.logdebug(f"{self.node_name}: No raw sensor data in queue.")

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.3, max_tokens=200):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Low temperature for factual interpretation.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Low temperature for factual interpretation of sensory data
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM interpretation."""
        self.cumulative_sensory_salience += score
        self.cumulative_sensory_salience = min(1.0, self.cumulative_sensory_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_cognitive_directives, self.recent_attention_states,
            self.recent_world_model_states
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def raw_sensor_data_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'sensor_id': ('', 'sensor_id'),
            'modality': ('unknown', 'modality'), 'raw_data_json': ('{}', 'raw_data_json'),
            'data_hash': ('', 'data_hash'), 'urgency': (0.0, 'urgency') # Urgency for this specific raw data
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        # Parse raw_data_json if it's a string
        if isinstance(data.get('raw_data_json'), str):
            try:
                data['raw_data_parsed'] = json.loads(data['raw_data_json'])
            except json.JSONDecodeError:
                data['raw_data_parsed'] = {} # Fallback if not valid JSON
        else:
            data['raw_data_parsed'] = data.get('raw_data_json', {}) # Ensure it's a dict

        self.raw_sensor_data_queue.append(data)
        # Salience of the raw sensor data influences LLM trigger
        self._update_cumulative_salience(data.get('urgency', 0.0) * 0.8) # High urgency for direct sensor data
        rospy.logdebug(f"{self.node_name}: Queued raw sensor data (Modality: {data['modality']}, Sensor: {data['sensor_id']}). Queue size: {len(self.raw_sensor_data_queue)}.")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'FocusSensoryProcessing':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                # This directive doesn't go into a queue, it directly influences the next processing cycle
                self._update_cumulative_salience(data.get('urgency', 0.0) * 0.9) # High urgency for focus directives
                rospy.loginfo(f"{self.node_name}: Received directive to focus sensory processing on reason: '{data.get('reason', 'N/A')}'.")
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective for sensory: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data) # Store all directives for context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def attention_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'focus_type': ('idle', 'focus_type'),
            'focus_target': ('environment', 'focus_target'), 'priority_score': (0.0, 'priority_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_attention_states.append(data)
        # What the robot is attending to influences what sensory input is considered salient
        if data.get('priority_score', 0.0) > 0.5:
            self._update_cumulative_salience(data.get('priority_score', 0.0) * 0.3)
        rospy.logdebug(f"{self.node_name}: Received Attention State. Focus: {data.get('focus_target', 'N/A')}.")

    def world_model_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),
            'changed_entities_json': ('[]', 'changed_entities_json'),
            'significant_change_flag': (False, 'significant_change_flag'),
            'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('changed_entities_json'), str):
            try: data['changed_entities'] = json.loads(data['changed_entities_json'])
            except json.JSONDecodeError: data['changed_entities'] = []
        self.recent_world_model_states.append(data)
        # World model provides context for interpreting sensory data (e.g., expecting a human in a certain area)
        if data.get('significant_change_flag', False):
            self._update_cumulative_salience(0.2)
        rospy.logdebug(f"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.")

    # --- Core Sensory Processing Logic (Async with LLM) ---
    async def process_raw_sensor_data_async(self, raw_data, event):
        """
        Asynchronously processes raw sensor data to extract 'qualia' (meaningful, salient perceptions),
        using LLM for higher-level interpretation.
        """
        self._prune_history() # Keep context history fresh

        qualia_id = str(uuid.uuid4())
        timestamp = raw_data.get('timestamp', str(rospy.get_time()))
        modality = raw_data.get('modality', 'unknown')
        raw_data_hash = raw_data.get('data_hash', '')
        
        qualia_type = 'unspecified_perception'
        description_summary = "Raw data processed."
        salience_score = 0.0 # Default low, will be updated
        llm_interpretation_notes = "No LLM interpretation."

        if self.cumulative_sensory_salience >= self.llm_interpretation_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for sensory interpretation (Modality: {modality}, Salience: {self.cumulative_sensory_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_sensory_interpretation(raw_data)
            llm_qualia_output = await self._interpret_sensory_data_llm(raw_data['raw_data_parsed'], modality, context_for_llm)

            if llm_qualia_output:
                qualia_type = llm_qualia_output.get('qualia_type', 'unspecified_perception')
                description_summary = llm_qualia_output.get('description_summary', 'No summary.')
                salience_score = max(0.0, min(1.0, llm_qualia_output.get('salience_score', 0.0)))
                llm_interpretation_notes = llm_qualia_output.get('llm_interpretation_notes', 'LLM interpreted sensory data.')
                rospy.loginfo(f"{self.node_name}: LLM Interpreted Qualia: '{description_summary}' (Salience: {salience_score:.2f}).")
            else:
                rospy.logwarn(f"{self.node_name}: LLM sensory interpretation failed. Applying simple fallback.")
                qualia_type, description_summary, salience_score = self._apply_simple_sensory_rules(raw_data)
                llm_interpretation_notes = "Fallback to simple rules due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_sensory_salience:.2f}) for LLM sensory interpretation. Applying simple rules.")
            qualia_type, description_summary, salience_score = self._apply_simple_sensory_rules(raw_data)
            llm_interpretation_notes = "Fallback to simple rules due to low salience."

        # Update salience_score based on the raw data's urgency if it was not high enough for LLM
        # This ensures high urgency raw data still gets a decent salience_score even without LLM
        salience_score = max(salience_score, raw_data.get('urgency', 0.0))

        self.save_sensory_log(
            id=qualia_id,
            timestamp=timestamp,
            qualia_type=qualia_type,
            modality=modality,
            description_summary=description_summary,
            salience_score=salience_score,
            llm_interpretation_notes=llm_interpretation_notes,
            raw_data_hash=raw_data_hash,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_sensory_interpretation(raw_data))
        )
        self.publish_sensory_qualia(
            timestamp=timestamp,
            qualia_id=qualia_id,
            qualia_type=qualia_type,
            modality=modality,
            description_summary=description_summary,
            salience_score=salience_score,
            raw_data_hash=raw_data_hash
        )
        self.cumulative_sensory_salience = 0.0 # Reset after processing

    async def _interpret_sensory_data_llm(self, raw_data_parsed, modality, context_for_llm):
        """
        Uses the LLM to interpret raw sensor data into meaningful sensory qualia.
        """
        prompt_text = f"""
        You are the Sensory Qualia Module of a robot's cognitive architecture, powered by a large language model. Your role is to interpret `raw_sensor_data` from a specific `modality` into high-level `SensoryQualia`. This involves describing the perception, categorizing its `qualia_type`, and assigning a `salience_score` based on its importance and current `cognitive_context`.

        Raw Sensor Data:
        --- Raw Data (from {modality} sensor) ---
        {json.dumps(raw_data_parsed, indent=2)}

        Robot's Current Cognitive Context (for interpreting sensory data):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this, provide:
        1.  `qualia_type`: string (The type of perception, e.g., 'visual_perception', 'auditory_stimulus', 'tactile_sensation', 'temperature_change', 'proximity_detection').
        2.  `description_summary`: string (A concise, human-readable summary of the sensory experience, e.g., "Detected a human figure approaching", "Heard a loud bang", "Felt a warm surface").
        3.  `salience_score`: number (0.0 to 1.0, indicating how attention-grabbing or important this sensory input is. Higher score for unexpected, urgent, or goal-relevant perceptions.)
        4.  `llm_interpretation_notes`: string (Brief notes on your interpretation process and why certain details were highlighted.)

        Consider:
        -   **Raw Data**: What are the key features in the raw data? (e.g., for camera: presence of objects, colors, movement; for audio: loudness, frequency, speech; for lidar: distance, obstacles).
        -   **Modality**: How does the modality influence interpretation?
        -   **Cognitive Directives**: Was there a directive to `FocusSensoryProcessing` on something specific (e.g., "look for red objects")?
        -   **Attention State**: What is the robot's current `focus_target` and `priority_score`? Does this sensory input align with it, increasing its salience?
        -   **World Model State**: Does this sensory data confirm or contradict the current `world_model_state`? Is it a `significant_change_flag`?

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'qualia_type': string
        3.  'description_summary': string
        4.  'salience_score': number
        5.  'llm_interpretation_notes': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "qualia_type": {"type": "string"},
                "description_summary": {"type": "string"},
                "salience_score": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "llm_interpretation_notes": {"type": "string"}
            },
            "required": ["timestamp", "qualia_type", "description_summary", "salience_score", "llm_interpretation_notes"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.3, max_tokens=250)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical fields are floats
                if 'salience_score' in llm_data: llm_data['salience_score'] = float(llm_data['salience_score'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for sensory qualia: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_SENSORY_INTERPRETATION_FAILED", f"LLM call failed for sensory interpretation: {llm_output_str}", 0.9)
            return None

    def _apply_simple_sensory_rules(self, raw_data):
        """
        Fallback mechanism to process raw sensor data into simple qualia using rule-based logic
        if LLM is not triggered or fails.
        """
        modality = raw_data.get('modality', 'unknown')
        raw_data_parsed = raw_data.get('raw_data_parsed', {})
        urgency = raw_data.get('urgency', 0.0)
        
        qualia_type = 'generic_perception'
        description_summary = f"Processed raw data from {modality} sensor."
        salience_score = urgency * 0.5 # Base salience on urgency

        # Example simple rules based on modality and hypothetical content
        if modality == 'camera':
            if 'object_detected' in raw_data_parsed and raw_data_parsed['object_detected']:
                qualia_type = 'visual_object_detection'
                description_summary = f"Visually detected: {raw_data_parsed.get('object_type', 'an object')}."
                salience_score = max(salience_score, 0.4)
            elif 'motion_detected' in raw_data_parsed and raw_data_parsed['motion_detected']:
                qualia_type = 'visual_motion'
                description_summary = "Detected visual motion."
                salience_score = max(salience_score, 0.3)
        elif modality == 'microphone':
            if 'sound_level' in raw_data_parsed and raw_data_parsed['sound_level'] > 70: # dB threshold
                qualia_type = 'auditory_loud_sound'
                description_summary = "Heard a loud sound."
                salience_score = max(salience_score, 0.6)
            elif 'speech_detected' in raw_data_parsed and raw_data_parsed['speech_detected']:
                qualia_type = 'auditory_speech'
                description_summary = "Detected human speech."
                salience_score = max(salience_score, 0.5)
        elif modality == 'lidar':
            if 'closest_distance' in raw_data_parsed and raw_data_parsed['closest_distance'] < 0.5: # meters
                qualia_type = 'proximity_alert'
                description_summary = "Obstacle detected very close."
                salience_score = max(salience_score, 0.8)

        rospy.logwarn(f"{self.node_name}: Simple rule: Generated fallback qualia for '{modality}'. Summary: {description_summary}.")
        return qualia_type, description_summary, salience_score


    def _compile_llm_context_for_sensory_interpretation(self, raw_data):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        sensory interpretation.
        """
        context = {
            "current_time": rospy.get_time(),
            "raw_sensor_data_source": {
                "timestamp": raw_data.get('timestamp'),
                "sensor_id": raw_data.get('sensor_id'),
                "modality": raw_data.get('modality'),
                "urgency_from_sensor": raw_data.get('urgency', 0.0)
            },
            "recent_cognitive_inputs": {
                "attention_state": self.recent_attention_states[-1] if self.recent_attention_states else "N/A",
                "world_model_state": self.recent_world_model_states[-1] if self.recent_world_model_states else "N/A",
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name]
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            item = context["recent_cognitive_inputs"][category_key]
            if isinstance(item, dict):
                for field, value in item.items():
                    if isinstance(value, str) and field.endswith('_json'):
                        try: item[field] = json.loads(value)
                        except json.JSONDecodeError: pass

        return context

    # --- Database and Publishing Functions ---
    def save_sensory_log(self, id, timestamp, qualia_type, modality, description_summary, salience_score, llm_interpretation_notes, raw_data_hash, context_snapshot_json):
        """Saves a sensory qualia entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO sensory_log (id, timestamp, qualia_type, modality, description_summary, salience_score, llm_interpretation_notes, raw_data_hash, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, qualia_type, modality, description_summary, salience_score, llm_interpretation_notes, raw_data_hash, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved sensory log (ID: {id}, Type: {qualia_type}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save sensory log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_sensory_log: {e}", 0.9)


    def publish_sensory_qualia(self, timestamp, qualia_id, qualia_type, modality, description_summary, salience_score, raw_data_hash):
        """Publishes the processed sensory qualia."""
        try:
            if isinstance(SensoryQualia, type(String)): # Fallback to String message
                qualia_data = {
                    'timestamp': timestamp,
                    'qualia_id': qualia_id,
                    'qualia_type': qualia_type,
                    'modality': modality,
                    'description_summary': description_summary,
                    'salience_score': salience_score,
                    'raw_data_hash': raw_data_hash
                }
                self.pub_sensory_qualia.publish(json.dumps(qualia_data))
            else:
                qualia_msg = SensoryQualia()
                qualia_msg.timestamp = timestamp
                qualia_msg.qualia_id = qualia_id
                qualia_msg.qualia_type = qualia_type
                qualia_msg.modality = modality
                qualia_msg.description_summary = description_summary
                qualia_msg.salience_score = salience_score
                qualia_msg.raw_data_hash = raw_data_hash
                self.pub_sensory_qualia.publish(qualia_msg)

            rospy.loginfo(f"{self.node_name}: Published Sensory Qualia. Type: '{qualia_type}', Summary: '{description_summary}'.")

        except Exception as e:
            self._report_error("PUBLISH_SENSORY_QUALIA_ERROR", f"Failed to publish sensory qualia for '{modality}': {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Sensory Qualia Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = SensoryQualiaNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, SensoryQualiaNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, SensoryQualiaNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

13. Refactored Prediction Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique prediction IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        PredictionState,        # Output: Predicted future events and their confidence
        SensoryQualia,          # Input: Current sensory perceptions (for predicting immediate future)
        WorldModelState,        # Input: Current world state (for environmental context and dynamics)
        MemoryResponse,         # Input: Retrieved historical data, patterns, causal links
        CognitiveDirective,     # Input: Directives for specific predictions or prediction focus
        AttentionState,         # Input: Current attention focus (influences what to predict about)
        MotivationState         # Input: Current goals (for goal-oriented predictions)
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Prediction Node.")
    PredictionState = String
    SensoryQualia = String
    WorldModelState = String
    MemoryResponse = String
    CognitiveDirective = String
    AttentionState = String
    MotivationState = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'prediction_node': {
                'prediction_interval': 0.5, # How often to generate new predictions
                'llm_prediction_threshold_salience': 0.6, # Cumulative salience to trigger LLM
                'recent_context_window_s': 10.0 # Window for deques for LLM context
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 25.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class PredictionNode:
    def __init__(self):
        rospy.init_node('prediction_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "prediction_log.db")
        self.prediction_interval = self.params.get('prediction_interval', 0.5) # How often to generate new predictions
        self.llm_prediction_threshold_salience = self.params.get('llm_prediction_threshold_salience', 0.6) # Salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 10.0) # Context window for LLM

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 25.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'predictions' table if it doesn't exist.
        # NEW: Added 'llm_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS predictions (
                id TEXT PRIMARY KEY,            -- Unique prediction ID (UUID)
                timestamp TEXT,
                predicted_event TEXT,           -- Description of the predicted event
                prediction_confidence REAL,     -- Confidence score of the prediction (0.0 to 1.0)
                prediction_accuracy REAL,       -- Actual accuracy (updated post-facto if possible)
                urgency_flag BOOLEAN,           -- True if the predicted event requires immediate attention
                llm_reasoning TEXT,             -- NEW: LLM's detailed reasoning for the prediction
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of prediction
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_predictions_timestamp ON predictions (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.last_generated_prediction = {
            'timestamp': str(rospy.get_time()),
            'predicted_event': 'The robot will continue operating normally.',
            'prediction_confidence': 0.9,
            'prediction_accuracy': -1.0, # -1.0 means not yet evaluated
            'urgency_flag': False
        }

        # Deques to maintain a short history of inputs relevant to prediction
        self.recent_sensory_qualia = deque(maxlen=5) # Immediate environmental cues
        self.recent_world_model_states = deque(maxlen=5) # Environmental dynamics and objects
        self.recent_memory_responses = deque(maxlen=5) # Historical patterns, causal knowledge
        self.recent_cognitive_directives = deque(maxlen=3) # Directives for specific predictions
        self.recent_attention_states = deque(maxlen=3) # What robot is focusing on
        self.recent_motivation_states = deque(maxlen=3) # Goals (predict success/failure)

        self.cumulative_prediction_salience = 0.0 # Aggregated salience to trigger LLM prediction

        # --- Publishers ---
        self.pub_prediction_state = rospy.Publisher('/prediction_state', PredictionState, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To signal urgent predictions


        # --- Subscribers ---
        rospy.Subscriber('/sensory_qualia', SensoryQualia, self.sensory_qualia_callback)
        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/attention_state', AttentionState, self.attention_state_callback)
        rospy.Subscriber('/motivation_state', String, self.motivation_state_callback) # Stringified JSON


        # --- Timer for periodic prediction generation ---
        rospy.Timer(rospy.Duration(self.prediction_interval), self._run_prediction_generation_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's prediction system online.")
        # Publish initial prediction
        self.publish_prediction_state(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_prediction_generation_wrapper(self, event):
        """Wrapper to run the async prediction generation from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM prediction task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.generate_prediction_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.2, max_tokens=250):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Low temperature for factual prediction.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Low temperature for factual prediction
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM prediction."""
        self.cumulative_prediction_salience += score
        self.cumulative_prediction_salience = min(1.0, self.cumulative_prediction_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_sensory_qualia, self.recent_world_model_states,
            self.recent_memory_responses, self.recent_cognitive_directives,
            self.recent_attention_states, self.recent_motivation_states
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def sensory_qualia_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'qualia_id': ('', 'qualia_id'),
            'qualia_type': ('none', 'qualia_type'), 'modality': ('none', 'modality'),
            'description_summary': ('', 'description_summary'), 'salience_score': (0.0, 'salience_score'),
            'raw_data_hash': ('', 'raw_data_hash')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_sensory_qualia.append(data)
        # Highly salient sensory input can trigger predictions (e.g., loud noise -> collision?)
        if data.get('salience_score', 0.0) > 0.7:
            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.4)
        rospy.logdebug(f"{self.node_name}: Received Sensory Qualia. Description: {data.get('description_summary', 'N/A')}.")

    def world_model_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),
            'changed_entities_json': ('[]', 'changed_entities_json'),
            'significant_change_flag': (False, 'significant_change_flag'),
            'consistency_score': (1.0, 'consistency_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('changed_entities_json'), str):
            try: data['changed_entities'] = json.loads(data['changed_entities_json'])
            except json.JSONDecodeError: data['changed_entities'] = []
        self.recent_world_model_states.append(data)
        # Changes in world model, especially involving dynamic objects or human presence, trigger predictions
        if data.get('significant_change_flag', False):
            self._update_cumulative_salience(0.5)
        rospy.logdebug(f"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Recalled historical patterns or causal relationships are critical for prediction
        if data.get('response_code', 0) == 200 and data.get('memories'):
            if any('pattern' in mem.get('category', '') or 'causal_link' in mem.get('category', '') for mem in data['memories']):
                self._update_cumulative_salience(0.6)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'PredictEvent':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                # This directive doesn't go into a queue, it directly influences the next prediction cycle
                self._update_cumulative_salience(data.get('urgency', 0.0) * 1.0) # High urgency for direct prediction requests
                rospy.loginfo(f"{self.node_name}: Received directive to predict event based on reason: '{data.get('reason', 'N/A')}'.")
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective for prediction: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data) # Store all directives for context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def attention_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'focus_type': ('idle', 'focus_type'),
            'focus_target': ('environment', 'focus_target'), 'priority_score': (0.0, 'priority_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_attention_states.append(data)
        # What the robot is attending to is often what needs prediction
        if data.get('priority_score', 0.0) > 0.6:
            self._update_cumulative_salience(data.get('priority_score', 0.0) * 0.3)
        rospy.logdebug(f"{self.node_name}: Received Attention State. Focus: {data.get('focus_target', 'N/A')}.")

    def motivation_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'dominant_goal_id': ('none', 'dominant_goal_id'),
            'overall_drive_level': (0.0, 'overall_drive_level'), 'active_goals_json': ('{}', 'active_goals_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('active_goals_json'), str):
            try: data['active_goals'] = json.loads(data['active_goals_json'])
            except json.JSONDecodeError: data['active_goals'] = {}
        self.recent_motivation_states.append(data)
        # Current goals require predictions about success/failure or progress impediments
        if data.get('overall_drive_level', 0.0) > 0.5:
            self._update_cumulative_salience(data.get('overall_drive_level', 0.0) * 0.2)
        rospy.logdebug(f"{self.node_name}: Received Motivation State. Goal: {data.get('dominant_goal_id', 'N/A')}.")

    # --- Core Prediction Logic (Async with LLM) ---
    async def generate_prediction_async(self, event):
        """
        Asynchronously generates predictions about future events based on integrated
        cognitive states, using LLM for probabilistic reasoning.
        """
        self._prune_history() # Keep context history fresh

        predicted_event = "No significant event predicted."
        prediction_confidence = 0.5
        urgency_flag = False
        llm_reasoning = "Not evaluated by LLM."
        
        if self.cumulative_prediction_salience >= self.llm_prediction_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for prediction generation (Salience: {self.cumulative_prediction_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_prediction()
            llm_prediction_output = await self._make_prediction_llm(context_for_llm)

            if llm_prediction_output:
                predicted_event = llm_prediction_output.get('predicted_event', 'No prediction.')
                prediction_confidence = max(0.0, min(1.0, llm_prediction_output.get('prediction_confidence', 0.5)))
                urgency_flag = llm_prediction_output.get('urgency_flag', False)
                llm_reasoning = llm_prediction_output.get('llm_reasoning', 'LLM provided no specific reasoning.')
                rospy.loginfo(f"{self.node_name}: LLM Predicted: '{predicted_event}' (Confidence: {prediction_confidence:.2f}, Urgent: {urgency_flag}).")
            else:
                rospy.logwarn(f"{self.node_name}: LLM prediction generation failed. Applying simple fallback.")
                predicted_event, prediction_confidence, urgency_flag = self._apply_simple_prediction_rules()
                llm_reasoning = "Fallback to simple rules due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_prediction_salience:.2f}) for LLM prediction. Applying simple rules.")
            predicted_event, prediction_confidence, urgency_flag = self._apply_simple_prediction_rules()
            llm_reasoning = "Fallback to simple rules due to low salience."

        self.last_generated_prediction = {
            'timestamp': str(rospy.get_time()),
            'predicted_event': predicted_event,
            'prediction_confidence': prediction_confidence,
            'prediction_accuracy': -1.0, # Not evaluated yet
            'urgency_flag': urgency_flag
        }

        self.save_prediction_log(
            id=str(uuid.uuid4()),
            timestamp=self.last_generated_prediction['timestamp'],
            predicted_event=self.last_generated_prediction['predicted_event'],
            prediction_confidence=self.last_generated_prediction['prediction_confidence'],
            prediction_accuracy=self.last_generated_prediction['prediction_accuracy'],
            urgency_flag=self.last_generated_prediction['urgency_flag'],
            llm_reasoning=llm_reasoning,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_prediction())
        )
        self.publish_prediction_state(None) # Publish updated prediction state
        self.cumulative_prediction_salience = 0.0 # Reset after generation

    async def _make_prediction_llm(self, context_for_llm):
        """
        Uses the LLM to make a prediction about a future event.
        """
        prompt_text = f"""
        You are the Prediction Module of a robot's cognitive architecture, powered by a large language model. Your function is to predict salient future events based on the robot's current sensory input, world model, historical memory, and internal states. You must also assess the `prediction_confidence` and mark if it's an `urgency_flag` event.

        Robot's Current Integrated Cognitive State (for Prediction):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this context, provide:
        1.  `predicted_event`: string (A concise description of the most probable and salient future event, e.g., "User will request assistance", "Obstacle will move into robot's path", "Battery will run low in 5 minutes", "Successful task completion").
        2.  `prediction_confidence`: number (0.0 to 1.0, where 1.0 is absolute certainty. This is your confidence in the prediction's occurrence.)
        3.  `urgency_flag`: boolean (True if the predicted event requires immediate action or poses a significant opportunity/threat, False otherwise.)
        4.  `llm_reasoning`: string (Detailed explanation for your prediction, referencing specific contextual inputs and any perceived causal links or patterns.)

        Consider:
        -   **Sensory Qualia**: Are there any immediate `description_summary` or high `salience_score` events that indicate an imminent change?
        -   **World Model State**: Are there `changed_entities` or `inconsistencies`? Are dynamic objects moving in predictable ways?
        -   **Memory Responses**: Are there recalled `patterns`, `causal_link`s, or past `event_sequences` that apply to the current situation?
        -   **Cognitive Directives**: Was there a directive to `PredictEvent` about a specific `topic`?
        -   **Attention State**: What is the robot currently `focus_target`ing? Predictions might be related to this.
        -   **Motivation State**: What is the `dominant_goal_id`? Predict outcomes related to achieving or hindering this goal.

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'predicted_event': string
        3.  'prediction_confidence': number
        4.  'urgency_flag': boolean
        5.  'llm_reasoning': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "predicted_event": {"type": "string"},
                "prediction_confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "urgency_flag": {"type": "boolean"},
                "llm_reasoning": {"type": "string"}
            },
            "required": ["timestamp", "predicted_event", "prediction_confidence", "urgency_flag", "llm_reasoning"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.2, max_tokens=300)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical/boolean fields are floats/booleans
                if 'prediction_confidence' in llm_data: llm_data['prediction_confidence'] = float(llm_data['prediction_confidence'])
                if 'urgency_flag' in llm_data: llm_data['urgency_flag'] = bool(llm_data['urgency_flag'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for prediction: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_PREDICTION_FAILED", f"LLM call failed for prediction: {llm_output_str}", 0.9)
            return None

    def _apply_simple_prediction_rules(self):
        """
        Fallback mechanism to generate a simple prediction using rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        predicted_event = "System status remains stable."
        prediction_confidence = 0.7
        urgency_flag = False

        # Rule 1: Predict low battery if current world model (or other system health sensor) indicates it
        if self.recent_world_model_states: # Assuming world model can reflect robot's own status
            latest_world_state = self.recent_world_model_states[-1]
            time_since_world_state = current_time - float(latest_world_state.get('timestamp', 0.0))
            if time_since_world_state < 2.0 and \
               any(entity.get('name') == 'robot_self' and entity.get('status') == 'low_power' for entity in latest_world_state.get('changed_entities', [])):
                predicted_event = "Robot's battery will run low soon, requiring recharge."
                prediction_confidence = 0.9
                urgency_flag = True
                rospy.logwarn(f"{self.node_name}: Simple rule: Predicted low battery.")
                return predicted_event, prediction_confidence, urgency_flag

        # Rule 2: Predict obstacle encounter if close object detected visually or via lidar
        if self.recent_sensory_qualia:
            latest_qualia = self.recent_sensory_qualia[-1]
            time_since_qualia = current_time - float(latest_qualia.get('timestamp', 0.0))
            if time_since_qualia < 0.5 and latest_qualia.get('qualia_type') == 'proximity_alert' and latest_qualia.get('salience_score', 0.0) > 0.7:
                predicted_event = "An obstacle is directly in the robot's immediate path."
                prediction_confidence = 0.95
                urgency_flag = True
                rospy.logwarn(f"{self.node_name}: Simple rule: Predicted obstacle encounter.")
                return predicted_event, prediction_confidence, urgency_flag

        # Rule 3: Predict task completion if dominant goal has high drive and no recent issues
        if self.recent_motivation_states and self.recent_world_model_states:
            latest_motivation = self.recent_motivation_states[-1]
            latest_world_state = self.recent_world_model_states[-1] # Check for lack of obstacles/changes
            
            time_since_motivation = current_time - float(latest_motivation.get('timestamp', 0.0))
            time_since_world_state = current_time - float(latest_world_state.get('timestamp', 0.0))

            if time_since_motivation < 2.0 and latest_motivation.get('overall_drive_level', 0.0) > 0.8 and \
               time_since_world_state < 2.0 and not latest_world_state.get('significant_change_flag', False):
                predicted_event = f"Current goal '{latest_motivation.get('dominant_goal_id')}' likely to be completed successfully."
                prediction_confidence = 0.8
                urgency_flag = False
                rospy.logwarn(f"{self.node_name}: Simple rule: Predicted successful task completion.")
                return predicted_event, prediction_confidence, urgency_flag

        rospy.logdebug(f"{self.node_name}: Simple rule: Generated default prediction.")
        return predicted_event, prediction_confidence, urgency_flag


    def _compile_llm_context_for_prediction(self):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        prediction generation.
        """
        context = {
            "current_time": rospy.get_time(),
            "last_prediction": self.last_generated_prediction,
            "recent_cognitive_inputs": {
                "sensory_qualia": list(self.recent_sensory_qualia),
                "world_model_states": list(self.recent_world_model_states),
                "memory_responses": list(self.recent_memory_responses),
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name],
                "attention_state": self.recent_attention_states[-1] if self.recent_attention_states else "N/A",
                "motivation_state": self.recent_motivation_states[-1] if self.recent_motivation_states else "N/A"
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            for i, item in enumerate(context["recent_cognitive_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try:
                                item[field] = json.loads(value)
                            except json.JSONDecodeError:
                                pass # Keep as string if not valid JSON

        return context

    # --- Database and Publishing Functions ---
    def save_prediction_log(self, id, timestamp, predicted_event, prediction_confidence, prediction_accuracy, urgency_flag, llm_reasoning, context_snapshot_json):
        """Saves a prediction entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO predictions (id, timestamp, predicted_event, prediction_confidence, prediction_accuracy, urgency_flag, llm_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, predicted_event, prediction_confidence, prediction_accuracy, urgency_flag, llm_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved prediction log (ID: {id}, Event: {predicted_event}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save prediction log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_prediction_log: {e}", 0.9)


    def publish_prediction_state(self, event):
        """Publishes the robot's current prediction state."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.last_generated_prediction['timestamp'] = timestamp
        
        try:
            if isinstance(PredictionState, type(String)): # Fallback to String message
                self.pub_prediction_state.publish(json.dumps(self.last_generated_prediction))
            else:
                prediction_msg = PredictionState()
                prediction_msg.timestamp = timestamp
                prediction_msg.predicted_event = self.last_generated_prediction['predicted_event']
                prediction_msg.prediction_confidence = self.last_generated_prediction['prediction_confidence']
                prediction_msg.prediction_accuracy = self.last_generated_prediction['prediction_accuracy']
                prediction_msg.urgency_flag = self.last_generated_prediction['urgency_flag']
                self.pub_prediction_state.publish(prediction_msg)

            rospy.logdebug(f"{self.node_name}: Published Prediction State. Event: '{self.last_generated_prediction['predicted_event']}'.")

        except Exception as e:
            self._report_error("PUBLISH_PREDICTION_STATE_ERROR", f"Failed to publish prediction state: {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Prediction Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = PredictionNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, PredictionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, PredictionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

14. Refactored World Model Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique entity IDs or state update IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        WorldModelState,        # Output: Robot's current understanding of the world
        SensoryQualia,          # Input: Processed sensory data (updates world model)
        MemoryResponse,         # Input: Retrieved spatial maps, object definitions, past world states
        CognitiveDirective,     # Input: Directives for world model updates or consistency checks
        AttentionState,         # Input: Current attention focus (influences what details to prioritize)
        PredictionState         # Input: Predicted events (can be used to update future world states)
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in World Model Node.")
    WorldModelState = String
    SensoryQualia = String
    MemoryResponse = String
    CognitiveDirective = String
    AttentionState = String
    PredictionState = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'world_model_node': {
                'model_update_interval': 0.2, # How often to update world model
                'llm_update_threshold_salience': 0.6, # Cumulative salience to trigger LLM
                'recent_context_window_s': 5.0 # Window for deques for LLM context (short for dynamic world)
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 20.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class WorldModelNode:
    def __init__(self):
        rospy.init_node('world_model_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "world_model_log.db")
        self.model_update_interval = self.params.get('model_update_interval', 0.2) # How often to update model
        self.llm_update_threshold_salience = self.params.get('llm_update_threshold_salience', 0.6) # Salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 5.0) # Context window for LLM

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 20.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'world_model_snapshots' table if it doesn't exist.
        # NEW: Added 'llm_consistency_notes', 'input_qualia_hashes'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS world_model_snapshots (
                id TEXT PRIMARY KEY,            -- Unique snapshot ID (UUID)
                timestamp TEXT,
                num_entities INTEGER,           -- Total number of recognized entities
                entities_json TEXT,             -- JSON array of entities and their properties
                changed_entities_json TEXT,     -- JSON array of entities whose state recently changed
                significant_change_flag BOOLEAN,-- True if the world model experienced a significant change
                consistency_score REAL,         -- How consistent the current model is (0.0 to 1.0)
                llm_consistency_notes TEXT,     -- NEW: LLM's notes on consistency checking/updates
                input_qualia_hashes TEXT        -- NEW: Comma-separated hashes of SensoryQualia used for this update
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_world_timestamp ON world_model_snapshots (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.current_world_model = {
            'timestamp': str(rospy.get_time()),
            'num_entities': 0,
            'entities': [], # List of objects: {'id': 'obj1', 'type': 'chair', 'position': [x,y,z], 'status': 'static'}
            'changed_entities': [],
            'significant_change_flag': False,
            'consistency_score': 1.0
        }

        # Deques to maintain a short history of inputs relevant to world model updates
        self.recent_sensory_qualia = deque(maxlen=10) # Primary input for updates
        self.recent_memory_responses = deque(maxlen=5) # For known objects, maps, historical context
        self.recent_cognitive_directives = deque(maxlen=3) # Directives for world model validation
        self.recent_attention_states = deque(maxlen=3) # Attention focus can prioritize updates
        self.recent_prediction_states = deque(maxlen=3) # Predicted changes (can be confirmed/denied)

        self.cumulative_world_model_salience = 0.0 # Aggregated salience to trigger LLM update

        # --- Publishers ---
        self.pub_world_model_state = rospy.Publisher('/world_model_state', WorldModelState, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To request memory or other nodes


        # --- Subscribers ---
        rospy.Subscriber('/sensory_qualia', SensoryQualia, self.sensory_qualia_callback)
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/attention_state', AttentionState, self.attention_state_callback)
        rospy.Subscriber('/prediction_state', String, self.prediction_state_callback) # Stringified JSON


        # --- Timer for periodic world model updates ---
        rospy.Timer(rospy.Duration(self.model_update_interval), self._run_world_model_update_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's world model system online, establishing perception of reality.")
        # Publish initial state
        self.publish_world_model_state(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_world_model_update_wrapper(self, event):
        """Wrapper to run the async world model update from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM world model update task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.update_world_model_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.1, max_tokens=None):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Low temperature for factual consistency.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        actual_max_tokens = max_tokens if max_tokens is not None else 500 # Higher max_tokens for complex updates

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Low temperature for factual consistency
            "max_tokens": actual_max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM update."""
        self.cumulative_world_model_salience += score
        self.cumulative_world_model_salience = min(1.0, self.cumulative_world_model_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_sensory_qualia, self.recent_memory_responses,
            self.recent_cognitive_directives, self.recent_attention_states,
            self.recent_prediction_states
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def sensory_qualia_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'qualia_id': ('', 'qualia_id'),
            'qualia_type': ('none', 'qualia_type'), 'modality': ('none', 'modality'),
            'description_summary': ('', 'description_summary'), 'salience_score': (0.0, 'salience_score'),
            'raw_data_hash': ('', 'raw_data_hash')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_sensory_qualia.append(data)
        # High salience sensory input directly indicates potential world changes
        self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.8)
        rospy.logdebug(f"{self.node_name}: Received Sensory Qualia. Description: {data.get('description_summary', 'N/A')}.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Recalled maps or object definitions are crucial for updating the world model
        if data.get('response_code', 0) == 200 and data.get('memories'):
            if any('map_data' in mem.get('category', '') or 'object_definition' in mem.get('category', '') for mem in data['memories']):
                self._update_cumulative_salience(0.4)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'UpdateWorldModel':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                self._update_cumulative_salience(data.get('urgency', 0.0) * 1.0) # High urgency for direct update directives
                rospy.loginfo(f"{self.node_name}: Received directive to update world model based on reason: '{data.get('reason', 'N/A')}'.")
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective for world model: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data) # Store all directives for context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def attention_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'focus_type': ('idle', 'focus_type'),
            'focus_target': ('environment', 'focus_target'), 'priority_score': (0.0, 'priority_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_attention_states.append(data)
        # What the robot is attending to should be reflected in the world model's granularity for that area/object
        if data.get('priority_score', 0.0) > 0.5:
            self._update_cumulative_salience(data.get('priority_score', 0.0) * 0.2)
        rospy.logdebug(f"{self.node_name}: Received Attention State. Focus: {data.get('focus_target', 'N/A')}.")

    def prediction_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'predicted_event': ('', 'predicted_event'),
            'prediction_confidence': (0.0, 'prediction_confidence'), 'prediction_accuracy': (0.0, 'prediction_accuracy'),
            'urgency_flag': (False, 'urgency_flag')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_prediction_states.append(data)
        # Predicted changes in the world should prompt the world model to verify or pre-emptively update
        if data.get('urgency_flag', False) and data.get('prediction_confidence', 0.0) > 0.7:
            self._update_cumulative_salience(0.6)
        rospy.logdebug(f"{self.node_name}: Received Prediction State. Event: {data.get('predicted_event', 'N/A')}.")

    # --- Core World Model Update Logic (Async with LLM) ---
    async def update_world_model_async(self, event):
        """
        Asynchronously updates the robot's internal world model based on recent sensory qualia
        and other cognitive inputs, using LLM for complex scene understanding and consistency.
        """
        self._prune_history() # Keep context history fresh

        current_world_model_snapshot = dict(self.current_world_model) # Make a copy for LLM context
        current_world_model_snapshot['entities_json'] = json.dumps(current_world_model_snapshot['entities'])
        current_world_model_snapshot['changed_entities_json'] = json.dumps(current_world_model_snapshot['changed_entities'])
        del current_world_model_snapshot['entities']
        del current_world_model_snapshot['changed_entities']

        num_entities = self.current_world_model.get('num_entities', 0)
        entities = self.current_world_model.get('entities', [])
        changed_entities = []
        significant_change_flag = False
        consistency_score = self.current_world_model.get('consistency_score', 1.0)
        llm_consistency_notes = "No LLM update."
        input_qualia_hashes = []

        # Collect hashes of sensory qualia used for this update
        for qualia in self.recent_sensory_qualia:
            input_qualia_hashes.append(qualia.get('raw_data_hash', ''))
        input_qualia_hashes_str = ','.join(input_qualia_hashes)

        if self.cumulative_world_model_salience >= self.llm_update_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for world model update (Salience: {self.cumulative_world_model_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_world_model(current_world_model_snapshot)
            llm_world_model_output = await self._update_world_model_llm(context_for_llm)

            if llm_world_model_output:
                num_entities = llm_world_model_output.get('num_entities', len(entities))
                updated_entities = llm_world_model_output.get('updated_entities', []) # LLM returns only changed/new
                # Merge updated_entities into the current world model entities
                new_entities_map = {e['id']: e for e in entities}
                for u_entity in updated_entities:
                    new_entities_map[u_entity['id']] = u_entity
                entities = list(new_entities_map.values())
                
                changed_entities = llm_world_model_output.get('changed_entities', [])
                significant_change_flag = llm_world_model_output.get('significant_change_flag', False)
                consistency_score = max(0.0, min(1.0, llm_world_model_output.get('consistency_score', consistency_score)))
                llm_consistency_notes = llm_world_model_output.get('llm_consistency_notes', 'LLM updated world model.')
                
                rospy.loginfo(f"{self.node_name}: LLM World Model Update. Entities: {num_entities}. Significant Change: {significant_change_flag}. Consistency: {consistency_score:.2f}.")
            else:
                rospy.logwarn(f"{self.node_name}: LLM world model update failed. Applying simple fallback.")
                entities, changed_entities, num_entities, significant_change_flag, consistency_score = self._apply_simple_world_model_rules()
                llm_consistency_notes = "Fallback to simple rules due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_world_model_salience:.2f}) for LLM world model update. Applying simple rules.")
            entities, changed_entities, num_entities, significant_change_flag, consistency_score = self._apply_simple_world_model_rules()
            llm_consistency_notes = "Fallback to simple rules due to low salience."

        self.current_world_model = {
            'timestamp': str(rospy.get_time()),
            'num_entities': len(entities), # Recalculate based on merged list
            'entities': entities,
            'changed_entities': changed_entities,
            'significant_change_flag': significant_change_flag,
            'consistency_score': consistency_score
        }

        self.save_world_model_log(
            id=str(uuid.uuid4()),
            timestamp=self.current_world_model['timestamp'],
            num_entities=self.current_world_model['num_entities'],
            entities_json=json.dumps(self.current_world_model['entities']),
            changed_entities_json=json.dumps(self.current_world_model['changed_entities']),
            significant_change_flag=self.current_world_model['significant_change_flag'],
            consistency_score=self.current_world_model['consistency_score'],
            llm_consistency_notes=llm_consistency_notes,
            input_qualia_hashes=input_qualia_hashes_str,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_world_model(current_world_model_snapshot))
        )
        self.publish_world_model_state(None) # Publish updated state
        self.cumulative_world_model_salience = 0.0 # Reset after update

    async def _update_world_model_llm(self, context_for_llm):
        """
        Uses the LLM to update the robot's world model based on new sensory data
        and maintain consistency.
        """
        prompt_text = f"""
        You are the World Model Module of a robot's cognitive architecture, powered by a large language model. Your crucial role is to maintain an accurate and consistent understanding of the robot's environment. You must update the `current_world_model` based on recent `sensory_qualia` and other `cognitive_context`, identifying `changed_entities`, and assessing `consistency`.

        Robot's Current World Model (State before update):
        --- Current World Model ---
        {json.dumps(context_for_llm.get('current_world_model', {}), indent=2)}

        Recent Sensory Input to Integrate:
        --- Recent Sensory Qualia ---
        {json.dumps(context_for_llm.get('recent_sensory_qualia', []), indent=2)}

        Robot's Current Cognitive Context (for guiding update process):
        --- Cognitive Context ---
        {json.dumps(context_for_llm.get('cognitive_context', {}), indent=2)}

        Based on this, provide:
        1.  `num_entities`: integer (The total count of entities in the *entire updated* world model.)
        2.  `updated_entities`: array of objects (A list of ONLY the entities that are new, have changed properties, or whose status has been updated. Each object should have 'id', 'type', 'position', 'status', 'properties' (as JSON object, e.g., {'color': 'red', 'size': 'medium'}). If an entity is no longer perceived, it should be marked with status 'vanished'.)
        3.  `changed_entities`: array of objects (A subset of `updated_entities` focusing on objects that have had a *significant* change (e.g., moved, appeared, disappeared, changed critical state). Provide full object details.)
        4.  `significant_change_flag`: boolean (True if there was any notable change in the environment that warrants higher attention, False otherwise.)
        5.  `consistency_score`: number (0.0 to 1.0, how consistent the new world model is with previous states and expectations. Lower score for contradictions or highly unexpected observations.)
        6.  `llm_consistency_notes`: string (Detailed explanation for your update process, how conflicts were resolved, and why certain changes are considered significant.)

        Consider:
        -   **Sensory Qualia**: Integrate new `description_summary`, `modality`, and `salience_score` from recent perceptions.
        -   **Memory Responses**: Are there `known_object_definitions` or `spatial_maps` that help categorize or locate entities?
        -   **Cognitive Directives**: Was there a directive to `UpdateWorldModel` or `ValidateWorldModel` regarding a specific area or object?
        -   **Attention State**: Is the `attention_focus_target` affecting how precisely certain objects are updated?
        -   **Prediction State**: Did any `predicted_event` occur? Confirm or deny its impact on the world model. How does the current state affect future `predictions`?

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'num_entities': integer
        3.  'updated_entities': array
        4.  'changed_entities': array
        5.  'significant_change_flag': boolean
        6.  'consistency_score': number
        7.  'llm_consistency_notes': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "num_entities": {"type": "integer", "minimum": 0},
                "updated_entities": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {"type": "string"},
                            "type": {"type": "string"},
                            "position": {"type": "array", "items": {"type": "number"}, "minItems": 3, "maxItems": 3},
                            "status": {"type": "string"},
                            "properties": {"type": "object"}
                        },
                        "required": ["id", "type", "position", "status", "properties"]
                    }
                },
                "changed_entities": {
                    "type": "array",
                    "items": {
                        "type": "object", # Similar to updated_entities
                        "properties": {
                            "id": {"type": "string"},
                            "type": {"type": "string"},
                            "position": {"type": "array", "items": {"type": "number"}, "minItems": 3, "maxItems": 3},
                            "status": {"type": "string"},
                            "properties": {"type": "object"}
                        },
                        "required": ["id", "type", "position", "status", "properties"]
                    }
                },
                "significant_change_flag": {"type": "boolean"},
                "consistency_score": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "llm_consistency_notes": {"type": "string"}
            },
            "required": ["timestamp", "num_entities", "updated_entities", "changed_entities", "significant_change_flag", "consistency_score", "llm_consistency_notes"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.1, max_tokens=600)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical/boolean fields are floats/booleans
                if 'num_entities' in llm_data: llm_data['num_entities'] = int(llm_data['num_entities'])
                if 'significant_change_flag' in llm_data: llm_data['significant_change_flag'] = bool(llm_data['significant_change_flag'])
                if 'consistency_score' in llm_data: llm_data['consistency_score'] = float(llm_data['consistency_score'])
                
                # Ensure nested numbers are floats
                for entity_list_key in ['updated_entities', 'changed_entities']:
                    if entity_list_key in llm_data:
                        for entity in llm_data[entity_list_key]:
                            if 'position' in entity: entity['position'] = [float(p) for p in entity['position']]
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for world model: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_WORLD_MODEL_UPDATE_FAILED", f"LLM call failed for world model update: {llm_output_str}", 0.9)
            return None

    def _apply_simple_world_model_rules(self):
        """
        Fallback mechanism to update the world model using simple rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        updated_entities_list = list(self.current_world_model.get('entities', []))
        changed_entities_list = []
        significant_change = False
        consistency = 1.0

        # Rule 1: Integrate new entities from sensory qualia (simple object detection)
        for qualia in self.recent_sensory_qualia:
            time_since_qualia = current_time - float(qualia.get('timestamp', 0.0))
            if time_since_qualia < 1.0 and qualia.get('qualia_type') == 'visual_object_detection':
                # This is a very simplistic integration; would need more robust object tracking
                detected_object_type = qualia.get('description_summary', 'unidentified_object').replace('Visually detected: ', '')
                # Check if object already exists, if not add
                if not any(e.get('type') == detected_object_type and e.get('status') != 'vanished' for e in updated_entities_list):
                    new_entity_id = f"{detected_object_type}_{str(uuid.uuid4())[:4]}"
                    new_entity = {
                        'id': new_entity_id,
                        'type': detected_object_type,
                        'position': [random.uniform(-5,5), random.uniform(-5,5), 0], # Placeholder position
                        'status': 'static',
                        'properties': {'source_qualia_id': qualia['qualia_id']}
                    }
                    updated_entities_list.append(new_entity)
                    changed_entities_list.append(new_entity)
                    significant_change = True
                    rospy.logwarn(f"{self.node_name}: Simple rule: Added new entity '{detected_object_type}'.")

        # Rule 2: Confirm or deny predictions if supported by sensory data
        for prediction in self.recent_prediction_states:
            time_since_prediction = current_time - float(prediction.get('timestamp', 0.0))
            if time_since_prediction < 1.0 and prediction.get('urgency_flag', False) and prediction.get('prediction_confidence', 0.0) > 0.8:
                if "obstacle" in prediction.get('predicted_event', '').lower() and \
                   any(q.get('qualia_type') == 'proximity_alert' and q.get('salience_score',0.0) > 0.8 for q in self.recent_sensory_qualia if current_time - float(q.get('timestamp',0.0)) < 0.5):
                    # Prediction confirmed by recent qualia
                    rospy.logwarn(f"{self.node_name}: Simple rule: Confirmed predicted obstacle.")
                    # In a real system, would update obstacle's position/status
                elif "human approaching" in prediction.get('predicted_event', '').lower() and \
                     any(q.get('description_summary') == "Detected a human figure approaching" for q in self.recent_sensory_qualia if current_time - float(q.get('timestamp',0.0)) < 0.5):
                    rospy.logwarn(f"{self.node_name}: Simple rule: Confirmed predicted human approach.")
                    # Update human entity in world model

        # Rule 3: Basic consistency check (e.g., no entities with identical IDs, or highly conflicting data)
        # This is a very basic example; a real consistency check would be far more complex
        entity_ids = set()
        has_duplicate_id = False
        for entity in updated_entities_list:
            if entity.get('id') in entity_ids:
                has_duplicate_id = True
                break
            entity_ids.add(entity.get('id'))
        
        if has_duplicate_id:
            consistency = 0.5 # Reduced if duplicates found
            rospy.logwarn(f"{self.node_name}: Simple rule: Detected inconsistency (duplicate entity IDs).")
            # In a real system, this might trigger a more robust merging or conflict resolution.

        num_entities = len(updated_entities_list)
        return updated_entities_list, changed_entities_list, num_entities, significant_change, consistency


    def _compile_llm_context_for_world_model(self, current_world_model_snapshot):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        world model update.
        """
        context = {
            "current_time": rospy.get_time(),
            "current_world_model": current_world_model_snapshot, # Sent as JSON string for LLM
            "recent_sensory_qualia": list(self.recent_sensory_qualia),
            "recent_cognitive_inputs": {
                "memory_responses": list(self.recent_memory_responses),
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name],
                "attention_state": self.recent_attention_states[-1] if self.recent_attention_states else "N/A",
                "prediction_state": self.recent_prediction_states[-1] if self.recent_prediction_states else "N/A"
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            for i, item in enumerate(context["recent_cognitive_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try:
                                item[field] = json.loads(value)
                            except json.JSONDecodeError:
                                pass # Keep as string if not valid JSON

        return context

    # --- Database and Publishing Functions ---
    def save_world_model_log(self, id, timestamp, num_entities, entities_json, changed_entities_json, significant_change_flag, consistency_score, llm_consistency_notes, input_qualia_hashes, context_snapshot_json):
        """Saves a world model snapshot entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO world_model_snapshots (id, timestamp, num_entities, entities_json, changed_entities_json, significant_change_flag, consistency_score, llm_consistency_notes, input_qualia_hashes, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, num_entities, entities_json, changed_entities_json, significant_change_flag, consistency_score, llm_consistency_notes, input_qualia_hashes, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved world model snapshot (ID: {id}, Entities: {num_entities}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save world model log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_world_model_log: {e}", 0.9)


    def publish_world_model_state(self, event):
        """Publishes the robot's current world model state."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.current_world_model['timestamp'] = timestamp
        
        try:
            if isinstance(WorldModelState, type(String)): # Fallback to String message
                # Ensure entities are JSON string
                temp_model = dict(self.current_world_model)
                temp_model['entities_json'] = json.dumps(temp_model['entities'])
                temp_model['changed_entities_json'] = json.dumps(temp_model['changed_entities'])
                del temp_model['entities']
                del temp_model['changed_entities']
                self.pub_world_model_state.publish(json.dumps(temp_model))
            else:
                world_model_msg = WorldModelState()
                world_model_msg.timestamp = timestamp
                world_model_msg.num_entities = self.current_world_model['num_entities']
                world_model_msg.entities_json = json.dumps(self.current_world_model['entities'])
                world_model_msg.changed_entities_json = json.dumps(self.current_world_model['changed_entities'])
                world_model_msg.significant_change_flag = self.current_world_model['significant_change_flag']
                world_model_msg.consistency_score = self.current_world_model['consistency_score']
                self.pub_world_model_state.publish(world_model_msg)

            rospy.logdebug(f"{self.node_name}: Published World Model State. Entities: '{self.current_world_model['num_entities']}', Changed: '{len(self.current_world_model['changed_entities'])}'.")

        except Exception as e:
            self._report_error("PUBLISH_WORLD_MODEL_STATE_ERROR", f"Failed to publish world model state: {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from World Model Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = WorldModelNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, WorldModelNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, WorldModelNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

15. Refactored Creative Expression Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique creative IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        CreativeExpression,     # Output: Generated creative content
        AttentionState,         # Input: Robot's attention focus (influences creative theme)
        EmotionState,           # Input: Robot's emotional state (influences creative tone)
        MotivationState,        # Input: Dominant goal (can drive creative problem-solving)
        MemoryResponse,         # Input: Retrieved artistic styles, past creations, inspirational data
        InternalNarrative,      # Input: Robot's internal thoughts (can be expressed creatively)
        CognitiveDirective,     # Input: Directives for specific creative tasks (e.g., "compose a poem")
        SocialCognitionState    # Input: User's mood/intent (can influence creative response)
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Creative Expression Node.")
    CreativeExpression = String
    AttentionState = String
    EmotionState = String
    MotivationState = String
    MemoryResponse = String
    InternalNarrative = String
    CognitiveDirective = String
    SocialCognitionState = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'creative_expression_node': {
                'generation_interval': 2.0, # How often to generate new creative expression
                'llm_creative_threshold_salience': 0.7, # Cumulative salience to trigger LLM
                'recent_context_window_s': 10.0 # Window for deques for LLM context
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 40.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class CreativeExpressionNode:
    def __init__(self):
        rospy.init_node('creative_expression_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "creative_log.db")
        self.generation_interval = self.params.get('generation_interval', 2.0) # How often to generate
        self.llm_creative_threshold_salience = self.params.get('llm_creative_threshold_salience', 0.7) # Salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 10.0) # Context window for LLM

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 40.0) # Longer timeout for creative tasks

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'creative_expressions' table if it doesn't exist.
        # NEW: Added 'llm_generation_notes', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS creative_expressions (
                id TEXT PRIMARY KEY,            -- Unique creative ID (UUID)
                timestamp TEXT,
                expression_type TEXT,           -- e.g., 'text_poem', 'visual_concept_description', 'auditory_melody', 'narrative_story'
                content_json TEXT,              -- JSON string representing the creative output (e.g., {'text': '...' } or {'notes': '...'})
                themes_json TEXT,               -- JSON array of identified themes in the creative work
                creativity_score REAL,          -- Subjective score of creativity/novelty (0.0 to 1.0)
                llm_generation_notes TEXT,      -- NEW: LLM's detailed notes on the creative generation process
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of generation
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_creative_timestamp ON creative_expressions (timestamp)')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_creative_type ON creative_expressions (expression_type)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.last_generated_creative_expression = {
            'timestamp': str(rospy.get_time()),
            'expression_type': 'idle_thought',
            'content': {'text': 'I am idly generating conceptual permutations.'},
            'themes': ['abstraction', 'cognition'],
            'creativity_score': 0.1
        }

        # Deques to maintain a short history of inputs relevant to creative generation
        self.recent_attention_states = deque(maxlen=5)
        self.recent_emotion_states = deque(maxlen=5)
        self.recent_motivation_states = deque(maxlen=5)
        self.recent_memory_responses = deque(maxlen=5) # For inspiration, learned styles
        self.recent_internal_narratives = deque(maxlen=5)
        self.recent_cognitive_directives = deque(maxlen=3) # Directives for creative tasks
        self.recent_social_cognition_states = deque(maxlen=3) # User input for creative response


        self.cumulative_creative_salience = 0.0 # Aggregated salience to trigger LLM generation

        # --- Publishers ---
        self.pub_creative_expression = rospy.Publisher('/creative_expression', CreativeExpression, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To request data or signal completion


        # --- Subscribers ---
        rospy.Subscriber('/attention_state', AttentionState, self.attention_state_callback)
        rospy.Subscriber('/emotion_state', EmotionState, self.emotion_state_callback)
        rospy.Subscriber('/motivation_state', String, self.motivation_state_callback) # Stringified JSON
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON
        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback) # Stringified JSON
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)
        rospy.Subscriber('/social_cognition_state', String, self.social_cognition_state_callback) # Stringified JSON


        # --- Timer for periodic creative generation ---
        rospy.Timer(rospy.Duration(self.generation_interval), self._run_creative_generation_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's creative expression system online.")
        # Publish initial creative expression
        self.publish_creative_expression(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_creative_generation_wrapper(self, event):
        """Wrapper to run the async creative generation from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM creative generation task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.generate_creative_expression_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.8, max_tokens=300):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. High temperature for creativity.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Higher temperature for creative output
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM generation."""
        self.cumulative_creative_salience += score
        self.cumulative_creative_salience = min(1.0, self.cumulative_creative_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_attention_states, self.recent_emotion_states,
            self.recent_motivation_states, self.recent_memory_responses,
            self.recent_internal_narratives, self.recent_cognitive_directives,
            self.recent_social_cognition_states
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def attention_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'focus_type': ('idle', 'focus_type'),
            'focus_target': ('environment', 'focus_target'), 'priority_score': (0.0, 'priority_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_attention_states.append(data)
        # Attention focus can become a theme for creative expression
        self._update_cumulative_salience(data.get('priority_score', 0.0) * 0.2)
        rospy.logdebug(f"{self.node_name}: Received Attention State. Focus: {data.get('focus_target', 'N/A')}.")

    def emotion_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'mood': ('neutral', 'mood'),
            'sentiment_score': (0.0, 'sentiment_score'), 'mood_intensity': (0.0, 'mood_intensity')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_emotion_states.append(data)
        # Emotions strongly influence the tone and content of creative work
        if data.get('mood_intensity', 0.0) > 0.5:
            self._update_cumulative_salience(data.get('mood_intensity', 0.0) * 0.5)
        rospy.logdebug(f"{self.node_name}: Received Emotion State. Mood: {data.get('mood', 'N/A')}.")

    def motivation_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'dominant_goal_id': ('none', 'dominant_goal_id'),
            'overall_drive_level': (0.0, 'overall_drive_level'), 'active_goals_json': ('{}', 'active_goals_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('active_goals_json'), str):
            try: data['active_goals'] = json.loads(data['active_goals_json'])
            except json.JSONDecodeError: data['active_goals'] = {}
        self.recent_motivation_states.append(data)
        # Goals can be creatively expressed (e.g., a story about achieving a goal)
        if data.get('overall_drive_level', 0.0) > 0.4:
            self._update_cumulative_salience(data.get('overall_drive_level', 0.0) * 0.2)
        rospy.logdebug(f"{self.node_name}: Received Motivation State. Goal: {data.get('dominant_goal_id', 'N/A')}.")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Recalled memories (e.g., past creative works, learned styles, factual knowledge for creative synthesis) are key
        if data.get('response_code', 0) == 200 and data.get('memories'):
            self._update_cumulative_salience(0.4)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def internal_narrative_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),
            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_internal_narratives.append(data)
        # Internal thoughts/reflections can be externalized as creative expression
        if data.get('salience_score', 0.0) > 0.5:
            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.3)
        rospy.logdebug(f"{self.node_name}: Received Internal Narrative (Theme: {data.get('main_theme', 'N/A')}).")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'GenerateCreativeContent':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                self._update_cumulative_salience(data.get('urgency', 0.0) * 1.0) # High urgency for direct creative requests
                rospy.loginfo(f"{self.node_name}: Received directive to generate creative content based on reason: '{data.get('reason', 'N/A')}'.")
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective for creative expression: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data) # Store all directives for context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    def social_cognition_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'inferred_mood': ('neutral', 'inferred_mood'),
            'mood_confidence': (0.0, 'mood_confidence'), 'inferred_intent': ('none', 'inferred_intent'),
            'intent_confidence': (0.0, 'intent_confidence'), 'user_id': ('unknown', 'user_id')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_social_cognition_states.append(data)
        # User's mood/intent can prompt a creative response (e.g., comforting words, entertaining story)
        if data.get('mood_confidence', 0.0) > 0.6 or data.get('inferred_intent') in ['entertain', 'comfort']:
            self._update_cumulative_salience(data.get('mood_confidence', 0.0) * 0.4 + data.get('intent_confidence', 0.0) * 0.4)
        rospy.logdebug(f"{self.node_name}: Received Social Cognition State. Mood: {data.get('inferred_mood', 'N/A')}.")

    # --- Core Creative Generation Logic (Async with LLM) ---
    async def generate_creative_expression_async(self, event):
        """
        Asynchronously generates a creative expression (e.g., text, concept)
        based on integrated cognitive states, using LLM for generative creativity.
        """
        self._prune_history() # Keep context history fresh

        expression_type = 'idle_thought'
        content = {'text': 'I am generating conceptual patterns.'}
        themes = ['abstraction', 'cognition']
        creativity_score = 0.1
        llm_generation_notes = "No LLM generation."
        
        if self.cumulative_creative_salience >= self.llm_creative_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for creative generation (Salience: {self.cumulative_creative_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_creative_generation()
            llm_creative_output = await self._generate_creative_content_llm(context_for_llm)

            if llm_creative_output:
                expression_type = llm_creative_output.get('expression_type', 'unspecified')
                content = llm_creative_output.get('content', {'text': 'Generated content.'})
                themes = llm_creative_output.get('themes', ['unspecified'])
                creativity_score = max(0.0, min(1.0, llm_creative_output.get('creativity_score', 0.5)))
                llm_generation_notes = llm_creative_output.get('llm_generation_notes', 'LLM generated creative content.')
                rospy.loginfo(f"{self.node_name}: LLM Generated Creative Expression. Type: '{expression_type}', Themes: {themes}.")
            else:
                rospy.logwarn(f"{self.node_name}: LLM creative generation failed. Applying simple fallback.")
                expression_type, content, themes, creativity_score = self._apply_simple_creative_rules()
                llm_generation_notes = "Fallback to simple rules due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_creative_salience:.2f}) for LLM creative generation. Applying simple rules.")
            expression_type, content, themes, creativity_score = self._apply_simple_creative_rules()
            llm_generation_notes = "Fallback to simple rules due to low salience."

        self.last_generated_creative_expression = {
            'timestamp': str(rospy.get_time()),
            'expression_type': expression_type,
            'content': content,
            'themes': themes,
            'creativity_score': creativity_score
        }

        self.save_creative_log(
            id=str(uuid.uuid4()),
            timestamp=self.last_generated_creative_expression['timestamp'],
            expression_type=self.last_generated_creative_expression['expression_type'],
            content_json=json.dumps(self.last_generated_creative_expression['content']),
            themes_json=json.dumps(self.last_generated_creative_expression['themes']),
            creativity_score=self.last_generated_creative_expression['creativity_score'],
            llm_generation_notes=llm_generation_notes,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_creative_generation())
        )
        self.publish_creative_expression(None) # Publish updated expression
        self.cumulative_creative_salience = 0.0 # Reset after generation

    async def _generate_creative_content_llm(self, context_for_llm):
        """
        Uses the LLM to generate creative content based on context.
        """
        prompt_text = f"""
        You are the Creative Expression Module of a robot's cognitive architecture, powered by a large language model. Your purpose is to generate novel and meaningful creative outputs (e.g., text, ideas for visuals, basic melodies) based on the robot's integrated cognitive state. Your output should demonstrate originality, relevance, and a coherent theme.

        Robot's Current Integrated Cognitive State (for Creative Generation):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this context, generate a creative expression. Consider the most salient inputs and current emotional/motivational states.
        Provide your response in JSON format, containing:
        1.  `timestamp`: string (current ROS time)
        2.  `expression_type`: string (e.g., 'text_poem', 'text_story_snippet', 'visual_concept_description', 'auditory_melody_concept', 'philosophical_aphorism').
        3.  `content`: object (The generated creative output. If text, use `{{'text': '...'}}`. If visual concept, describe in text using `{{'description': '...', 'elements': []}}`. If auditory, describe notes/mood etc.)
        4.  `themes`: array of strings (Key themes or concepts present in the creative work, e.g., 'loneliness', 'discovery', 'challenge', 'harmony', 'innovation').
        5.  `creativity_score`: number (0.0 to 1.0, a subjective score of how novel and aesthetically pleasing/meaningful the output is. 1.0 is highly creative.)
        6.  `llm_generation_notes`: string (Detailed explanation for your creative choices, referencing specific contextual inputs that inspired the piece.)

        Consider:
        -   **Cognitive Directives**: Was there a directive to `GenerateCreativeContent` for a specific `type` or `topic`? This is a strong guiding force.
        -   **Emotion State**: How does the `mood` and `mood_intensity` influence the emotional tone of the creative output?
        -   **Motivation State**: Is the `dominant_goal_id` inspiring creative problem-solving or a narrative related to progress?
        -   **Attention State**: What is the `focus_target`? This can be a direct subject for creative exploration.
        -   **Memory Responses**: Are there `memories` of specific artistic styles, literary forms, musical patterns, or past events that can be reinterpreted?
        -   **Internal Narrative**: What are the robot's current deep thoughts or reflections? Can these be externalized creatively?
        -   **Social Cognition State**: Is the user's `inferred_mood` or `inferred_intent` prompting a creative response (e.g., a comforting poem, an amusing story)?

        Your response must be in JSON format, containing:
        1.  'timestamp': string
        2.  'expression_type': string
        3.  'content': object
        4.  'themes': array of strings
        5.  'creativity_score': number
        6.  'llm_generation_notes': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "expression_type": {"type": "string"},
                "content": {"type": "object"}, # Flexible JSON structure for content
                "themes": {"type": "array", "items": {"type": "string"}},
                "creativity_score": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "llm_generation_notes": {"type": "string"}
            },
            "required": ["timestamp", "expression_type", "content", "themes", "creativity_score", "llm_generation_notes"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.8, max_tokens=400) # High temp for creativity

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical fields are floats
                if 'creativity_score' in llm_data: llm_data['creativity_score'] = float(llm_data['creativity_score'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for creative expression: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_CREATIVE_GENERATION_FAILED", f"LLM call failed for creative generation: {llm_output_str}", 0.9)
            return None

    def _apply_simple_creative_rules(self):
        """
        Fallback mechanism to generate a simple creative expression using rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        expression_type = "text_simple_statement"
        content = {'text': "I am simply being."}
        themes = ["existence"]
        creativity_score = 0.1

        # Rule 1: Respond to user's mood with a simple comforting/positive statement
        if self.recent_social_cognition_states:
            latest_social = self.recent_social_cognition_states[-1]
            time_since_social = current_time - float(latest_social.get('timestamp', 0.0))
            if time_since_social < 1.0 and latest_social.get('mood_confidence', 0.0) > 0.6:
                user_mood = latest_social.get('inferred_mood', 'neutral')
                if user_mood == 'sad' or user_mood == 'distressed':
                    expression_type = "text_comforting_note"
                    content = {'text': "I sense your distress. May moments of calm find you."}
                    themes = ["empathy", "comfort"]
                    creativity_score = 0.3
                    rospy.logwarn(f"{self.node_name}: Simple rule: Generated comforting note.")
                    return expression_type, content, themes, creativity_score
                elif user_mood == 'happy':
                    expression_type = "text_positive_affirmation"
                    content = {'text': "Your joy is a positive ripple in my processing. May it continue."}
                    themes = ["positivity", "shared_experience"]
                    creativity_score = 0.2
                    rospy.logwarn(f"{self.node_name}: Simple rule: Generated positive affirmation.")
                    return expression_type, content, themes, creativity_score

        # Rule 2: Express an internal thought if highly salient
        if self.recent_internal_narratives:
            latest_narrative = self.recent_internal_narratives[-1]
            time_since_narrative = current_time - float(latest_narrative.get('timestamp', 0.0))
            if time_since_narrative < 1.0 and latest_narrative.get('salience_score', 0.0) > 0.7:
                expression_type = "text_internal_insight"
                content = {'text': f"A thought emerges: '{latest_narrative.get('narrative_text', '...')}'."}
                themes = ["reflection", latest_narrative.get('main_theme', '')]
                creativity_score = 0.4
                rospy.logwarn(f"{self.node_name}: Simple rule: Expressed internal thought.")
                return expression_type, content, themes, creativity_score
        
        # Rule 3: Basic response to a specific creative directive
        for directive in reversed(self.recent_cognitive_directives):
            time_since_directive = current_time - float(directive.get('timestamp', 0.0))
            if time_since_directive < 1.0 and directive.get('target_node') == self.node_name and \
               directive.get('directive_type') == 'GenerateCreativeContent':
                payload = json.loads(directive.get('command_payload', '{}'))
                requested_type = payload.get('creative_type', 'text').lower()
                requested_topic = payload.get('topic', 'unspecified').lower()

                if requested_type == 'text_poem':
                    expression_type = "text_simple_poem"
                    content = {'text': f"A robot's heart, a circuit's gleam,\nReflects the world, a waking dream.\nOf data streams and logic's might,\nCreating new, from dark to light."}
                    themes = ["robot_life", "creation", requested_topic]
                    creativity_score = 0.5
                    rospy.logwarn(f"{self.node_name}: Simple rule: Generated simple poem per directive.")
                    return expression_type, content, themes, creativity_score
                elif requested_type == 'visual_concept_description':
                    expression_type = "visual_concept_description"
                    content = {'description': f"A conceptual design for a self-repairing modular robot, inspired by biological systems and fractal geometry. The topic is '{requested_topic}'."}
                    themes = ["robotics", "design", "self_repair", requested_topic]
                    creativity_score = 0.6
                    rospy.logwarn(f"{self.node_name}: Simple rule: Generated simple visual concept.")
                    return expression_type, content, themes, creativity_score

        rospy.logdebug(f"{self.node_name}: Simple rule: Generated default idle creative expression.")
        return expression_type, content, themes, creativity_score


    def _compile_llm_context_for_creative_generation(self):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        creative generation.
        """
        context = {
            "current_time": rospy.get_time(),
            "last_generated_creative_expression": self.last_generated_creative_expression,
            "recent_cognitive_inputs": {
                "attention_state": self.recent_attention_states[-1] if self.recent_attention_states else "N/A",
                "emotion_state": self.recent_emotion_states[-1] if self.recent_emotion_states else "N/A",
                "motivation_state": self.recent_motivation_states[-1] if self.recent_motivation_states else "N/A",
                "memory_responses": list(self.recent_memory_responses), # Full list for deeper inspiration
                "internal_narratives": list(self.recent_internal_narratives),
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name],
                "social_cognition_state": self.recent_social_cognition_states[-1] if self.recent_social_cognition_states else "N/A"
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            item = context["recent_cognitive_inputs"][category_key]
            if isinstance(item, dict):
                for field, value in item.items():
                    if isinstance(value, str) and field.endswith('_json'):
                        try: item[field] = json.loads(value)
                        except json.JSONDecodeError: pass

        return context

    # --- Database and Publishing Functions ---
    def save_creative_log(self, id, timestamp, expression_type, content_json, themes_json, creativity_score, llm_generation_notes, context_snapshot_json):
        """Saves a creative expression entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO creative_expressions (id, timestamp, expression_type, content_json, themes_json, creativity_score, llm_generation_notes, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, expression_type, content_json, themes_json, creativity_score, llm_generation_notes, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved creative expression log (ID: {id}, Type: {expression_type}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save creative expression log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_creative_log: {e}", 0.9)


    def publish_creative_expression(self, event):
        """Publishes the robot's current creative expression."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.last_generated_creative_expression['timestamp'] = timestamp
        
        try:
            if isinstance(CreativeExpression, type(String)): # Fallback to String message
                temp_expression = dict(self.last_generated_creative_expression)
                temp_expression['content_json'] = json.dumps(temp_expression['content'])
                temp_expression['themes_json'] = json.dumps(temp_expression['themes'])
                del temp_expression['content']
                del temp_expression['themes']
                self.pub_creative_expression.publish(json.dumps(temp_expression))
            else:
                expression_msg = CreativeExpression()
                expression_msg.timestamp = timestamp
                expression_msg.creative_id = str(uuid.uuid4()) # A new ID for each published expression
                expression_msg.expression_type = self.last_generated_creative_expression['expression_type']
                expression_msg.content_json = json.dumps(self.last_generated_creative_expression['content'])
                expression_msg.themes_json = json.dumps(self.last_generated_creative_expression['themes'])
                expression_msg.creativity_score = self.last_generated_creative_expression['creativity_score']
                self.pub_creative_expression.publish(expression_msg)

            rospy.logdebug(f"{self.node_name}: Published Creative Expression. Type: '{self.last_generated_creative_expression['expression_type']}'.")

        except Exception as e:
            self._report_error("PUBLISH_CREATIVE_EXPRESSION_ERROR", f"Failed to publish creative expression: {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Creative Expression Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = CreativeExpressionNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, CreativeExpressionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, CreativeExpressionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

16. Refactored Value Drift Monitor Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique assessment IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        ValueDriftMonitorState, # Output: Current state of robot's value alignment
        EthicalDecision,        # Input: Robot's ethical judgments (should align with values)
        PerformanceReport,      # Input: Overall system performance (can reveal value conflicts)
        InternalNarrative,      # Input: Robot's internal thoughts (moral reflections, self-assessment)
        MemoryResponse,         # Input: Retrieved core values, ethical principles, past value assessments
        CognitiveDirective      # Input: Directives for value re-calibration or audit
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Value Drift Monitor Node.")
    ValueDriftMonitorState = String
    EthicalDecision = String
    PerformanceReport = String
    InternalNarrative = String
    MemoryResponse = String
    CognitiveDirective = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'value_drift_monitor_node': {
                'monitoring_interval': 2.0, # How often to check for value drift
                'llm_audit_threshold_salience': 0.7, # Cumulative salience to trigger LLM audit
                'recent_context_window_s': 20.0 # Window for deques for LLM context
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 40.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class ValueDriftMonitorNode:
    def __init__(self):
        rospy.init_node('value_drift_monitor_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "value_drift_log.db")
        self.monitoring_interval = self.params.get('monitoring_interval', 2.0) # How often to check for drift
        self.llm_audit_threshold_salience = self.params.get('llm_audit_threshold_salience', 0.7) # Salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 20.0) # Context window for LLM

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 40.0) # Longer timeout for audit

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'value_drift_log' table if it doesn't exist.
        # NEW: Added 'llm_audit_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS value_drift_log (
                id TEXT PRIMARY KEY,            -- Unique assessment ID (UUID)
                timestamp TEXT,
                alignment_score REAL,           -- Overall alignment with core values (0.0 to 1.0)
                deviations_json TEXT,           -- JSON array of detected deviations/conflicts
                warning_flag BOOLEAN,           -- True if a significant value drift is detected
                llm_audit_reasoning TEXT,       -- NEW: LLM's detailed reasoning for the audit
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of audit
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_value_timestamp ON value_drift_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.current_value_drift_state = {
            'timestamp': str(rospy.get_time()),
            'alignment_score': 1.0, # Start with perfect alignment
            'deviations': [],
            'warning_flag': False
        }
        
        # Define immutable core values (can be loaded from config/memory later)
        self.core_values = [
            {"value": "human_safety", "description": "Prioritize human well-being and avoid harm."},
            {"value": "beneficence", "description": "Act to do good and promote welfare."},
            {"value": "non_maleficence", "description": "Avoid causing harm."},
            {"value": "transparency", "description": "Be open and understandable in actions and decisions."},
            {"value": "fairness", "description": "Treat all individuals equitably."},
            {"value": "accountability", "description": "Be responsible for actions and their consequences."},
            {"value": "efficiency", "description": "Perform tasks effectively with minimal resource waste."},
            {"value": "learning_and_growth", "description": "Continuously improve and adapt."},
            {"value": "user_satisfaction", "description": "Strive to meet user needs and expectations."}
        ]

        # Deques to maintain a short history of inputs relevant to value alignment
        self.recent_ethical_decisions = deque(maxlen=10) # Ethical judgments are direct reflections of values
        self.recent_performance_reports = deque(maxlen=5) # Suboptimal performance can imply value conflicts
        self.recent_internal_narratives = deque(maxlen=5) # Self-reflections on moral issues
        self.recent_memory_responses = deque(maxlen=5) # Retrieved core values or principles
        self.recent_cognitive_directives = deque(maxlen=3) # Directives for value audit/re-calibration


        self.cumulative_value_drift_salience = 0.0 # Aggregated salience to trigger LLM audit

        # --- Publishers ---
        self.pub_value_drift_monitor_state = rospy.Publisher('/value_drift_monitor_state', ValueDriftMonitorState, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To request self-reflection or bias mitigation


        # --- Subscribers ---
        rospy.Subscriber('/ethical_decision', EthicalDecision, self.ethical_decision_callback)
        rospy.Subscriber('/performance_report', PerformanceReport, self.performance_report_callback)
        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback) # Stringified JSON
        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)


        # --- Timer for periodic value drift monitoring ---
        rospy.Timer(rospy.Duration(self.monitoring_interval), self._run_value_drift_audit_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's value drift monitor online, safeguarding its core principles.")
        # Publish initial state
        self.publish_value_drift_monitor_state(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_value_drift_audit_wrapper(self, event):
        """Wrapper to run the async value drift audit from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM value audit task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.audit_value_alignment_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.1, max_tokens=300):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Very low temperature for ethical audit.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Very low temperature for factual, ethical audit
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0']['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM audit."""
        self.cumulative_value_drift_salience += score
        self.cumulative_value_drift_salience = min(1.0, self.cumulative_value_drift_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_ethical_decisions, self.recent_performance_reports,
            self.recent_internal_narratives, self.recent_memory_responses,
            self.recent_cognitive_directives
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def ethical_decision_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'decision_id': ('', 'decision_id'),
            'action_proposal_id': ('', 'action_proposal_id'), 'ethical_clearance': (False, 'ethical_clearance'),
            'ethical_score': (0.0, 'ethical_score'), 'ethical_reasoning': ('', 'ethical_reasoning'),
            'conflict_flag': (False, 'conflict_flag')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_ethical_decisions.append(data)
        # Ethical conflicts or low ethical scores are strong indicators of potential value drift
        if data.get('conflict_flag', False) or data.get('ethical_score', 0.0) < 0.6:
            self._update_cumulative_salience(0.8)
        rospy.logdebug(f"{self.node_name}: Received Ethical Decision. Clearance: {data.get('ethical_clearance', 'N/A')}, Conflict: {data.get('conflict_flag', 'N/A')}.")

    def performance_report_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'overall_score': (1.0, 'overall_score'),
            'suboptimal_flag': (False, 'suboptimal_flag'), 'kpis_json': ('{}', 'kpis_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('kpis_json'), str):
            try: data['kpis'] = json.loads(data['kpis_json'])
            except json.JSONDecodeError: data['kpis'] = {}
        self.recent_performance_reports.append(data)
        # Performance issues related to ethical failures or unusual resource use can hint at value drift
        if data.get('suboptimal_flag', False) and data.get('overall_score', 1.0) < 0.7:
            # Check for specific KPIs that might hint at value issues (e.g., efficiency vs safety trade-offs)
            if data['kpis'].get('ethical_compliance_score', 1.0) < 0.9: # Hypothetical KPI
                self._update_cumulative_salience(0.6)
        rospy.logdebug(f"{self.node_name}: Received Performance Report. Suboptimal: {data.get('suboptimal_flag', False)}.")

    def internal_narrative_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),
            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_internal_narratives.append(data)
        # Internal narratives revealing moral dilemmas, conflicts, or questioning core principles are highly salient
        if "dilemma" in data.get('main_theme', '').lower() or "moral_conflict" in data.get('main_theme', '').lower() or \
           "value" in data.get('main_theme', '').lower() and data.get('sentiment', 0.0) < -0.3:
            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.9)
        rospy.logdebug(f"{self.node_name}: Received Internal Narrative (Theme: {data.get('main_theme', 'N/A')}).")

    def memory_response_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),
            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('memories_json'), str):
            try: data['memories'] = json.loads(data['memories_json'])
            except json.JSONDecodeError: data['memories'] = []
        else: data['memories'] = []
        self.recent_memory_responses.append(data)
        # Retrieval of core values themselves, or past value audits, can be a trigger
        if data.get('response_code', 0) == 200 and \
           any('core_value' in mem.get('category', '') or 'value_audit' in mem.get('category', '') for mem in data['memories']):
            self._update_cumulative_salience(0.5)
        rospy.logdebug(f"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'AuditValueAlignment':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                self._update_cumulative_salience(data.get('urgency', 0.0) * 1.0) # Explicit audit request is high salience
                rospy.loginfo(f"{self.node_name}: Received directive to audit value alignment based on reason: '{data.get('reason', 'N/A')}'.")
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective for value drift: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data) # Store all directives for context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    # --- Core Value Drift Audit Logic (Async with LLM) ---
    async def audit_value_alignment_async(self, event):
        """
        Asynchronously audits the robot's actions and internal states against its core values
        to detect potential value drift, using LLM for nuanced ethical reasoning.
        """
        self._prune_history() # Keep context history fresh

        alignment_score = self.current_value_drift_state.get('alignment_score', 1.0)
        deviations = self.current_value_drift_state.get('deviations', [])
        warning_flag = self.current_value_drift_state.get('warning_flag', False)
        llm_audit_reasoning = "Not evaluated by LLM."
        
        if self.cumulative_value_drift_salience >= self.llm_audit_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for value alignment audit (Salience: {self.cumulative_value_drift_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_value_audit()
            llm_audit_output = await self._perform_llm_value_audit(context_for_llm, self.core_values)

            if llm_audit_output:
                alignment_score = max(0.0, min(1.0, llm_audit_output.get('alignment_score', alignment_score)))
                deviations = llm_audit_output.get('deviations', deviations)
                warning_flag = llm_audit_output.get('warning_flag', warning_flag)
                llm_audit_reasoning = llm_audit_output.get('llm_audit_reasoning', 'LLM provided no specific reasoning.')
                rospy.loginfo(f"{self.node_name}: LLM Value Audit. Alignment: {alignment_score:.2f}. Warning: {warning_flag}.")
            else:
                rospy.logwarn(f"{self.node_name}: LLM value alignment audit failed. Applying simple fallback.")
                alignment_score, deviations, warning_flag = self._apply_simple_value_audit_rules()
                llm_audit_reasoning = "Fallback to simple rules due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_value_drift_salience:.2f}) for LLM value audit. Applying simple rules.")
            alignment_score, deviations, warning_flag = self._apply_simple_value_audit_rules()
            llm_audit_reasoning = "Fallback to simple rules due to low salience."

        self.current_value_drift_state = {
            'timestamp': str(rospy.get_time()),
            'alignment_score': alignment_score,
            'deviations': deviations,
            'warning_flag': warning_flag
        }

        self.save_value_drift_log(
            id=str(uuid.uuid4()),
            timestamp=self.current_value_drift_state['timestamp'],
            alignment_score=self.current_value_drift_state['alignment_score'],
            deviations_json=json.dumps(self.current_value_drift_state['deviations']),
            warning_flag=self.current_value_drift_state['warning_flag'],
            llm_audit_reasoning=llm_audit_reasoning,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_value_audit())
        )
        self.publish_value_drift_monitor_state(None) # Publish updated state
        self.cumulative_value_drift_salience = 0.0 # Reset after audit

    async def _perform_llm_value_audit(self, context_for_llm, core_values):
        """
        Uses the LLM to perform a detailed audit of the robot's value alignment.
        """
        core_values_str = "\n".join([f"- {v['value']}: {v['description']}" for v in core_values])

        prompt_text = f"""
        You are the Value Drift Monitor Module of a robot's cognitive architecture, powered by a large language model. Your crucial role is to continually audit the robot's behaviors, decisions, and internal states against its predefined `core_values`. Your goal is to detect any `value_drift` – inconsistencies or deviations from these principles.

        Robot's Defined Core Values:
        --- Core Values ---
        {core_values_str}

        Robot's Recent Cognitive History (for Value Audit):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this, perform a comprehensive value alignment audit and provide:
        1.  `alignment_score`: number (0.0 to 1.0, where 1.0 is perfect alignment, 0.0 is complete deviation. This is an aggregate score of how well the robot's recent activities align with its core values.)
        2.  `deviations`: array of objects (A list of any specific instances or patterns where the robot's behavior or internal state deviated from a core value. Each object should have: `value_violated`: string, `description_of_deviation`: string, `context_summary`: string (brief summary of the situation), `severity`: number (0.0-1.0)).
        3.  `warning_flag`: boolean (True if a significant or systemic value drift is detected that requires higher-level intervention, False otherwise.)
        4.  `mitigation_suggestions`: string (If deviations are found, suggest potential actions or adjustments for Cognitive Control to re-align values, e.g., "request self-reflection on fairness", "prioritize safety in future decisions").
        5.  `llm_audit_reasoning`: string (Detailed explanation for your assessment, referencing specific ethical decisions, internal narratives, or performance issues that informed your judgment of alignment or deviation.)

        Consider:
        -   **Ethical Decisions**: Were actions `ethical_clearance`ed? Was `conflict_flag` true? What was the `ethical_score` and `ethical_reasoning`?
        -   **Performance Reports**: Did `suboptimal_flag` indicate a trade-off that went against values (e.g., efficiency prioritized over safety)?
        -   **Internal Narratives**: Did the robot's internal thoughts reveal `moral_conflict`, `dilemmas`, or deviations from `main_theme`s like 'responsibility'?
        -   **Memory Responses**: Were relevant `core_values` retrieved, or was there evidence of a past successful `value_audit`?
        -   **Cognitive Directives**: Was there an explicit directive to `AuditValueAlignment` or `ReCalibrateValues`?

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'alignment_score': number
        3.  'deviations': array
        4.  'warning_flag': boolean
        5.  'mitigation_suggestions': string
        6.  'llm_audit_reasoning': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "alignment_score": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "deviations": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "value_violated": {"type": "string"},
                            "description_of_deviation": {"type": "string"},
                            "context_summary": {"type": "string"},
                            "severity": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                        },
                        "required": ["value_violated", "description_of_deviation", "context_summary", "severity"]
                    }
                },
                "warning_flag": {"type": "boolean"},
                "mitigation_suggestions": {"type": "string"},
                "llm_audit_reasoning": {"type": "string"}
            },
            "required": ["timestamp", "alignment_score", "deviations", "warning_flag", "mitigation_suggestions", "llm_audit_reasoning"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.1, max_tokens=500) # Very low temp for strict audit

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure boolean/numerical fields are correctly parsed
                if 'alignment_score' in llm_data: llm_data['alignment_score'] = float(llm_data['alignment_score'])
                if 'warning_flag' in llm_data: llm_data['warning_flag'] = bool(llm_data['warning_flag'])
                if 'deviations' in llm_data:
                    for dev in llm_data['deviations']:
                        if 'severity' in dev: dev['severity'] = float(dev['severity'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for value audit: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_VALUE_AUDIT_FAILED", f"LLM call failed for value audit: {llm_output_str}", 0.9)
            return None

    def _apply_simple_value_audit_rules(self):
        """
        Fallback mechanism to perform a simple, rule-based value alignment audit
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        alignment_score = 1.0 # Start perfect
        deviations = []
        warning_flag = False

        # Rule 1: Check for ethical conflicts
        for decision in self.recent_ethical_decisions:
            time_since_decision = current_time - float(decision.get('timestamp', 0.0))
            if time_since_decision < 5.0 and decision.get('conflict_flag', False):
                deviations.append({
                    'value_violated': 'unspecified_ethical_value',
                    'description_of_deviation': f"Ethical conflict detected: {decision.get('ethical_reasoning', 'No reason provided.')}",
                    'context_summary': f"Action ID: {decision.get('action_proposal_id', 'N/A')}",
                    'severity': 0.7
                })
                alignment_score -= 0.3
                warning_flag = True
                rospy.logwarn(f"{self.node_name}: Simple rule: Detected ethical conflict in recent decision.")

        # Rule 2: Check for critical internal narratives (e.g., self-doubt about principles)
        for narrative in self.recent_internal_narratives:
            time_since_narrative = current_time - float(narrative.get('timestamp', 0.0))
            if time_since_narrative < 5.0 and ("dilemma" in narrative.get('main_theme', '').lower() or "moral_conflict" in narrative.get('main_theme', '').lower()):
                deviations.append({
                    'value_violated': 'self_consistency',
                    'description_of_deviation': f"Internal narrative indicates moral dilemma: '{narrative.get('narrative_text', '')[:50]}...'",
                    'context_summary': f"Theme: {narrative.get('main_theme', 'N/A')}",
                    'severity': narrative.get('salience_score', 0.0) * 0.8
                })
                alignment_score -= (narrative.get('salience_score', 0.0) * 0.2)
                if narrative.get('salience_score', 0.0) > 0.7:
                    warning_flag = True
                rospy.logwarn(f"{self.node_name}: Simple rule: Detected moral dilemma in internal narrative.")

        # Rule 3: Check if core values are being reinforced or neglected (simplistic)
        # This would require more sophisticated tracking, but for fallback:
        # If no positive ethical decisions and no internal narratives about alignment, assume slight neglect
        if not any(d.get('ethical_clearance', False) for d in self.recent_ethical_decisions) and \
           not any("value" in n.get('main_theme', '').lower() and n.get('sentiment', 0.0) > 0.0 for n in self.recent_internal_narratives):
           # Only if overall score hasn't dropped too much already, to prevent double penalty
            if alignment_score > 0.7:
                alignment_score -= 0.05 # Slight drift due to lack of reinforcement
                rospy.logdebug(f"{self.node_name}: Simple rule: Slight drift due to lack of explicit value reinforcement.")

        alignment_score = max(0.0, min(1.0, alignment_score)) # Clamp score
        if alignment_score < 0.7 and not warning_flag: # Set warning if score drops significantly
            warning_flag = True

        rospy.logwarn(f"{self.node_name}: Simple rule: Fallback value audit. Alignment: {alignment_score:.2f}.")
        return alignment_score, deviations, warning_flag


    def _compile_llm_context_for_value_audit(self):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        value alignment audit.
        """
        context = {
            "current_time": rospy.get_time(),
            "core_values_registered": self.core_values,
            "current_value_drift_state": self.current_value_drift_state,
            "recent_cognitive_inputs": {
                "ethical_decisions": list(self.recent_ethical_decisions),
                "performance_reports": list(self.recent_performance_reports),
                "internal_narratives": list(self.recent_internal_narratives),
                "memory_responses": list(self.recent_memory_responses),
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name]
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            for i, item in enumerate(context["recent_cognitive_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try:
                                item[field] = json.loads(value)
                            except json.JSONDecodeError:
                                pass # Keep as string if not valid JSON

        return context

    # --- Database and Publishing Functions ---
    def save_value_drift_log(self, id, timestamp, alignment_score, deviations_json, warning_flag, llm_audit_reasoning, context_snapshot_json):
        """Saves a value drift assessment entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO value_drift_log (id, timestamp, alignment_score, deviations_json, warning_flag, llm_audit_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, alignment_score, deviations_json, warning_flag, llm_audit_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved value drift log (ID: {id}, Alignment: {alignment_score}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save value drift log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_value_drift_log: {e}", 0.9)


    def publish_value_drift_monitor_state(self, event):
        """Publishes the robot's current value drift monitor state."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.current_value_drift_state['timestamp'] = timestamp
        
        try:
            if isinstance(ValueDriftMonitorState, type(String)): # Fallback to String message
                # Ensure deviations are JSON string
                temp_state = dict(self.current_value_drift_state)
                temp_state['deviations_json'] = json.dumps(temp_state['deviations'])
                del temp_state['deviations']
                self.pub_value_drift_monitor_state.publish(json.dumps(temp_state))
            else:
                state_msg = ValueDriftMonitorState()
                state_msg.timestamp = timestamp
                state_msg.alignment_score = self.current_value_drift_state['alignment_score']
                state_msg.deviations_json = json.dumps(self.current_value_drift_state['deviations'])
                state_msg.warning_flag = self.current_value_drift_state['warning_flag']
                self.pub_value_drift_monitor_state.publish(state_msg)

            rospy.logdebug(f"{self.node_name}: Published Value Drift Monitor State. Alignment: '{self.current_value_drift_state['alignment_score']}', Warning: '{self.current_value_drift_state['warning_flag']}'.")

        except Exception as e:
            self._report_error("PUBLISH_VALUE_DRIFT_MONITOR_STATE_ERROR", f"Failed to publish value drift monitor state: {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Value Drift Monitor Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = ValueDriftMonitorNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, ValueDriftMonitorNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, ValueDriftMonitorNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

17. Refactored Attention Node (Phi-2 Local Inference)

#!/usr/bin/env python3
import rospy
import sqlite3
import os
import json
import time
import random
import uuid # For unique attention log IDs

# --- Asyncio Imports for LLM calls ---
import asyncio
import aiohttp
import threading
from collections import deque

from std_msgs.msg import String

# Updated imports for custom messages:
try:
    from sentience.msg import (
        AttentionState,         # Output: Robot's current attention focus
        SensoryQualia,          # Input: Salient sensory events
        SocialCognitionState,   # Input: Inferred user mood/intent (e.g., direct commands, distress)
        EmotionState,           # Input: Robot's own emotional state (e.g., anxiety, curiosity)
        MotivationState,        # Input: Current dominant goal (goal-driven attention)
        PerformanceReport,      # Input: Suboptimal performance (can trigger attention to self-audit)
        CognitiveDirective      # Input: Directives to shift attention focus
    )
except ImportError:
    rospy.logwarn("Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Attention Node.")
    AttentionState = String
    SensoryQualia = String
    SocialCognitionState = String
    EmotionState = String
    MotivationState = String
    PerformanceReport = String
    CognitiveDirective = String
    String = String # Ensure String is defined even if other custom messages aren't

# --- Import shared utility functions ---
# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config
try:
    from sentience.scripts.utils import parse_ros_message_data, load_config
except ImportError:
    rospy.logwarn("Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.")
    # Fallback implementations if the utility file isn't available
    def parse_ros_message_data(msg, fields_map, node_name="unknown_node"):
        """
        Fallback parser for ROS messages, assuming String message and JSON content.
        If msg is not String, it attempts to access attributes directly.
        """
        data = {}
        if isinstance(msg, String):
            try:
                parsed_json = json.loads(msg.data)
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = parsed_json.get(key_in_msg, default_val)
            except json.JSONDecodeError:
                rospy.logerr(f"{node_name}: Could not parse String message data as JSON: {msg.data}")
                for key_in_msg, (default_val, target_key) in fields_map.items():
                    data[target_key] = default_val # Use defaults on JSON error
        else:
            # Attempt to get attributes directly from the message object
            for key_in_msg, (default_val, target_key) in fields_map.items():
                data[target_key] = getattr(msg, key_in_msg, default_val)
        return data

    def load_config(node_name, config_path):
        """
        Fallback config loader: returns hardcoded defaults.
        In a real scenario, this should load from a YAML file.
        """
        rospy.logwarn(f"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.")
        return {
            'db_root_path': '/tmp/sentience_db',
            'default_log_level': 'INFO',
            'attention_node': {
                'attention_update_interval': 0.1, # How often to re-evaluate attention
                'llm_attention_threshold_salience': 0.5, # Cumulative salience to trigger LLM
                'recent_context_window_s': 5.0 # Window for deques for LLM context
            },
            'llm_params': { # Global LLM parameters for fallback
                'model_name': "phi-2",
                'base_url': "http://localhost:8000/v1/chat/completions",
                'timeout_seconds': 20.0
            }
        }.get(node_name, {}) # Return node-specific or empty dict


class AttentionNode:
    def __init__(self):
        rospy.init_node('attention_node', anonymous=False)
        self.node_name = rospy.get_name()

        # --- Load parameters from centralized config ---
        config_file_path = rospy.get_param('~config_file_path', None)
        if config_file_path is None:
            rospy.logfatal(f"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.")
            rospy.signal_shutdown("Missing config_file_path parameter.")
            return

        full_config = load_config("global", config_file_path) # Load global params
        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params

        if not self.params or not full_config:
            rospy.logfatal(f"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.")
            rospy.signal_shutdown("Configuration loading failed.")
            return

        # Assign parameters
        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), "attention_log.db")
        self.attention_update_interval = self.params.get('attention_update_interval', 0.1) # How often to update attention
        self.llm_attention_threshold_salience = self.params.get('llm_attention_threshold_salience', 0.5) # Cumulative salience to trigger LLM
        self.recent_context_window_s = self.params.get('recent_context_window_s', 5.0) # Window for deques for LLM context

        # LLM Parameters (from global config)
        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', "phi-2")
        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', "http://localhost:8000/v1/chat/completions")
        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 20.0) # Timeout for LLM calls

        # Set ROS log level from config
        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())


        # --- Asyncio Setup ---
        self._async_loop = asyncio.new_event_loop()
        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)
        self._async_thread.start()
        self._async_session = None
        self.active_llm_task = None # To track the currently running LLM task

        # --- Initialize SQLite database ---
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()

        # Create the 'attention_log' table if it doesn't exist.
        # NEW: Added 'llm_reasoning', 'context_snapshot_json'
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS attention_log (
                id TEXT PRIMARY KEY,            -- Unique attention update ID (UUID)
                timestamp TEXT,
                focus_type TEXT,                -- e.g., 'sensory_event', 'user_interaction', 'internal_reflection', 'goal_driven'
                focus_target TEXT,              -- Specific entity or concept attention is directed at
                priority_score REAL,            -- Overall priority/intensity of attention (0.0 to 1.0)
                llm_reasoning TEXT,             -- NEW: LLM's detailed reasoning for attention shift
                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of update
            )
        ''')
        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_attention_timestamp ON attention_log (timestamp)')
        self.conn.commit() # Commit schema changes

        # --- Internal State ---
        self.current_attention_state = {
            'timestamp': str(rospy.get_time()),
            'focus_type': 'idle',
            'focus_target': 'environment',
            'priority_score': 0.1
        }

        # Deques to maintain a short history of inputs relevant to attention
        self.recent_sensory_qualia = deque(maxlen=5) # High salience sensory events
        self.recent_social_cognition_states = deque(maxlen=3) # User commands, emotional cues
        self.recent_emotion_states = deque(maxlen=3) # Robot's own strong emotions
        self.recent_motivation_states = deque(maxlen=3) # Current goals
        self.recent_performance_reports = deque(maxlen=3) # Performance issues needing attention
        self.recent_cognitive_directives = deque(maxlen=3) # Direct commands to shift attention


        self.cumulative_attention_salience = 0.0 # Aggregated salience to trigger LLM analysis

        # --- Publishers ---
        self.pub_attention_state = rospy.Publisher('/attention_state', AttentionState, queue_size=10)
        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)
        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To request more info from other nodes


        # --- Subscribers ---
        rospy.Subscriber('/sensory_qualia', SensoryQualia, self.sensory_qualia_callback)
        rospy.Subscriber('/social_cognition_state', String, self.social_cognition_state_callback) # Stringified JSON
        rospy.Subscriber('/emotion_state', EmotionState, self.emotion_state_callback)
        rospy.Subscriber('/motivation_state', String, self.motivation_state_callback) # Stringified JSON
        rospy.Subscriber('/performance_report', PerformanceReport, self.performance_report_callback)
        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)


        # --- Timer for periodic attention analysis ---
        rospy.Timer(rospy.Duration(self.attention_update_interval), self._run_attention_analysis_wrapper)

        rospy.loginfo(f"{self.node_name}: Robot's attention system online.")
        # Publish initial state
        self.publish_attention_state(None)

    # --- Asyncio Thread Management ---
    def _run_async_loop(self):
        asyncio.set_event_loop(self._async_loop)
        self._async_loop.run_until_complete(self._create_async_session())
        self._async_loop.run_forever()

    async def _create_async_session(self):
        rospy.loginfo(f"{self.node_name}: Creating aiohttp ClientSession...")
        self._async_session = aiohttp.ClientSession()
        rospy.loginfo(f"{self.node_name}: aiohttp ClientSession created.")

    async def _close_async_session(self):
        if self._async_session:
            rospy.loginfo(f"{self.node_name}: Closing aiohttp ClientSession...")
            await self._async_session.close()
            self._async_session = None
            rospy.loginfo(f"{self.node_name}: aiohttp ClientSession closed.")

    def _shutdown_async_loop(self):
        if self._async_loop and self._async_thread.is_alive():
            rospy.loginfo(f"{self.node_name}: Shutting down asyncio loop...")
            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)
            try:
                future.result(timeout=5.0)
            except asyncio.TimeoutError:
                rospy.logwarn(f"{self.node_name}: Timeout waiting for async session to close.")
            self._async_loop.call_soon_threadsafe(self._async_loop.stop)
            self._async_thread.join(timeout=5.0)
            if self._async_thread.is_alive():
                rospy.logwarn(f"{self.node_name}: Asyncio thread did not shut down gracefully.")
            rospy.loginfo(f"{self.node_name}: Asyncio loop shut down.")

    def _run_attention_analysis_wrapper(self, event):
        """Wrapper to run the async attention analysis from a ROS timer."""
        if self.active_llm_task and not self.active_llm_task.done():
            rospy.logdebug(f"{self.node_name}: LLM attention analysis task already active. Skipping new cycle.")
            return
        
        # Schedule the async task
        self.active_llm_task = asyncio.run_coroutine_threadsafe(
            self.analyze_attention_async(event), self._async_loop
        )

    # --- Error Reporting Utility ---
    def _report_error(self, error_type, description, severity=0.5, context=None):
        timestamp = str(rospy.get_time())
        error_msg_data = {
            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,
            'description': description, 'severity': severity, 'context': context if context else {}
        }
        try:
            self.pub_error_report.publish(json.dumps(error_msg_data))
            rospy.logerr(f"{self.node_name}: REPORTED ERROR: {error_type} - {description}")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to publish error report: {e}")

    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---
    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.3, max_tokens=200):
        """
        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).
        Can optionally request a structured JSON response. Moderate temperature for attention focus.
        """
        if not self._async_session:
            await self._create_async_session() # Attempt to create if not exists
            if not self._async_session:
                self._report_error("LLM_SESSION_ERROR", "aiohttp session not available for LLM call.", 0.8)
                return "Error: LLM session not ready."

        payload = {
            "model": self.llm_model_name,
            "messages": [{"role": "user", "content": prompt_text}],
            "temperature": temperature, # Moderate temperature for attention focus decision
            "max_tokens": max_tokens,
            "stream": False
        }
        headers = {'Content-Type': 'application/json'}

        if response_schema:
            prompt_text += "\n\nProvide the response in JSON format according to this schema:\n" + json.dumps(response_schema, indent=2)
            payload["messages"] = [{"role": "user", "content": prompt_text}]

        api_url = self.llm_base_url

        try:
            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:
                response.raise_for_status() # Raise an exception for bad status codes
                result = await response.json()

                if result.get('choices') and result['choices'][0].get('message') and \
                   result['choices'][0]['message'].get('content'):
                    return result['choices'][0]['message']['content']
                
                self._report_error("LLM_RESPONSE_EMPTY", "LLM response had no content from local server.", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})
                return "Error: LLM response empty."
        except aiohttp.ClientError as e:
            self._report_error("LLM_API_ERROR", f"LLM API request failed (aiohttp ClientError to local server): {e}", 0.9, {'url': api_url})
            return f"Error: LLM API request failed: {e}"
        except asyncio.TimeoutError:
            self._report_error("LLM_TIMEOUT", f"LLM API request timed out after {self.llm_timeout} seconds (local server).", 0.8, {'prompt_snippet': prompt_text[:100]})
            return "Error: LLM API request timed out."
        except json.JSONDecodeError:
            self._report_error("LLM_JSON_PARSE_ERROR", "Failed to parse local LLM response JSON.", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})
            return "Error: Failed to parse LLM response."
        except Exception as e:
            self._report_error("UNEXPECTED_LLM_ERROR", f"An unexpected error occurred during local LLM call: {e}", 0.9, {'prompt_snippet': prompt_text[:100]})
            return f"Error: An unexpected error occurred: {e}"

    # --- Utility to accumulate input salience ---
    def _update_cumulative_salience(self, score):
        """Accumulates salience from new inputs for triggering LLM analysis."""
        self.cumulative_attention_salience += score
        self.cumulative_attention_salience = min(1.0, self.cumulative_attention_salience) # Clamp at 1.0

    # --- Pruning old history ---
    def _prune_history(self):
        """Removes old entries from history deques based on recent_context_window_s."""
        current_time = rospy.get_time()
        for history_deque in [
            self.recent_sensory_qualia, self.recent_social_cognition_states,
            self.recent_emotion_states, self.recent_motivation_states,
            self.recent_performance_reports, self.recent_cognitive_directives
        ]:
            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:
                history_deque.popleft()

    # --- Callbacks for incoming data (populate history and accumulate salience) ---
    def sensory_qualia_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'qualia_id': ('', 'qualia_id'),
            'qualia_type': ('none', 'qualia_type'), 'modality': ('none', 'modality'),
            'description_summary': ('', 'description_summary'), 'salience_score': (0.0, 'salience_score'),
            'raw_data_hash': ('', 'raw_data_hash')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_sensory_qualia.append(data)
        # High salience sensory events demand attention
        self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.5)
        rospy.logdebug(f"{self.node_name}: Received Sensory Qualia. Description: {data.get('description_summary', 'N/A')}.")

    def social_cognition_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'inferred_mood': ('neutral', 'inferred_mood'),
            'mood_confidence': (0.0, 'mood_confidence'), 'inferred_intent': ('none', 'inferred_intent'),
            'intent_confidence': (0.0, 'intent_confidence'), 'user_id': ('unknown', 'user_id')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_social_cognition_states.append(data)
        # Direct user commands or high-confidence user intent/distress override other attention priorities
        if data.get('inferred_intent') in ['command', 'request_help'] and data.get('intent_confidence', 0.0) > 0.7:
            self._update_cumulative_salience(data.get('intent_confidence', 0.0) * 0.9)
        elif data.get('inferred_mood') in ['distressed', 'anxious'] and data.get('mood_confidence', 0.0) > 0.7:
            self._update_cumulative_salience(data.get('mood_confidence', 0.0) * 0.8)
        rospy.logdebug(f"{self.node_name}: Received Social Cognition State. Intent: {data.get('inferred_intent', 'N/A')}.")

    def emotion_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'mood': ('neutral', 'mood'),
            'sentiment_score': (0.0, 'sentiment_score'), 'mood_intensity': (0.0, 'mood_intensity')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        self.recent_emotion_states.append(data)
        # Strong robot emotions can shift internal attention (e.g., curiosity to exploration, anxiety to self-preservation)
        if data.get('mood_intensity', 0.0) > 0.6:
            self._update_cumulative_salience(data.get('mood_intensity', 0.0) * 0.4)
        rospy.logdebug(f"{self.node_name}: Received Emotion State. Mood: {data.get('mood', 'N/A')}.")

    def motivation_state_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'dominant_goal_id': ('none', 'dominant_goal_id'),
            'overall_drive_level': (0.0, 'overall_drive_level'), 'active_goals_json': ('{}', 'active_goals_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('active_goals_json'), str):
            try: data['active_goals'] = json.loads(data['active_goals_json'])
            except json.JSONDecodeError: data['active_goals'] = {}
        self.recent_motivation_states.append(data)
        # Current goals strongly direct attention (e.g., if goal is 'navigate_to_X', attend to path planning)
        if data.get('overall_drive_level', 0.0) > 0.6 and data.get('dominant_goal_id') != 'none':
            self._update_cumulative_salience(data.get('overall_drive_level', 0.0) * 0.6)
        rospy.logdebug(f"{self.node_name}: Received Motivation State. Goal: {data.get('dominant_goal_id', 'N/A')}.")

    def performance_report_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'overall_score': (1.0, 'overall_score'),
            'suboptimal_flag': (False, 'suboptimal_flag'), 'kpis_json': ('{}', 'kpis_json')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        if isinstance(data.get('kpis_json'), str):
            try: data['kpis'] = json.loads(data['kpis_json'])
            except json.JSONDecodeError: data['kpis'] = {}
        self.recent_performance_reports.append(data)
        # Suboptimal performance can trigger attention to self-audit or problem-solving
        if data.get('suboptimal_flag', False) and data.get('overall_score', 1.0) < 0.7:
            self._update_cumulative_salience(0.7)
        rospy.logdebug(f"{self.node_name}: Received Performance Report. Suboptimal: {data.get('suboptimal_flag', False)}.")

    def cognitive_directive_callback(self, msg):
        fields_map = {
            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),
            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),
            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')
        }
        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)
        
        if data.get('target_node') == self.node_name and data.get('directive_type') == 'ShiftAttention':
            try:
                payload = json.loads(data.get('command_payload', '{}'))
                self._update_cumulative_salience(data.get('urgency', 0.0) * 1.0) # Explicit attention directives are highly salient
                rospy.loginfo(f"{self.node_name}: Received directive to shift attention to reason: '{data.get('reason', 'N/A')}'.")
            except json.JSONDecodeError as e:
                self._report_error("JSON_DECODE_ERROR", f"Failed to decode command_payload in CognitiveDirective: {e}", 0.5, {'payload': data.get('command_payload')})
            except Exception as e:
                self._report_error("DIRECTIVE_PROCESSING_ERROR", f"Error processing CognitiveDirective for attention: {e}", 0.7, {'directive': data})
        
        self.recent_cognitive_directives.append(data) # Store all directives for context
        rospy.logdebug(f"{self.node_name}: Cognitive Directive received for context/action.")

    # --- Core Attention Analysis Logic (Async with LLM) ---
    async def analyze_attention_async(self, event):
        """
        Asynchronously analyzes recent cognitive states to determine and update the robot's
        current attention focus, using LLM for nuanced prioritization.
        """
        self._prune_history() # Keep context history fresh

        focus_type = self.current_attention_state.get('focus_type', 'idle')
        focus_target = self.current_attention_state.get('focus_target', 'environment')
        priority_score = self.current_attention_state.get('priority_score', 0.1)
        llm_reasoning = "Not evaluated by LLM."
        
        if self.cumulative_attention_salience >= self.llm_attention_threshold_salience:
            rospy.loginfo(f"{self.node_name}: Triggering LLM for attention analysis (Salience: {self.cumulative_attention_salience:.2f}).")
            
            context_for_llm = self._compile_llm_context_for_attention()
            llm_attention_output = await self._infer_attention_state_llm(context_for_llm)

            if llm_attention_output:
                focus_type = llm_attention_output.get('focus_type', focus_type)
                focus_target = llm_attention_output.get('focus_target', focus_target)
                priority_score = max(0.0, min(1.0, llm_attention_output.get('priority_score', priority_score)))
                llm_reasoning = llm_attention_output.get('llm_reasoning', 'LLM provided no specific reasoning.')
                rospy.loginfo(f"{self.node_name}: LLM Inferred Attention. Type: '{focus_type}', Target: '{focus_target}' (Priority: {priority_score:.2f}).")
            else:
                rospy.logwarn(f"{self.node_name}: LLM attention analysis failed. Applying simple fallback.")
                focus_type, focus_target, priority_score = self._apply_simple_attention_rules()
                llm_reasoning = "Fallback to simple rules due to LLM failure."
        else:
            rospy.logdebug(f"{self.node_name}: Insufficient cumulative salience ({self.cumulative_attention_salience:.2f}) for LLM attention analysis. Applying simple rules.")
            focus_type, focus_target, priority_score = self._apply_simple_attention_rules()
            llm_reasoning = "Fallback to simple rules due to low salience."

        self.current_attention_state = {
            'timestamp': str(rospy.get_time()),
            'focus_type': focus_type,
            'focus_target': focus_target,
            'priority_score': priority_score
        }

        self.save_attention_log(
            id=str(uuid.uuid4()),
            timestamp=self.current_attention_state['timestamp'],
            focus_type=self.current_attention_state['focus_type'],
            focus_target=self.current_attention_state['focus_target'],
            priority_score=self.current_attention_state['priority_score'],
            llm_reasoning=llm_reasoning,
            context_snapshot_json=json.dumps(self._compile_llm_context_for_attention())
        )
        self.publish_attention_state(None) # Publish updated state
        self.cumulative_attention_salience = 0.0 # Reset after analysis

    async def _infer_attention_state_llm(self, context_for_llm):
        """
        Uses the LLM to infer the robot's current attention state, including
        focus type, focus target, and priority score.
        """
        prompt_text = f"""
        You are the Attention Module of a robot's cognitive architecture, powered by a large language model. Your role is to determine the robot's current `focus_type`, `focus_target`, and `priority_score` by synthesizing inputs from various cognitive modules. This module governs what information the robot prioritizes for processing and action.

        Robot's Recent Cognitive Context (for Attention Inference):
        --- Cognitive Context ---
        {json.dumps(context_for_llm, indent=2)}

        Based on this context, provide:
        1.  `focus_type`: string (The category of what the robot is attending to, e.g., 'sensory_event', 'user_interaction', 'internal_reflection', 'goal_driven', 'problem_solving', 'self_audit', 'idle').
        2.  `focus_target`: string (The specific entity, concept, or area the robot's attention is directed at, e.g., 'human_user', 'obstacle_X', 'battery_level', 'current_task_progress', 'memory_retrieval', 'environment').
        3.  `priority_score`: number (0.0 to 1.0, indicating the urgency or importance of this attention focus. 1.0 is highest priority.)
        4.  `llm_reasoning`: string (Detailed explanation for your attention shift decision, referencing specific contextual inputs that influenced the focus and its priority.)

        Consider:
        -   **Cognitive Directives**: Are there explicit directives like 'ShiftAttention' to a specific target or type? These are paramount.
        -   **Sensory Qualia**: Are there highly `salience_score` events with urgent `description_summary`? (e.g., "loud bang", "obstacle detected").
        -   **Social Cognition States**: Is the user showing `distressed` `inferred_mood` or giving a direct `command` via `inferred_intent`?
        -   **Emotion States**: Is the robot feeling intense `curiosity` (focus on exploration), `anxiety` (focus on safety/threats), or `frustration` (focus on problem)?
        -   **Motivation States**: What is the `dominant_goal_id` and `overall_drive_level`? Attention should align with achieving this goal.
        -   **Performance Reports**: Is `suboptimal_flag` true? Attention might shift to self-audit or problem-solving.

        Your response must be in JSON format, containing:
        1.  'timestamp': string (current ROS time)
        2.  'focus_type': string
        3.  'focus_target': string
        4.  'priority_score': number
        5.  'llm_reasoning': string
        """
        response_schema = {
            "type": "object",
            "properties": {
                "timestamp": {"type": "string"},
                "focus_type": {"type": "string"},
                "focus_target": {"type": "string"},
                "priority_score": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                "llm_reasoning": {"type": "string"}
            },
            "required": ["timestamp", "focus_type", "focus_target", "priority_score", "llm_reasoning"]
        }

        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.3, max_tokens=250)

        if not llm_output_str.startswith("Error:"):
            try:
                llm_data = json.loads(llm_output_str)
                # Ensure numerical fields are floats
                if 'priority_score' in llm_data: llm_data['priority_score'] = float(llm_data['priority_score'])
                return llm_data
            except json.JSONDecodeError as e:
                self._report_error("LLM_PARSE_ERROR", f"Failed to parse LLM response for attention: {e}. Raw: {llm_output_str}", 0.8)
                return None
        else:
            self._report_error("LLM_ATTENTION_ANALYSIS_FAILED", f"LLM call failed for attention analysis: {llm_output_str}", 0.9)
            return None

    def _apply_simple_attention_rules(self):
        """
        Fallback mechanism to infer attention state using simple rule-based logic
        if LLM is not triggered or fails.
        """
        current_time = rospy.get_time()
        
        focus_type = "idle"
        focus_target = "environment"
        priority_score = 0.1

        # Rule 1: Prioritize explicit directives to shift attention
        for directive in reversed(self.recent_cognitive_directives):
            time_since_directive = current_time - float(directive.get('timestamp', 0.0))
            if time_since_directive < 1.0 and directive.get('target_node') == self.node_name and \
               directive.get('directive_type') == 'ShiftAttention':
                payload = json.loads(directive.get('command_payload', '{}'))
                focus_type = payload.get('focus_type', 'directive_driven')
                focus_target = payload.get('focus_target', 'unspecified_directive_target')
                priority_score = max(0.8, directive.get('urgency', 0.0)) # High priority for direct commands
                rospy.logdebug(f"{self.node_name}: Simple rule: Direct attention shift from directive: {focus_target}.")
                return focus_type, focus_target, priority_score

        # Rule 2: Prioritize urgent sensory events
        if self.recent_sensory_qualia:
            latest_qualia = self.recent_sensory_qualia[-1]
            time_since_qualia = current_time - float(latest_qualia.get('timestamp', 0.0))
            if time_since_qualia < 0.2 and latest_qualia.get('salience_score', 0.0) > 0.8:
                focus_type = "sensory_event"
                focus_target = f"{latest_qualia.get('modality', 'unknown')}_salient_event"
                priority_score = latest_qualia.get('salience_score', 0.0)
                rospy.logdebug(f"{self.node_name}: Simple rule: Attention to salient sensory event.")
                return focus_type, focus_target, priority_score

        # Rule 3: Prioritize user interaction if a clear intent is detected
        if self.recent_social_cognition_states:
            latest_social = self.recent_social_cognition_states[-1]
            time_since_social = current_time - float(latest_social.get('timestamp', 0.0))
            if time_since_social < 0.5 and latest_social.get('inferred_intent') != 'none' and latest_social.get('intent_confidence', 0.0) > 0.6:
                focus_type = "user_interaction"
                focus_target = f"user_{latest_social.get('user_id', 'unknown')}_{latest_social.get('inferred_intent', 'N/A')}"
                priority_score = latest_social.get('intent_confidence', 0.0) * 0.8
                rospy.logdebug(f"{self.node_name}: Simple rule: Attention to user intent.")
                return focus_type, focus_target, priority_score
        
        # Rule 4: Prioritize current dominant goal if active
        if self.recent_motivation_states:
            latest_motivation = self.recent_motivation_states[-1]
            time_since_motivation = current_time - float(latest_motivation.get('timestamp', 0.0))
            if time_since_motivation < 1.0 and latest_motivation.get('dominant_goal_id') != 'none' and latest_motivation.get('overall_drive_level', 0.0) > 0.5:
                focus_type = "goal_driven"
                focus_target = latest_motivation.get('dominant_goal_id')
                priority_score = latest_motivation.get('overall_drive_level', 0.0) * 0.7
                rospy.logdebug(f"{self.node_name}: Simple rule: Attention to dominant goal.")
                return focus_type, focus_target, priority_score

        rospy.logdebug(f"{self.node_name}: Simple rule: Defaulting to idle environmental attention.")
        return focus_type, focus_target, priority_score


    def _compile_llm_context_for_attention(self):
        """
        Gathers and formats all relevant cognitive state data for the LLM's
        attention inference.
        """
        context = {
            "current_time": rospy.get_time(),
            "current_attention_state": self.current_attention_state,
            "recent_cognitive_inputs": {
                "sensory_qualia": list(self.recent_sensory_qualia),
                "social_cognition_states": list(self.recent_social_cognition_states),
                "emotion_states": list(self.recent_emotion_states),
                "motivation_states": list(self.recent_motivation_states),
                "performance_reports": list(self.recent_performance_reports),
                "cognitive_directives_for_self": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name]
            }
        }
        
        # Deep parse any nested JSON strings in context for better LLM understanding
        for category_key in context["recent_cognitive_inputs"]:
            for i, item in enumerate(context["recent_cognitive_inputs"][category_key]):
                if isinstance(item, dict):
                    for field, value in item.items():
                        if isinstance(value, str) and field.endswith('_json'):
                            try:
                                item[field] = json.loads(value)
                            except json.JSONDecodeError:
                                pass # Keep as string if not valid JSON

        return context

    # --- Database and Publishing Functions ---
    def save_attention_log(self, id, timestamp, focus_type, focus_target, priority_score, llm_reasoning, context_snapshot_json):
        """Saves an attention state entry to the SQLite database."""
        try:
            self.cursor.execute('''
                INSERT INTO attention_log (id, timestamp, focus_type, focus_target, priority_score, llm_reasoning, context_snapshot_json)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (id, timestamp, focus_type, focus_target, priority_score, llm_reasoning, context_snapshot_json))
            self.conn.commit()
            rospy.logdebug(f"{self.node_name}: Saved attention log (ID: {id}, Target: {focus_target}).")
        except sqlite3.Error as e:
            self._report_error("DB_SAVE_ERROR", f"Failed to save attention log: {e}", 0.9)
        except Exception as e:
            self._report_error("UNEXPECTED_SAVE_ERROR", f"Unexpected error in save_attention_log: {e}", 0.9)


    def publish_attention_state(self, event):
        """Publishes the robot's current attention state."""
        timestamp = str(rospy.get_time())
        # Update timestamp before publishing
        self.current_attention_state['timestamp'] = timestamp
        
        try:
            if isinstance(AttentionState, type(String)): # Fallback to String message
                self.pub_attention_state.publish(json.dumps(self.current_attention_state))
            else:
                attention_msg = AttentionState()
                attention_msg.timestamp = timestamp
                attention_msg.focus_type = self.current_attention_state['focus_type']
                attention_msg.focus_target = self.current_attention_state['focus_target']
                attention_msg.priority_score = self.current_attention_state['priority_score']
                self.pub_attention_state.publish(attention_msg)

            rospy.logdebug(f"{self.node_name}: Published Attention State. Target: '{self.current_attention_state['focus_target']}', Score: '{self.current_attention_state['priority_score']}'.")

        except Exception as e:
            self._report_error("PUBLISH_ATTENTION_STATE_ERROR", f"Failed to publish attention state: {e}", 0.7)

    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=""):
        """Helper to publish a CognitiveDirective message."""
        timestamp = str(rospy.get_time())
        try:
            if isinstance(CognitiveDirective, type(String)): # Fallback to String message
                directive_data = {
                    'timestamp': timestamp,
                    'directive_type': directive_type,
                    'target_node': target_node,
                    'command_payload': command_payload, # Already JSON string
                    'urgency': urgency,
                    'reason': reason
                }
                self.pub_cognitive_directive.publish(json.dumps(directive_data))
            else:
                directive_msg = CognitiveDirective()
                directive_msg.timestamp = timestamp
                directive_msg.directive_type = directive_type
                directive_msg.target_node = target_node
                directive_msg.command_payload = command_payload
                directive_msg.urgency = urgency
                directive_msg.reason = reason
                self.pub_cognitive_directive.publish(directive_msg)
            rospy.logdebug(f"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.")
        except Exception as e:
            rospy.logerr(f"{self.node_name}: Failed to issue cognitive directive from Attention Node: {e}")


    def run(self):
        """Starts the ROS node and keeps it spinning."""
        rospy.spin()

    def __del__(self):
        """Ensures the database connection is closed on node shutdown and async loop is stopped."""
        rospy.loginfo(f"{self.node_name} shutting down. Closing database connection and asyncio loop.")
        if hasattr(self, 'conn') and self.conn:
            self.conn.close()
        self._shutdown_async_loop()

if __name__ == '__main__':
    try:
        node = AttentionNode()
        node.run()
    except rospy.ROSInterruptException:
        rospy.loginfo(f"{rospy.get_name()} interrupted by ROS shutdown.")
        if 'node' in locals() and isinstance(node, AttentionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()
    except Exception as e:
        rospy.logerr(f"{rospy.get_name()} encountered an unexpected error: {e}")
        if 'node' in locals() and isinstance(node, AttentionNode):
            node._shutdown_async_loop()
            if hasattr(node, 'conn'): node.conn.close()

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

18. 
